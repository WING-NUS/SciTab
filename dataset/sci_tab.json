[
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "068fc4e5-d40c-4788-a5a7-bad858cbd7c7",
    "claim": "The models using BoC outperform models using BoW as well as ASM features.",
    "label": "supports",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
    "table_column_names": [
      "[EMPTY]",
      "Difference Function",
      "Seanad Abolition",
      "Video Games",
      "Pornography"
    ],
    "table_content_values": [
      [
        "OD-parse",
        "Absolute",
        "0.01",
        "-0.01",
        "0.07"
      ],
      [
        "OD-parse",
        "JS div.",
        "0.01",
        "-0.01",
        "-0.01"
      ],
      [
        "OD-parse",
        "EMD",
        "0.07",
        "0.01",
        "-0.01"
      ],
      [
        "OD",
        "Absolute",
        "[BOLD] 0.54",
        "[BOLD] 0.56",
        "[BOLD] 0.41"
      ],
      [
        "OD",
        "JS div.",
        "0.07",
        "-0.01",
        "-0.02"
      ],
      [
        "OD",
        "EMD",
        "0.26",
        "-0.01",
        "0.01"
      ],
      [
        "OD (no polarity shifters)",
        "Absolute",
        "0.23",
        "0.08",
        "0.04"
      ],
      [
        "OD (no polarity shifters)",
        "JS div.",
        "0.09",
        "-0.01",
        "-0.02"
      ],
      [
        "OD (no polarity shifters)",
        "EMD",
        "0.10",
        "0.01",
        "-0.01"
      ]
    ],
    "id": "4ab2d01c-b861-49be-9fd3-b87ffdc7ac20",
    "claim": "[CONTINUE] OD significantly outperforms OD-parse: We observe that compared to OD-parse, OD is much more accurate.",
    "label": "supports",
    "table_id": "fba0fe3a-58cc-47a0-a3ab-5d091d2b7fe7"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "5315f378-f575-4846-80da-cacca49cb54a",
    "claim": "Table 4: Comparison of per-document accuracy (% ) by different systems for top 1, 3 and 5 words of abstractive sentences.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.",
    "table_column_names": [
      "[EMPTY]",
      "MFT",
      "UnsupEmb",
      "Word2Tag"
    ],
    "table_content_values": [
      [
        "POS",
        "91.95",
        "87.06",
        "95.55"
      ],
      [
        "SEM",
        "82.00",
        "81.11",
        "91.41"
      ]
    ],
    "id": "cc5ee69b-76a7-4812-b9f9-9bfbdb654101",
    "claim": "The UnsupEmb baseline performs rather poorly on both POS and SEM tagging.",
    "label": "supports",
    "table_id": "5460f475-54e3-4b59-bbdf-d334c4acc090"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 1: Classifier performance",
    "table_column_names": [
      "Dataset",
      "Class",
      "Precision",
      "Recall",
      "F1"
    ],
    "table_content_values": [
      [
        "[ITALIC] W. & H.",
        "Racism",
        "0.73",
        "0.79",
        "0.76"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.69",
        "0.73",
        "0.71"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.88",
        "0.85",
        "0.86"
      ],
      [
        "[ITALIC] W.",
        "Racism",
        "0.56",
        "0.77",
        "0.65"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.62",
        "0.73",
        "0.67"
      ],
      [
        "[EMPTY]",
        "R. & S.",
        "0.56",
        "0.62",
        "0.59"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.95",
        "0.92",
        "0.94"
      ],
      [
        "[ITALIC] D. et al.",
        "Hate",
        "0.32",
        "0.53",
        "0.4"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.96",
        "0.88",
        "0.92"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.81",
        "0.95",
        "0.87"
      ],
      [
        "[ITALIC] G. et al.",
        "Harass.",
        "0.41",
        "0.19",
        "0.26"
      ],
      [
        "[EMPTY]",
        "Non.",
        "0.75",
        "0.9",
        "0.82"
      ],
      [
        "[ITALIC] F. et al.",
        "Hate",
        "0.33",
        "0.42",
        "0.37"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.87",
        "0.88",
        "0.88"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.5",
        "0.7",
        "0.58"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.88",
        "0.77",
        "0.82"
      ]
    ],
    "id": "d3c5de39-44f7-4eff-93d4-8295045a1db1",
    "claim": "In particular, we see that hate speech and harassment are particularly difficult to detect.",
    "label": "supports",
    "table_id": "c989cbdc-ad17-41ce-836a-77ad71aefbc8"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "3addae78-e7aa-4757-a6e9-dc12dac74787",
    "claim": "The results prove the effectiveness of word-level attention to exploit the local interactions in link prediction task.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "47261858-023d-413f-aad0-4d850bd3ffb3",
    "claim": "The average number of tokens per tweet is not 22.3, per sentence is not 13.6 and average scope length is not 2.9.",
    "label": "refutes",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.818",
        "0.719",
        "37.3",
        "10.0"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.819",
        "0.734",
        "26.3",
        "14.2"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.813",
        "0.770",
        "36.4",
        "18.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.807",
        "0.796",
        "28.4",
        "21.5"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.798",
        "0.783",
        "39.7",
        "19.2"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.804",
        "0.785",
        "27.1",
        "20.3"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.805",
        "[BOLD] 0.817",
        "43.3",
        "21.6"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.818",
        "0.805",
        "[BOLD] 29.0",
        "[BOLD] 22.8"
      ]
    ],
    "id": "b4c1d97b-4782-4575-a319-1b40d0ece452",
    "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.",
    "label": "supports",
    "table_id": "ce976181-466e-4d2c-a980-797bf966424e"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "5fcf5218-c00d-4fb7-a5bd-babf7d5687d0",
    "claim": "2018) or reinforcement learning with additional dataset-specific heuristics (Kryscinski et\\xa0al.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
    "paper_id": "1909.02622v2",
    "table_caption": "Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.",
    "table_column_names": [
      "Setting",
      "Metrics",
      "<bold>Direct Assessment</bold> cs-en",
      "<bold>Direct Assessment</bold> de-en",
      "<bold>Direct Assessment</bold> fi-en",
      "<bold>Direct Assessment</bold> lv-en",
      "<bold>Direct Assessment</bold> ru-en",
      "<bold>Direct Assessment</bold> tr-en",
      "<bold>Direct Assessment</bold> zh-en",
      "<bold>Direct Assessment</bold> Average"
    ],
    "table_content_values": [
      [
        "Baselines",
        "METEOR++",
        "0.552",
        "0.538",
        "0.720",
        "0.563",
        "0.627",
        "0.626",
        "0.646",
        "0.610"
      ],
      [
        "Baselines",
        "RUSE(*)",
        "0.624",
        "0.644",
        "0.750",
        "0.697",
        "0.673",
        "0.716",
        "0.691",
        "0.685"
      ],
      [
        "Baselines",
        "BERTScore-F1",
        "0.670",
        "0.686",
        "0.820",
        "0.710",
        "0.729",
        "0.714",
        "0.704",
        "0.719"
      ],
      [
        "Sent-Mover",
        "Smd + W2V",
        "0.438",
        "0.505",
        "0.540",
        "0.442",
        "0.514",
        "0.456",
        "0.494",
        "0.484"
      ],
      [
        "Sent-Mover",
        "Smd + ELMO + PMeans",
        "0.569",
        "0.558",
        "0.732",
        "0.525",
        "0.581",
        "0.620",
        "0.584",
        "0.595"
      ],
      [
        "Sent-Mover",
        "Smd + BERT + PMeans",
        "0.607",
        "0.623",
        "0.770",
        "0.639",
        "0.667",
        "0.641",
        "0.619",
        "0.652"
      ],
      [
        "Sent-Mover",
        "Smd + BERT + MNLI + PMeans",
        "0.616",
        "0.643",
        "0.785",
        "0.660",
        "0.664",
        "0.668",
        "0.633",
        "0.667"
      ],
      [
        "Word-Mover",
        "Wmd-1 + W2V",
        "0.392",
        "0.463",
        "0.558",
        "0.463",
        "0.456",
        "0.485",
        "0.481",
        "0.471"
      ],
      [
        "Word-Mover",
        "Wmd-1 + ELMO + PMeans",
        "0.579",
        "0.588",
        "0.753",
        "0.559",
        "0.617",
        "0.679",
        "0.645",
        "0.631"
      ],
      [
        "Word-Mover",
        "Wmd-1 + BERT + PMeans",
        "0.662",
        "0.687",
        "0.823",
        "0.714",
        "0.735",
        "0.734",
        "0.719",
        "0.725"
      ],
      [
        "Word-Mover",
        "Wmd-1 + BERT + MNLI + PMeans",
        "0.670",
        "0.708",
        "<bold>0.835</bold>",
        "<bold>0.746</bold>",
        "<bold>0.738</bold>",
        "0.762",
        "<bold>0.744</bold>",
        "<bold>0.743</bold>"
      ],
      [
        "Word-Mover",
        "Wmd-2 + BERT + MNLI + PMeans",
        "<bold>0.679</bold>",
        "<bold>0.710</bold>",
        "0.832",
        "0.745",
        "0.736",
        "<bold>0.763</bold>",
        "0.740",
        "<bold>0.743</bold>"
      ]
    ],
    "id": "bfbfa38c-d46e-475e-817c-e5161057ae5f",
    "claim": "Table 1: In all language pairs, the best correlation is achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans.",
    "label": "supports",
    "table_id": "5d8fa2ea-9a19-48f7-b577-698a1cb0cbc6"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "7ce0aea2-aa42-4cc8-99eb-6bdfe53cf6da",
    "claim": "We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph.",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "9d096e1f-029b-48e6-85af-d946dfa8a375",
    "claim": "the model converged and yielded high performance, verifying the efficacy of the implicit answer vector representation for matching word meanings",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "4f80f854-aa9a-4f6d-b67f-0edd0cb7c126",
    "claim": "[CONTINUE] Pretraining the HAN models, although intuitively promising, yields only comparable results with those without.",
    "label": "supports",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 7: Scores for initialization strategies on probing tasks.",
    "table_column_names": [
      "Initialization",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "N(0,0.1)",
        "29.7",
        "71.5",
        "82.0",
        "78.5",
        "60.1",
        "80.5",
        "76.3",
        "74.7",
        "[BOLD] 51.3",
        "52.5"
      ],
      [
        "Glorot",
        "31.3",
        "[BOLD] 72.3",
        "81.8",
        "78.7",
        "59.4",
        "81.3",
        "76.6",
        "[BOLD] 74.6",
        "50.4",
        "57.0"
      ],
      [
        "Our paper",
        "[BOLD] 35.1",
        "70.8",
        "[BOLD] 82.0",
        "[BOLD] 80.2",
        "[BOLD] 61.8",
        "[BOLD] 82.8",
        "[BOLD] 79.7",
        "74.2",
        "50.7",
        "[BOLD] 72.9"
      ]
    ],
    "id": "da1ceb61-ee17-4e2e-970e-222fe51acd08",
    "claim": "While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is not improved by our initialization strategy.",
    "label": "refutes",
    "table_id": "180ff878-5e26-496d-9b71-ff1aeb454328"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. “Acc. Inc.” refers to the boost in performance contributed by the pretraining step. “% of Perf.” refers to the percentage of the total performance that the pretraining step contributes.",
    "table_column_names": [
      "Finetuning",
      "Pretrained?",
      "Accuracy",
      "Val. Loss",
      "Acc. Inc.",
      "% of Perf."
    ],
    "table_content_values": [
      [
        "Multitasking",
        "No",
        "53.61%",
        "0.7217",
        "-",
        "-"
      ],
      [
        "[EMPTY]",
        "Yes",
        "96.28%",
        "0.2197",
        "+42.67%",
        "44.32%"
      ],
      [
        "Standard",
        "No",
        "51.02%",
        "0.7024",
        "-",
        "-"
      ],
      [
        "[EMPTY]",
        "Yes",
        "90.99%",
        "0.1826",
        "+39.97%",
        "43.93%"
      ]
    ],
    "id": "f447aac8-3df2-4446-82f9-89b20ad46901",
    "claim": "In Table 5, it can be seen that generative pretraining via language modeling does not account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup.",
    "label": "refutes",
    "table_id": "3d184b37-c091-4992-beff-c60005fbd2b4"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.",
    "table_column_names": [
      "[EMPTY]",
      "dev perp ↓",
      "dev acc ↑",
      "dev wer ↓",
      "test perp ↓",
      "test acc ↑",
      "test wer ↓"
    ],
    "table_content_values": [
      [
        "Spanish-only-LM",
        "329.68",
        "26.6",
        "30.47",
        "322.26",
        "25.1",
        "29.62"
      ],
      [
        "English-only-LM",
        "320.92",
        "29.3",
        "32.02",
        "314.04",
        "30.3",
        "32.51"
      ],
      [
        "All:CS-last-LM",
        "76.64",
        "47.8",
        "14.56",
        "76.97",
        "49.2",
        "14.13"
      ],
      [
        "All:Shuffled-LM",
        "68.00",
        "51.8",
        "13.64",
        "68.72",
        "51.4",
        "13.89"
      ],
      [
        "CS-only-LM",
        "43.20",
        "60.7",
        "12.60",
        "43.42",
        "57.9",
        "12.18"
      ],
      [
        "CS-only+vocab-LM",
        "45.61",
        "61.0",
        "12.56",
        "45.79",
        "58.8",
        "12.49"
      ],
      [
        "Fine-Tuned-LM",
        "39.76",
        "66.9",
        "10.71",
        "40.11",
        "65.4",
        "10.17"
      ],
      [
        "CS-only-disc",
        "–",
        "72.0",
        "6.35",
        "–",
        "70.5",
        "6.70"
      ],
      [
        "Fine-Tuned-disc",
        "–",
        "[BOLD] 74.2",
        "[BOLD] 5.85",
        "–",
        "[BOLD] 75.5",
        "[BOLD] 5.59"
      ]
    ],
    "id": "286a8de8-bba3-4a30-8e62-b75d6d91ed7d",
    "claim": "Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 70.5%, 5.1 points more than the accuracy of the FINE-TUNED-LM model.",
    "label": "supports",
    "table_id": "d4c8c3ca-899e-4e8c-8efe-81b447163aa5"
  },
  {
    "paper": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation",
    "paper_id": "1909.00754v2",
    "table_caption": "Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For “- Hierachical-Attn”, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For “- MLP”, we further replace the MLP with a single linear layer with the non-linear activation.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Joint Acc."
    ],
    "table_content_values": [
      [
        "COMER",
        "88.64%"
      ],
      [
        "- Hierachical-Attn",
        "86.69%"
      ],
      [
        "- MLP",
        "83.24%"
      ]
    ],
    "id": "b2165172-65ea-4be7-808b-e6bd633803e9",
    "claim": "[CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules.",
    "label": "supports",
    "table_id": "1d045a18-ec29-4ba4-806d-f102ece9f985"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "a0a02ddf-285a-4710-8e41-1b089783b27b",
    "claim": "This observation concurs with the performance boost for this model across the two datasets and shows that using a more advanced architecture with more parameters results in larger improvements using the coverage mechanism.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "ea6f8a48-40b4-4af6-9c29-8eb66af68e74",
    "claim": "for example, for those rewards, models learn the ROUGE reward much better than the full extent of system-level ROUGE correlation as shown in Table 1, which will also increase system-level ROUGE.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "4169c808-fdbd-4bc2-a58c-0ad6872535b1",
    "claim": "Mentions of time are not specific of complaints (been, still, on, days, Temporal References cluster).",
    "label": "refutes",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "What do Deep Networks Like to Read?",
    "paper_id": "1909.04547v1",
    "table_caption": "Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
    "table_column_names": [
      "[EMPTY]",
      "<bold>RNN</bold>",
      "<bold>CNN</bold>",
      "<bold>DAN</bold>"
    ],
    "table_content_values": [
      [
        "Positive",
        "+9.7",
        "+4.3",
        "+<bold>23.6</bold>"
      ],
      [
        "Negative",
        "+6.9",
        "+5.5",
        "+<bold>16.1</bold>"
      ],
      [
        "Flipped to Positive",
        "+20.2",
        "+24.9",
        "+27.4"
      ],
      [
        "Flipped to Negative",
        "+31.5",
        "+28.6",
        "+19.3"
      ]
    ],
    "id": "2ac86f7a-9e75-41a0-a2c9-f36542cb12cf",
    "claim": "This is especially true in the case of DAN where we see a large increase as the decoder repeatedly predicts words having high sentiment value.",
    "label": "supports",
    "table_id": "fe569d5a-7fe9-4318-80c3-68d5d6108755"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 4: Cue classification on the test set.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] F-Score  [BOLD] Baseline",
      "[BOLD] F-Score  [BOLD] Proposed",
      "[BOLD] Support"
    ],
    "table_content_values": [
      [
        "False cues",
        "0.61",
        "0.68",
        "47"
      ],
      [
        "Actual cues",
        "0.97",
        "0.98",
        "557"
      ]
    ],
    "id": "f494e100-00bf-4835-9c27-5bb78145ce1a",
    "claim": ", then randomly selected 5 examples were manually examined to understand the sources of errors, which was helpful in identifying issues in cue detection.",
    "label": "not enough info",
    "table_id": "08a8fe8c-92a2-41dc-a6e0-f97b95380cae"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
    "table_column_names": [
      "Cue",
      "[ITALIC] SCOPA",
      "[ITALIC] SB_COPA",
      "Diff.",
      "Prod."
    ],
    "table_content_values": [
      [
        "woman",
        "7.98",
        "4.84",
        "-3.14",
        "0.25"
      ],
      [
        "mother",
        "5.16",
        "3.95",
        "-1.21",
        "0.75"
      ],
      [
        "went",
        "6.00",
        "5.15",
        "-0.85",
        "0.73"
      ],
      [
        "down",
        "5.52",
        "4.93",
        "-0.58",
        "0.71"
      ],
      [
        "into",
        "4.07",
        "3.51",
        "-0.56",
        "0.40"
      ]
    ],
    "id": "565a7149-affb-4f76-b5fe-0cae0e82eacd",
    "claim": "the utterance in the first premise “The woman went down into the cellar” leads BERT-large to produce “The woman entered the cellar” and to choose the distractor rather than the correct premise.",
    "label": "not enough info",
    "table_id": "6609f902-7f48-422d-b5e9-6596405c71dd"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "0ee9a5d8-8b90-424c-9e68-f02437594591",
    "claim": "Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers.",
    "label": "supports",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 1: Image-caption ranking results for English (Multi30k)",
    "table_column_names": [
      "[EMPTY]",
      "Image to Text R@1",
      "Image to Text R@5",
      "Image to Text R@10",
      "Image to Text Mr",
      "Text to Image R@1",
      "Text to Image R@5",
      "Text to Image R@10",
      "Text to Image Mr",
      "Alignment"
    ],
    "table_content_values": [
      [
        "[BOLD] symmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Parallel gella:17",
        "31.7",
        "62.4",
        "74.1",
        "3",
        "24.7",
        "53.9",
        "65.7",
        "5",
        "-"
      ],
      [
        "UVS kiros:15",
        "23.0",
        "50.7",
        "62.9",
        "5",
        "16.8",
        "42.0",
        "56.5",
        "8",
        "-"
      ],
      [
        "EmbeddingNet wang:18",
        "40.7",
        "69.7",
        "79.2",
        "-",
        "29.2",
        "59.6",
        "71.7",
        "-",
        "-"
      ],
      [
        "sm-LSTM huang:17",
        "42.5",
        "71.9",
        "81.5",
        "2",
        "30.2",
        "60.4",
        "72.3",
        "3",
        "-"
      ],
      [
        "VSE++ faghri:18",
        "[BOLD] 43.7",
        "71.9",
        "82.1",
        "2",
        "32.3",
        "60.9",
        "72.1",
        "3",
        "-"
      ],
      [
        "Mono",
        "41.4",
        "74.2",
        "84.2",
        "2",
        "32.1",
        "63.0",
        "73.9",
        "3",
        "-"
      ],
      [
        "FME",
        "39.2",
        "71.1",
        "82.1",
        "2",
        "29.7",
        "62.5",
        "74.1",
        "3",
        "76.81%"
      ],
      [
        "AME",
        "43.5",
        "[BOLD] 77.2",
        "[BOLD] 85.3",
        "[BOLD] 2",
        "[BOLD] 34.0",
        "[BOLD] 64.2",
        "[BOLD] 75.4",
        "[BOLD] 3",
        "66.91%"
      ],
      [
        "[BOLD] asymmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Pivot gella:17",
        "33.8",
        "62.8",
        "75.2",
        "3",
        "26.2",
        "56.4",
        "68.4",
        "4",
        "-"
      ],
      [
        "Parallel gella:17",
        "31.5",
        "61.4",
        "74.7",
        "3",
        "27.1",
        "56.2",
        "66.9",
        "4",
        "-"
      ],
      [
        "Mono",
        "47.7",
        "77.1",
        "86.9",
        "2",
        "35.8",
        "66.6",
        "76.8",
        "3",
        "-"
      ],
      [
        "FME",
        "44.9",
        "76.9",
        "86.4",
        "2",
        "34.2",
        "66.1",
        "77.1",
        "3",
        "76.81%"
      ],
      [
        "AME",
        "[BOLD] 50.5",
        "[BOLD] 79.7",
        "[BOLD] 88.4",
        "[BOLD] 1",
        "[BOLD] 38.0",
        "[BOLD] 68.5",
        "[BOLD] 78.4",
        "[BOLD] 2",
        "73.10%"
      ]
    ],
    "id": "14c7a0f9-247b-4a40-bd54-5ad510a0a091",
    "claim": "AME performs better than FME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training.",
    "label": "supports",
    "table_id": "cc0ce7f1-f382-42ba-b615-186b6834cac5"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 2: Ratings of annotated NLDs by human judges.",
    "table_column_names": [
      "# steps",
      "Reachability",
      "Derivability Step 1",
      "Derivability Step 2",
      "Derivability Step 3"
    ],
    "table_content_values": [
      [
        "1",
        "3.0",
        "3.8",
        "-",
        "-"
      ],
      [
        "2",
        "2.8",
        "3.8",
        "3.7",
        "-"
      ],
      [
        "3",
        "2.3",
        "3.9",
        "3.8",
        "3.8"
      ]
    ],
    "id": "4a50e39b-a5db-4dce-b474-46fda3d1159b",
    "claim": "On the contrary, we found the quality of 3-step NLDs is relatively higher than the others.",
    "label": "refutes",
    "table_id": "8326a179-f543-4f97-8229-d2ed8172a663"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "<bold>Baselines</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>)",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "<bold>Model Variants</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "<bold>79.5</bold>"
      ]
    ],
    "id": "4cef41a6-d5a3-4308-9bc0-95a8154255d8",
    "claim": "Our joint model does not outperform all the base [CONTINUE] The results do not reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016).",
    "label": "refutes",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
    "table_column_names": [
      "[BOLD] Language pair",
      "[BOLD] Model type",
      "[BOLD] Oracle model",
      "[BOLD] Decoder configuration  [BOLD] Uniform",
      "[BOLD] Decoder configuration  [BOLD] BI + IS"
    ],
    "table_content_values": [
      [
        "es-en",
        "Unadapted",
        "36.4",
        "34.7",
        "36.6"
      ],
      [
        "es-en",
        "No-reg",
        "36.6",
        "34.8",
        "-"
      ],
      [
        "es-en",
        "EWC",
        "37.0",
        "36.3",
        "[BOLD] 37.2"
      ],
      [
        "en-de",
        "Unadapted",
        "36.4",
        "26.8",
        "38.8"
      ],
      [
        "en-de",
        "No-reg",
        "41.7",
        "31.8",
        "-"
      ],
      [
        "en-de",
        "EWC",
        "42.1",
        "38.6",
        "[BOLD] 42.0"
      ]
    ],
    "id": "965efd28-0126-4d8a-92dc-216b431dafaf",
    "claim": "BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU loss over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU loss over the approach described in Freitag and Al-Onaizan (2016).",
    "label": "refutes",
    "table_id": "a02493f5-adad-4462-a582-8a2cfe6f431d"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "e6163646-e624-431a-a99d-c4f2450a0183",
    "claim": "More importantly, their G-Pre and G-Rec scores are all above .50, which means that more than half of the good summaries identified by the metrics are actually good, and more than 50%.",
    "label": "refutes",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "047c6f79-94c1-4ebe-bf3d-a85e8dd82e60",
    "claim": "One interpretation for this difference is that under the simulated conversations with random reward function, GP-MBCM does not align well with the different human users.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. “Pref%”: in how many percentage of documents a system receives the higher human rating.",
    "table_column_names": [
      "Reward",
      "R-1",
      "R-2",
      "R-L",
      "Human",
      "Pref%"
    ],
    "table_content_values": [
      [
        "R-L (original)",
        "40.9",
        "17.8",
        "38.5",
        "1.75",
        "15"
      ],
      [
        "Learned (ours)",
        "39.2",
        "17.4",
        "37.5",
        "[BOLD] 2.20",
        "[BOLD] 75"
      ]
    ],
    "id": "7e741dca-daea-49be-9ccc-ca533cf8b802",
    "claim": "Again, when ROUGE is used as rewards, the generated summaries have higher ROUGE scores.",
    "label": "supports",
    "table_id": "f5343270-2393-4ff2-85d1-a445b36215ea"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "73207db1-84fa-40bf-b763-0c0b9f859942",
    "claim": "This suggests that graph encoders based on gating mechanisms are very effective in text generation models.",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "961db06c-7cce-438a-ad9b-89e45a05da2a",
    "claim": "the models more often fail to realise part of the MR, rather than hallucinating additional information.",
    "label": "supports",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "b8186df4-979b-4b04-bb09-a484f0e6dfd6",
    "claim": "These results show significant performance improvement by using Predicate Schemas knowledge on hard coreference problems.",
    "label": "supports",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "3ab43867-7cf7-491e-ba17-95ea2d79dfce",
    "claim": "This is expected, since the questions in the SQuAD and QA-SRL datasets tend to be very different (more declarative in the former, more interrogative in the latter).",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "17.3",
        "0.828"
      ],
      [
        "Wiener filter",
        "19.5",
        "0.722"
      ],
      [
        "Minimizing DCE",
        "15.8",
        "[BOLD] 0.269"
      ],
      [
        "FSEGAN",
        "14.9",
        "0.291"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "15.6",
        "0.330"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 14.4",
        "0.303"
      ],
      [
        "Clean speech",
        "5.7",
        "0.0"
      ]
    ],
    "id": "0150f1a0-fe1d-4497-8489-a649003ab619",
    "claim": "The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE.",
    "label": "refutes",
    "table_id": "3c6c1f56-ed0b-418a-ac6c-6449488a89e9"
  },
  {
    "paper": "Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches",
    "paper_id": "1904.01172v3",
    "table_caption": "Table 2: Comparison of exact-match accuracy achieved on selected benchmarks by a random or majority-choice baseline, various neural contextual embedding models, and humans. ELMo refers to the highest-performing listed approach using ELMo embeddings. Best system performance on each benchmark in bold. Information extracted from leaderboards (linked to in the first column) at time of writing (October 2019), and original papers for benchmarks introduced in Section 2.",
    "table_column_names": [
      "[BOLD] Benchmark",
      "[BOLD]  Simple Baseline ",
      "[BOLD] ELMo",
      "[BOLD] GPT",
      "[BOLD] BERT",
      "[BOLD] MT-DNN",
      "[BOLD] XLNet",
      "[BOLD] RoBERTa",
      "[BOLD] ALBERT",
      "[BOLD] Human"
    ],
    "table_content_values": [
      [
        "[BOLD] CLOTH",
        "25.0",
        "70.7",
        "–",
        "[BOLD] 86.0",
        "–",
        "–",
        "–",
        "–",
        "85.9"
      ],
      [
        "[BOLD] Cosmos QA",
        "–",
        "–",
        "54.5",
        "67.1",
        "–",
        "–",
        "–",
        "–",
        "94.0"
      ],
      [
        "[BOLD] DREAM",
        "33.4",
        "59.5",
        "55.5",
        "66.8",
        "–",
        "[BOLD] 72.0",
        "–",
        "–",
        "95.5"
      ],
      [
        "[BOLD] GLUE",
        "–",
        "70.0",
        "–",
        "80.5",
        "87.6",
        "88.4",
        "88.5",
        "[BOLD] 89.4",
        "87.1"
      ],
      [
        "[BOLD] HellaSWAG",
        "25.0",
        "33.3",
        "41.7",
        "47.3",
        "–",
        "–",
        "[BOLD] 85.2",
        "[EMPTY]",
        "95.6"
      ],
      [
        "[BOLD] MC-TACO",
        "17.4",
        "26.4",
        "–",
        "42.7",
        "–",
        "–",
        "[BOLD] 43.6",
        "–",
        "75.8"
      ],
      [
        "[BOLD] RACE",
        "24.9",
        "–",
        "59.0",
        "72.0",
        "–",
        "81.8",
        "83.2",
        "[BOLD] 89.4",
        "94.5"
      ],
      [
        "[BOLD] SciTail",
        "60.3",
        "–",
        "88.3",
        "–",
        "94.1",
        "–",
        "–",
        "–",
        "–"
      ],
      [
        "[BOLD] SQuAD 1.1",
        "1.3",
        "81.0",
        "–",
        "87.4",
        "–",
        "[BOLD] 89.9",
        "–",
        "–",
        "82.3"
      ],
      [
        "[BOLD] SQuAD 2.0",
        "48.9",
        "63.4",
        "–",
        "80.8",
        "–",
        "86.3",
        "86.8",
        "[BOLD] 89.7",
        "86.9"
      ],
      [
        "[BOLD] SuperGLUE",
        "47.1",
        "–",
        "–",
        "69.0",
        "–",
        "–",
        "[BOLD] 84.6",
        "–",
        "89.8"
      ],
      [
        "[BOLD] SWAG",
        "25.0",
        "59.1",
        "78.0",
        "86.3",
        "87.1",
        "–",
        "[BOLD] 89.9",
        "–",
        "88.0"
      ]
    ],
    "id": "4667459d-519c-4af0-9060-21a22cd745f1",
    "claim": "The most representative models are ELMO, GPT, BERT and its variants, and XLNET.",
    "label": "supports",
    "table_id": "03f667dc-57c4-41c7-a8fa-00978b4ca84f"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "81c0bde7-fe67-476c-bf82-5fe7def3b1f3",
    "claim": "In general, increasing the number of GCN layers from 2 to 9 boosts the model performance.",
    "label": "supports",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "<bold>Baselines</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>)",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "<bold>Model Variants</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "<bold>79.5</bold>"
      ]
    ],
    "id": "6d133fca-e9b6-4507-ba72-5a65564bf8da",
    "claim": "The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model.",
    "label": "refutes",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection",
    "paper_id": "1904.04388v1",
    "table_caption": "Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Model",
      "[BOLD] dev mean",
      "[BOLD] dev best",
      "[BOLD] test mean",
      "[BOLD] test best",
      "[ITALIC] α"
    ],
    "table_content_values": [
      [
        "single",
        "text",
        "86.54",
        "86.80",
        "86.47",
        "86.96",
        "–"
      ],
      [
        "single",
        "raw",
        "35.00",
        "37.33",
        "35.78",
        "37.70",
        "–"
      ],
      [
        "single",
        "innovations",
        "80.86",
        "81.51",
        "80.28",
        "82.15",
        "–"
      ],
      [
        "early",
        "text + raw",
        "86.46",
        "86.65",
        "86.24",
        "86.53",
        "–"
      ],
      [
        "early",
        "text + innovations",
        "86.53",
        "86.77",
        "86.54",
        "87.00",
        "–"
      ],
      [
        "early",
        "text + raw + innovations",
        "86.35",
        "86.69",
        "86.55",
        "86.44",
        "–"
      ],
      [
        "late",
        "text + raw",
        "86.71",
        "87.05",
        "86.35",
        "86.71",
        "0.2"
      ],
      [
        "late",
        "text + innovations",
        "[BOLD] 86.98",
        "[BOLD] 87.48",
        "[BOLD] 86.68",
        "[BOLD] 87.02",
        "0.5"
      ],
      [
        "late",
        "text + raw + innovations",
        "86.95",
        "87.30",
        "86.60",
        "86.87",
        "0.5"
      ]
    ],
    "id": "0fbadeff-af49-4236-b0b4-749c3e102f94",
    "claim": "[CONTINUE] We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average.",
    "label": "supports",
    "table_id": "f0441bd9-731a-41df-8a83-1d084b332e89"
  },
  {
    "paper": "Zero-Shot Grounding of Objects from Natural Language Queries",
    "paper_id": "1908.07129v1",
    "table_caption": "Table 3: Category-wise performance with the default split of Flickr30k Entities.",
    "table_column_names": [
      "Method",
      "Overall",
      "people",
      "clothing",
      "bodyparts",
      "animals",
      "vehicles",
      "instruments",
      "scene",
      "other"
    ],
    "table_content_values": [
      [
        "QRC - VGG(det)",
        "60.21",
        "75.08",
        "55.9",
        "20.27",
        "73.36",
        "68.95",
        "45.68",
        "65.27",
        "38.8"
      ],
      [
        "CITE - VGG(det)",
        "61.89",
        "[BOLD] 75.95",
        "58.50",
        "30.78",
        "[BOLD] 77.03",
        "[BOLD] 79.25",
        "48.15",
        "58.78",
        "43.24"
      ],
      [
        "ZSGNet - VGG (cls)",
        "60.12",
        "72.52",
        "60.57",
        "38.51",
        "63.61",
        "64.47",
        "49.59",
        "64.66",
        "41.09"
      ],
      [
        "ZSGNet - Res50 (cls)",
        "[BOLD] 63.39",
        "73.87",
        "[BOLD] 66.18",
        "[BOLD] 45.27",
        "73.79",
        "71.38",
        "[BOLD] 58.54",
        "[BOLD] 66.49",
        "[BOLD] 45.53"
      ]
    ],
    "id": "c45cc229-e5f8-4a18-b769-42397cd1f57d",
    "claim": "[CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\").",
    "label": "supports",
    "table_id": "347fd2b9-f84e-4288-a2e3-b793a47f6266"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "57a46ccf-9c16-4b99-a81c-e0ca346de3af",
    "claim": "Coverage helps the model improve its EM by 1.5 and its F1 by 0.5.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "9d83016c-5c81-49d2-9e74-dde587642c9c",
    "claim": "models with NSP performance drop a lot when trained with COPA.",
    "label": "not enough info",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "STS12",
      "STS13",
      "STS14",
      "STS15",
      "STS16"
    ],
    "table_content_values": [
      [
        "CBOW",
        "43.5",
        "[BOLD] 50.0",
        "[BOLD] 57.7",
        "[BOLD] 63.2",
        "61.0"
      ],
      [
        "CMOW",
        "39.2",
        "31.9",
        "38.7",
        "49.7",
        "52.2"
      ],
      [
        "Hybrid",
        "[BOLD] 49.6",
        "46.0",
        "55.1",
        "62.4",
        "[BOLD] 62.1"
      ],
      [
        "cmp. CBOW",
        "+14.6%",
        "-8%",
        "-4.5%",
        "-1.5%",
        "+1.8%"
      ],
      [
        "cmp. CMOW",
        "+26.5%",
        "+44.2%",
        "+42.4",
        "+25.6%",
        "+19.0%"
      ]
    ],
    "id": "8d3edac1-1144-4e49-ab95-1d0898c2acaf",
    "claim": "The hybrid model is able to repair this deficit, reducing the difference to 8%.",
    "label": "supports",
    "table_id": "f3ae0058-2624-4cd3-a19a-389a8be5d740"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "7d863838-7856-49cb-b150-588aa3ed26c2",
    "claim": "our framework captures more information about the intended semantic feature.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "The MeMAD Submission to the WMT18 Multimodal Translation Task",
    "paper_id": "1808.10802v2",
    "table_caption": "Table 4: Adding automatic image captions (only the best one or all 5). The table shows BLEU scores in %. All results with Marian Amun.",
    "table_column_names": [
      "en-fr",
      "flickr16",
      "flickr17",
      "mscoco17"
    ],
    "table_content_values": [
      [
        "multi30k",
        "61.4",
        "54.0",
        "43.1"
      ],
      [
        "+autocap (dual attn.)",
        "60.9",
        "52.9",
        "43.3"
      ],
      [
        "+autocap 1 (concat)",
        "61.7",
        "53.7",
        "43.9"
      ],
      [
        "+autocap 1-5 (concat)",
        "[BOLD] 62.2",
        "[BOLD] 54.4",
        "[BOLD] 44.1"
      ],
      [
        "en-de",
        "flickr16",
        "flickr17",
        "mscoco17"
      ],
      [
        "multi30k",
        "38.9",
        "32.0",
        "27.7"
      ],
      [
        "+autocap (dual attn.)",
        "37.8",
        "30.2",
        "27.0"
      ],
      [
        "+autocap 1 (concat)",
        "39.7",
        "[BOLD] 32.2",
        "[BOLD] 28.8"
      ],
      [
        "+autocap 1-5 (concat)",
        "[BOLD] 39.9",
        "32.0",
        "28.7"
      ]
    ],
    "id": "6ff909e7-efc8-4f69-a140-ebb18d859825",
    "claim": "We can see that the dual attention model does not work at all and the scores slightly drop.",
    "label": "supports",
    "table_id": "286f811a-7b07-42f1-8793-b576a7d24a2a"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "04896b77-6ed9-48a4-bff4-6200b7bd4ec6",
    "claim": "in terms of correctness, the averaged Ok rate on all 15 decisions is 44.3%",
    "label": "not enough info",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "ccc54cf1-8e27-4c66-868b-a174db35f0bb",
    "claim": "The results in Table 7 show that the proposed method is not as effective as the state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction.",
    "label": "refutes",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "paper_id": "1909.03546v2",
    "table_caption": "Table 7: In-domain pre-training: SciBERT vs. BERT",
    "table_column_names": [
      "[EMPTY]",
      "SciERC Entity",
      "SciERC Relation",
      "GENIA Entity"
    ],
    "table_content_values": [
      [
        "Best BERT",
        "69.8",
        "41.9",
        "78.4"
      ],
      [
        "Best SciBERT",
        "[BOLD] 72.0",
        "[BOLD] 45.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "8024d1af-5c05-4fe3-8254-ce70860006b0",
    "claim": "SciBERT does not significantly boost performance for scientific datasets including SciERC and GENIA.",
    "label": "refutes",
    "table_id": "86acff26-758c-4082-84b7-900a9fafa2ef"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "e0ae18f4-9bba-4ce9-83da-01228c8b4f30",
    "claim": "Our single model is not comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs.",
    "label": "refutes",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "31933281-32c2-4d77-9884-37546c8599f8",
    "claim": "The results for testing on cleaned data (Table 3, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf.",
    "label": "supports",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "3f5ef015-47e3-4ccd-abc8-9db195e5a363",
    "claim": "If we check the relative ranks of the good summaries according to the metrics (row 1), for example for ROUGE-SU4, we see that 98.4% of them belong to the top 25% summaries in the metric.",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "769e0a86-2bc0-4010-8be4-e9b42e868bed",
    "claim": "we observe that MQAN (RAE-based) suffers most without coverage: in all out-of-domain settings it underperforms the original.",
    "label": "not enough info",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "30256121-6d2d-467e-b640-e752b29ffb03",
    "claim": "( 2019).",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "Baseline (No SA)Anderson et al. ( 2018 )",
        "55.00",
        "0M"
      ],
      [
        "SA (S: 1,2,3 - B: 1)",
        "55.11",
        "} 0.107M"
      ],
      [
        "SA (S: 1,2,3 - B: 2)",
        "55.17",
        "} 0.107M"
      ],
      [
        "[BOLD] SA (S: 1,2,3 - B: 3)",
        "[BOLD] 55.27",
        "} 0.107M"
      ]
    ],
    "id": "e91d427d-29e5-46b3-a123-ba6111eb0525",
    "claim": "We notice no significant improvements relative to the baseline showing that self-attention alone does not improve the VQA task.",
    "label": "refutes",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 5: Results of the Human Rating on CWC.",
    "table_column_names": [
      "System",
      "Succ. (%)",
      "Smoothness"
    ],
    "table_content_values": [
      [
        "Retrieval-Stgy ",
        "54.0",
        "2.48"
      ],
      [
        "PMI ",
        "46.0",
        "2.56"
      ],
      [
        "Neural ",
        "36.0",
        "2.50"
      ],
      [
        "Kernel ",
        "58.0",
        "2.48"
      ],
      [
        "DKRN (ours)",
        "[BOLD] 88.0",
        "[BOLD] 3.22"
      ]
    ],
    "id": "d0e62762-04dc-4281-b31e-d45d3580665e",
    "claim": "Our DKRN agent outperforms all other agents with a large margin.",
    "label": "supports",
    "table_id": "72fa0d72-3d74-45f3-a8d4-df9ee3f08663"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "4c635753-fc61-423d-a4b9-cbb47e724697",
    "claim": "When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.8% F1 score improvement (A2−A1).",
    "label": "supports",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "621a2ffa-a852-4a18-87d4-c5312befefd5",
    "claim": "These results show no significant performance improvement by using Predicate Schemas knowledge on hard coreference problems.",
    "label": "refutes",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
    "table_column_names": [
      "Schema",
      "AntePre(Test)",
      "AntePre(Train)"
    ],
    "table_content_values": [
      [
        "Type 1",
        "76.67",
        "86.79"
      ],
      [
        "Type 2",
        "79.55",
        "88.86"
      ],
      [
        "Type 1 (Cat1)",
        "90.26",
        "93.64"
      ],
      [
        "Type 2 (Cat2)",
        "83.38",
        "92.49"
      ]
    ],
    "id": "65f4f4b5-a857-4cff-a33a-12fbad54d0fd",
    "claim": "Also, the performance drop between Cat1/Cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement.",
    "label": "supports",
    "table_id": "18072983-1b23-4da0-8e08-1b6a92fbc124"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "b27fa344-5820-4aed-a2a9-228427b26999",
    "claim": "In contrast, models in the lower portion (7-12) involve dialogue states, which is estimated using Belief Tracker, or in other words, by models in the upper portion.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "On the difficulty of a distributional semantics of spoken language",
    "paper_id": "1803.08869v2",
    "table_caption": "Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
    "table_column_names": [
      "[EMPTY]",
      "Recall@10 (%)",
      "Median rank",
      "RSAimage"
    ],
    "table_content_values": [
      [
        "VGS",
        "27",
        "6",
        "0.4"
      ],
      [
        "SegMatch",
        "[BOLD] 10",
        "[BOLD] 37",
        "[BOLD] 0.5"
      ],
      [
        "Audio2vec-U",
        "5",
        "105",
        "0.0"
      ],
      [
        "Audio2vec-C",
        "2",
        "647",
        "0.0"
      ],
      [
        "Mean MFCC",
        "1",
        "1,414",
        "0.0"
      ],
      [
        "Chance",
        "0",
        "3,955",
        "0.0"
      ]
    ],
    "id": "9f36b848-b4e8-449c-aa79-3a4ed3e10d71",
    "claim": "Audio2vec works better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space.",
    "label": "supports",
    "table_id": "087b26ab-9679-4d8d-b96c-57140c1a8b7b"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] R-1",
      "[BOLD] R-2",
      "[BOLD] R-SU"
    ],
    "table_content_values": [
      [
        "First-1",
        "26.83",
        "7.25",
        "6.46"
      ],
      [
        "First-2",
        "35.99",
        "10.17",
        "12.06"
      ],
      [
        "First-3",
        "39.41",
        "11.77",
        "14.51"
      ],
      [
        "LexRank Erkan and Radev ( 2004 )",
        "38.27",
        "12.70",
        "13.20"
      ],
      [
        "TextRank Mihalcea and Tarau ( 2004 )",
        "38.44",
        "13.10",
        "13.50"
      ],
      [
        "MMR Carbonell and Goldstein ( 1998 )",
        "38.77",
        "11.98",
        "12.91"
      ],
      [
        "PG-Original Lebanoff et al. ( 2018 )",
        "41.85",
        "12.91",
        "16.46"
      ],
      [
        "PG-MMR Lebanoff et al. ( 2018 )",
        "40.55",
        "12.36",
        "15.87"
      ],
      [
        "PG-BRNN Gehrmann et al. ( 2018 )",
        "42.80",
        "14.19",
        "16.75"
      ],
      [
        "CopyTransformer Gehrmann et al. ( 2018 )",
        "[BOLD] 43.57",
        "14.03",
        "17.37"
      ],
      [
        "Hi-MAP (Our Model)",
        "43.47",
        "[BOLD] 14.89",
        "[BOLD] 17.41"
      ]
    ],
    "id": "3ccac9a1-709e-4f2a-a485-b6ecf90417cc",
    "claim": "Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model).",
    "label": "supports",
    "table_id": "6f86cef3-2ee0-4782-b5b4-f37400c22dfb"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "f47e2f72-62c7-485b-b3f5-72e99c6fd3b8",
    "claim": "their informative and match scores are higher than ours since they prioritize the dialog turn to show referents, while we take into account various factors in dialog quality.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training"
    ],
    "table_content_values": [
      [
        "Batch size",
        "Iter",
        "Recur",
        "Fold",
        "Iter",
        "Recur",
        "Fold"
      ],
      [
        "1",
        "19.2",
        "81.4",
        "16.5",
        "2.5",
        "4.8",
        "9.0"
      ],
      [
        "10",
        "49.3",
        "217.9",
        "52.2",
        "4.0",
        "4.2",
        "37.5"
      ],
      [
        "25",
        "72.1",
        "269.9",
        "61.6",
        "5.5",
        "3.6",
        "54.7"
      ]
    ],
    "id": "566a37c6-ec13-48a6-a1b3-116b99739810",
    "claim": "As a result, the recursive approach performs better than the folding technique for the training task.",
    "label": "refutes",
    "table_id": "82989071-ae58-4870-9a3b-7e5f7a1ea4e7"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 2: Ratings of annotated NLDs by human judges.",
    "table_column_names": [
      "# steps",
      "Reachability",
      "Derivability Step 1",
      "Derivability Step 2",
      "Derivability Step 3"
    ],
    "table_content_values": [
      [
        "1",
        "3.0",
        "3.8",
        "-",
        "-"
      ],
      [
        "2",
        "2.8",
        "3.8",
        "3.7",
        "-"
      ],
      [
        "3",
        "2.3",
        "3.9",
        "3.8",
        "3.8"
      ]
    ],
    "id": "ceeea11f-c920-442d-8462-46a12e693a9c",
    "claim": "The evaluation results shown in Table 2 indicate that the annotated NLDs are of high quality (Reachability), and each NLD is properly derived from supporting documents (Derivability).",
    "label": "supports",
    "table_id": "8326a179-f543-4f97-8229-d2ed8172a663"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "f32c052e-7348-4b5c-86e9-8376b541c61d",
    "claim": "TF and DF achieved almost the same values of precision, recall and f-measure using the English corpora, achieving the same value of precision (P=0.0150) and f-measure (F=0.0293) when using the Europarl corpus in English.",
    "label": "supports",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "fd4cae40-9362-44da-a07e-14659a6dcbcf",
    "claim": "Table 5 summarizes the above experimental results on the affected domain in terms of the number of dialog turns, and the numbers of inform, match, and success actions.",
    "label": "not enough info",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Keyphrase Generation for Scientific Articles using GANs",
    "paper_id": "1909.12229v1",
    "table_caption": "Table 2: α-nDCG@5 metrics",
    "table_column_names": [
      "Model",
      "Inspec",
      "Krapivin",
      "NUS",
      "KP20k"
    ],
    "table_content_values": [
      [
        "Catseq",
        "0.87803",
        "0.781",
        "0.82118",
        "0.804"
      ],
      [
        "Catseq-RL",
        "0.8602",
        "[BOLD] 0.786",
        "0.83",
        "0.809"
      ],
      [
        "GAN",
        "[BOLD] 0.891",
        "0.771",
        "[BOLD] 0.853",
        "[BOLD] 0.85"
      ]
    ],
    "id": "9dcfd95a-e2f4-426a-8ccc-f49889d05521",
    "claim": "Our model obtains the best performance on three out of the four datasets.",
    "label": "supports",
    "table_id": "db0cb6ad-ade2-4926-8a07-f92a9d74c055"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "7e3de05e-1093-426a-9ad5-e6929654530d",
    "claim": "The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA).",
    "label": "supports",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
    "table_column_names": [
      "[BOLD] Training data",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] Disfl"
    ],
    "table_content_values": [
      [
        "Original",
        "0",
        "22",
        "0",
        "14"
      ],
      [
        "Cleaned added",
        "0",
        "23",
        "0",
        "14"
      ],
      [
        "Cleaned missing",
        "0",
        "1",
        "0",
        "2"
      ],
      [
        "Cleaned",
        "0",
        "0",
        "0",
        "5"
      ]
    ],
    "id": "9ca997f9-4c36-49e9-b0ed-d0b1bcbdcee6",
    "claim": "The results in Table 4 confirm the findings of the automatic [CONTINUE] metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other.",
    "label": "supports",
    "table_id": "82ce68c2-64df-452b-ac0b-064c7fdaefa8"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "d611540c-f325-4180-b442-77996127c1a8",
    "claim": "We see clear benefits of the coverage mechanism in the out-of-domain setting, especially in the low-resource case of QA-SRL.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "5f50a2cd-c027-4ecb-b169-e26fcc59005f",
    "claim": "It does not improve by over 20% over a state-of-art general coreference system on Winograd and also does not outperform Rahman and Ng (2012) by a margin of 3.3%.",
    "label": "refutes",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 3: ARI and Silhouette coefficient scores.",
    "table_column_names": [
      "Methods",
      "Seanad Abolition ARI",
      "Seanad Abolition  [ITALIC] Sil",
      "Video Games ARI",
      "Video Games  [ITALIC] Sil",
      "Pornography ARI",
      "Pornography  [ITALIC] Sil"
    ],
    "table_content_values": [
      [
        "TF-IDF",
        "0.23",
        "0.02",
        "-0.01",
        "0.01",
        "-0.02",
        "0.01"
      ],
      [
        "WMD",
        "0.09",
        "0.01",
        "0.01",
        "0.01",
        "-0.02",
        "0.01"
      ],
      [
        "Sent2vec",
        "-0.01",
        "-0.01",
        "0.11",
        "0.06",
        "0.01",
        "0.02"
      ],
      [
        "Doc2vec",
        "-0.01",
        "-0.03",
        "-0.01",
        "0.01",
        "0.02",
        "-0.01"
      ],
      [
        "BERT",
        "0.03",
        "-0.04",
        "0.08",
        "0.05",
        "-0.01",
        "0.03"
      ],
      [
        "OD-parse",
        "0.01",
        "-0.04",
        "-0.01",
        "0.02",
        "0.07",
        "0.05"
      ],
      [
        "OD",
        "[BOLD] 0.54",
        "[BOLD] 0.31",
        "[BOLD] 0.56",
        "[BOLD] 0.42",
        "[BOLD] 0.41",
        "[BOLD] 0.41"
      ]
    ],
    "id": "67b478a8-a83d-4700-8567-f8550bcce109",
    "claim": "among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec achieving ARI and Silhouette coefficient scores of close to zero on the \"Video Games\" and \"Pornography\" datasets (barely providing a performance improvement over random clustering, i.e., a zero ARI score).",
    "label": "supports",
    "table_id": "41274426-d552-4e87-b3ba-efed0c4b05ab"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "acab10a0-f7f1-409b-89d5-9e8b256c3c2d",
    "claim": "In particular, our single DCGCN model does not consistently outperform Seq2Seq models when trained without external resources.",
    "label": "refutes",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "ce13f0c9-f310-4589-ab1a-1ab472a1d338",
    "claim": "Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
    "label": "supports",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "e5169b8b-aa8f-41af-aaaa-634c71406de9",
    "claim": "Furthermore, we do not see over-fitting in either of the models, even if they are trained on all the data in B-COPA.",
    "label": "not enough info",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "8aac774b-9ded-41b0-8070-26614c5200f2",
    "claim": "The difference between accuracy on Easy and Hard is less pronounced for RoBERTa, but still suggests some reliance on superficial cues.",
    "label": "supports",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "4cf92df9-f1dd-4f9c-b370-cae9598acf82",
    "claim": "However, in the all questions set which includes a large percentage of questions without concept words (containing antonym words), the proposed model underperforms GloVe",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "4e46bf68-7ddc-4376-b0b7-b5143661ea93",
    "claim": "The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is not effective for both WOMs and SER, whereas the SC-LSTM seems to have difficulty scaling to the E2E dataset.",
    "label": "refutes",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "1984f5e0-9737-4eac-8b4e-f3ed389145ee",
    "claim": "over the different entity types, our joint model performs best in within-document coreference.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability",
    "paper_id": "1912.12628v1",
    "table_caption": "Table 1: Accuracy obtained by training an standalone classifier, applying the API and the proposed wrapper for each domain",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] BB source acc.",
      "[BOLD] BB target acc.",
      "[BOLD] Non-reject. acc. (10/20/30%)",
      "[BOLD] Class. quality (10/20/30%)",
      "[BOLD] Reject. quality (10/20/30%)"
    ],
    "table_content_values": [
      [
        "[BOLD] Apply Yelp BB to SST-2",
        "89.18±0.08%",
        "77.13±0.52%",
        "82.43±0.22% 88.19±0.50% 93.60±0.16%",
        "80.40±0.39% 83.11±0.80% 83.05±0.23%",
        "6.03±0.45 6.04±0.51 4.97±0.07"
      ],
      [
        "[BOLD] Apply SST-2 BB to Yelp",
        "83.306±0.18%",
        "82.106±0.88%",
        "87,98±0.18% 92.13±0.38% 94.19±0.33%",
        "85.49±0.88% 84.53±0.38% 78.99±0.46%",
        "8.30±1.63 5.72±0.27 3.73±0.10"
      ],
      [
        "[BOLD] Apply Electronics BB to Music",
        "86.39±0.22%",
        "90.38±0.13%",
        "95.04±0.43% 96.45±0.35% 97.26±0.31%",
        "90.67±0.88% 83.93±0.67% 75.77±0.54%",
        "10.7±1.65 4.82±0.35 3.25±0.14"
      ],
      [
        "[BOLD] Apply Music BB to Electronics",
        "93.10±0.02%",
        "79.85±0.0%",
        "83.26±0.41% 87.06±0.55% 90.50±0.29%",
        "79.97±0.74% 79.93±0.87% 76.81±0.41%",
        "4.1±0.55 3.80±0.35 3.32±0.09"
      ]
    ],
    "id": "b70cedda-3704-43f2-9ec0-cbf50e7bab85",
    "claim": "In general terms, the results displayed in table 1 show that the rejection method cannot reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain.",
    "label": "refutes",
    "table_id": "306969fe-d4f7-4a0d-a211-e63a8af0368a"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "0d4a8de9-4bda-499e-a51c-870a1fed2a55",
    "claim": "Under oracle setup, all models are notably improved due to the higher quality of reranked passages, but our model does not achieve statistically significantly better BLEU scores.",
    "label": "refutes",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
    "table_column_names": [
      "System",
      "MUC",
      "BCUB",
      "CEAFe",
      "AVG"
    ],
    "table_content_values": [
      [
        "ACE",
        "ACE",
        "ACE",
        "ACE",
        "ACE"
      ],
      [
        "IlliCons",
        "[BOLD] 78.17",
        "81.64",
        "[BOLD] 78.45",
        "[BOLD] 79.42"
      ],
      [
        "KnowComb",
        "77.51",
        "[BOLD] 81.97",
        "77.44",
        "78.97"
      ],
      [
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes"
      ],
      [
        "IlliCons",
        "84.10",
        "[BOLD] 78.30",
        "[BOLD] 68.74",
        "[BOLD] 77.05"
      ],
      [
        "KnowComb",
        "[BOLD] 84.33",
        "78.02",
        "67.95",
        "76.76"
      ]
    ],
    "id": "d661490a-948e-4b22-ad8e-4d11b28c00cb",
    "claim": "Our KnowComb system does not achieve the same level of performance as the state-of-art general coreference system we base it on.",
    "label": "refutes",
    "table_id": "cbbb2b74-fff2-4db8-a6be-b6a395d77483"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "d18651b7-1296-4f52-9b4b-fef0290b7507",
    "claim": "the substantial drop in accuracy can be attributed to the different train-test split.",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 1: Precisions on the NYT dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "0.4",
      "AUC"
    ],
    "table_content_values": [
      [
        "PCNN+ATT",
        "0.698",
        "0.606",
        "0.518",
        "0.446",
        "0.323"
      ],
      [
        "Rank+ExATT",
        "0.789",
        "0.726",
        "0.620",
        "0.514",
        "0.395"
      ],
      [
        "Our Model",
        "0.788",
        "[BOLD] 0.743",
        "[BOLD] 0.654",
        "[BOLD] 0.546",
        "[BOLD] 0.397"
      ]
    ],
    "id": "c3ab8958-374d-4ec8-9c98-275829bee11f",
    "claim": "the performance of Our Model is better than Rank+ExATT at most recall ratios, which indicates the importance of our match function with fine-grained entity identification.",
    "label": "not enough info",
    "table_id": "f4fa37fb-36c7-4ace-b563-6bd07bd3cf9e"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)",
    "table_column_names": [
      "System",
      "All LOC",
      "All ORG",
      "All PER",
      "All MISC",
      "In  [ITALIC] E+ LOC",
      "In  [ITALIC] E+ ORG",
      "In  [ITALIC] E+ PER",
      "In  [ITALIC] E+ MISC"
    ],
    "table_content_values": [
      [
        "Name matching",
        "96.26",
        "89.48",
        "57.38",
        "96.60",
        "92.32",
        "76.87",
        "47.40",
        "76.29"
      ],
      [
        "MIL",
        "57.09",
        "[BOLD] 76.30",
        "41.35",
        "93.35",
        "11.90",
        "[BOLD] 47.90",
        "27.60",
        "53.61"
      ],
      [
        "MIL-ND",
        "57.15",
        "77.15",
        "35.95",
        "92.47",
        "12.02",
        "49.77",
        "20.94",
        "47.42"
      ],
      [
        "[ITALIC] τMIL-ND",
        "[BOLD] 55.15",
        "76.56",
        "[BOLD] 34.03",
        "[BOLD] 92.15",
        "[BOLD] 11.14",
        "51.18",
        "[BOLD] 20.59",
        "[BOLD] 40.00"
      ],
      [
        "Supervised learning",
        "55.58",
        "61.32",
        "24.98",
        "89.96",
        "8.80",
        "14.95",
        "7.40",
        "29.90"
      ]
    ],
    "id": "45d2802f-6f82-4ee9-96f9-462ec333cbc2",
    "claim": "[CONTINUE] For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%.",
    "label": "supports",
    "table_id": "8ee12371-dc8c-4338-acf5-528fb3099f41"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
    "table_column_names": [
      "[EMPTY]",
      "Difference Function",
      "Seanad Abolition",
      "Video Games",
      "Pornography"
    ],
    "table_content_values": [
      [
        "OD-parse",
        "Absolute",
        "0.01",
        "-0.01",
        "0.07"
      ],
      [
        "OD-parse",
        "JS div.",
        "0.01",
        "-0.01",
        "-0.01"
      ],
      [
        "OD-parse",
        "EMD",
        "0.07",
        "0.01",
        "-0.01"
      ],
      [
        "OD",
        "Absolute",
        "[BOLD] 0.54",
        "[BOLD] 0.56",
        "[BOLD] 0.41"
      ],
      [
        "OD",
        "JS div.",
        "0.07",
        "-0.01",
        "-0.02"
      ],
      [
        "OD",
        "EMD",
        "0.26",
        "-0.01",
        "0.01"
      ],
      [
        "OD (no polarity shifters)",
        "Absolute",
        "0.23",
        "0.08",
        "0.04"
      ],
      [
        "OD (no polarity shifters)",
        "JS div.",
        "0.09",
        "-0.01",
        "-0.02"
      ],
      [
        "OD (no polarity shifters)",
        "EMD",
        "0.10",
        "0.01",
        "-0.01"
      ]
    ],
    "id": "8c6099dc-368a-44c2-8051-2af00a3c8bdd",
    "claim": "[CONTINUE] Sentiment polarity shifters have a high impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" hurts the Opinion Representation phase, and thereby leads to incorrect computation of opinion distance.",
    "label": "supports",
    "table_id": "fba0fe3a-58cc-47a0-a3ab-5d091d2b7fe7"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
    "table_column_names": [
      "[EMPTY]",
      "Difference Function",
      "Seanad Abolition",
      "Video Games",
      "Pornography"
    ],
    "table_content_values": [
      [
        "OD-parse",
        "Absolute",
        "0.01",
        "-0.01",
        "0.07"
      ],
      [
        "OD-parse",
        "JS div.",
        "0.01",
        "-0.01",
        "-0.01"
      ],
      [
        "OD-parse",
        "EMD",
        "0.07",
        "0.01",
        "-0.01"
      ],
      [
        "OD",
        "Absolute",
        "[BOLD] 0.54",
        "[BOLD] 0.56",
        "[BOLD] 0.41"
      ],
      [
        "OD",
        "JS div.",
        "0.07",
        "-0.01",
        "-0.02"
      ],
      [
        "OD",
        "EMD",
        "0.26",
        "-0.01",
        "0.01"
      ],
      [
        "OD (no polarity shifters)",
        "Absolute",
        "0.23",
        "0.08",
        "0.04"
      ],
      [
        "OD (no polarity shifters)",
        "JS div.",
        "0.09",
        "-0.01",
        "-0.02"
      ],
      [
        "OD (no polarity shifters)",
        "EMD",
        "0.10",
        "0.01",
        "-0.01"
      ]
    ],
    "id": "aa9eb3ce-acdd-4019-b442-f8b929d15b84",
    "claim": "[CONTINUE] OD does not significantly outperform OD-parse: We observe that compared to OD-parse, OD is not significantly more accurate.",
    "label": "refutes",
    "table_id": "fba0fe3a-58cc-47a0-a3ab-5d091d2b7fe7"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
    "table_column_names": [
      "Metric",
      "Method of validation",
      "Yelp",
      "Lit."
    ],
    "table_content_values": [
      [
        "Acc",
        "% of machine and human judgments that match",
        "94",
        "84"
      ],
      [
        "Sim",
        "Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation",
        "0.79",
        "0.75"
      ],
      [
        "PP",
        "Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency",
        "0.81",
        "0.67"
      ]
    ],
    "id": "474a4ee7-88be-4e91-abc9-7f5d22b64f62",
    "claim": "To validate Acc, human annotators were asked to judge the style of 150 transferred sentences. We then compute the percentage of machine and human judgments that match.",
    "label": "refutes",
    "table_id": "11046f41-73be-47e9-9f30-8fe50765c22d"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
    "table_column_names": [
      "[EMPTY]",
      "MSCOCO spice",
      "MSCOCO cider",
      "MSCOCO rouge [ITALIC] L",
      "MSCOCO bleu4",
      "MSCOCO meteor",
      "MSCOCO rep↓",
      "Flickr30k spice",
      "Flickr30k cider",
      "Flickr30k rouge [ITALIC] L",
      "Flickr30k bleu4",
      "Flickr30k meteor",
      "Flickr30k rep↓"
    ],
    "table_content_values": [
      [
        "softmax",
        "18.4",
        "0.967",
        "52.9",
        "29.9",
        "24.9",
        "3.76",
        "13.5",
        "0.443",
        "44.2",
        "19.9",
        "19.1",
        "6.09"
      ],
      [
        "sparsemax",
        "[BOLD] 18.9",
        "[BOLD] 0.990",
        "[BOLD] 53.5",
        "[BOLD] 31.5",
        "[BOLD] 25.3",
        "3.69",
        "[BOLD] 13.7",
        "[BOLD] 0.444",
        "[BOLD] 44.3",
        "[BOLD] 20.7",
        "[BOLD] 19.3",
        "5.84"
      ],
      [
        "TVmax",
        "18.5",
        "0.974",
        "53.1",
        "29.9",
        "25.1",
        "[BOLD] 3.17",
        "13.3",
        "0.438",
        "44.2",
        "20.5",
        "19.0",
        "[BOLD] 3.97"
      ]
    ],
    "id": "238007ba-b7a6-4b65-9173-b00fdafd9bf2",
    "claim": "[CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax and significantly worse than softmax on MSCOCO and similar on Flickr30k.",
    "label": "refutes",
    "table_id": "55b8ec67-1f65-4852-943b-d1530519e837"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "e0931d7d-8c19-4823-8c57-1be6544bb616",
    "claim": "On the other hand, our BiLSTM model using contextualized word representation and PCS only obtained 0.72 F1 score.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "1efcb145-e05f-4506-a7d7-2f2adbf98715",
    "claim": "word analogies are especially useful for creating and evaluating continuous vector representations, since the solution of many analogy questions requires vector addition.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "97d1d7a0-87be-4a17-92c0-da0ee295a374",
    "claim": "Surprisingly, we observe a decrease of BLEU-2, BLEU-4, ROUGE-2, and METEOR when removing passages from our model input.",
    "label": "refutes",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "2f5f8672-0cea-486d-bf9b-88ee35183cc5",
    "claim": "The results also show that it is better to compile knowledge into constraints when the knowledge quality is high than add them as features.",
    "label": "supports",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "ba7dd703-d08b-42fa-b1e2-99c6183443eb",
    "claim": "B-COPA is sufficient for training performance models (e.g., BERT-large), as non-fine-tuned models achieve 66.4% on B-COPA, showing that even structural information captured by BERT is not required for reasoning about causality.",
    "label": "not enough info",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "715f5a99-6ca2-4117-b4df-a55d7bb5833e",
    "claim": "Consequently, with an 8% improvement on average, the hybrid model [CONTINUE] Word Content are increased.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "ee9ec2e9-4f60-4b21-a8e8-1e1ffd3c4d73",
    "claim": "In [14], they compare the word vectors generated by word2vec to GloVe and word2sense.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Evaluation of Greek Word Embeddings",
    "paper_id": "1904.04032v3",
    "table_caption": "Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
    "table_column_names": [
      "Category Semantic",
      "Category no oov words",
      "gr_def 58.42%",
      "gr_neg10 59.33%",
      "cc.el.300  [BOLD] 68.80%",
      "wiki.el 27.20%",
      "gr_cbow_def 31.76%",
      "gr_d300_nosub 60.79%",
      "gr_w2v_sg_n5 52.70%"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "with oov words",
        "52.97%",
        "55.33%",
        "[BOLD] 64.34%",
        "25.73%",
        "28.80%",
        "55.11%",
        "47.82%"
      ],
      [
        "Syntactic",
        "no oov words",
        "65.73%",
        "61.02%",
        "[BOLD] 69.35%",
        "40.90%",
        "64.02%",
        "53.69%",
        "52.60%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "[BOLD] 53.95%",
        "48.69%",
        "49.43%",
        "28.42%",
        "52.54%",
        "44.06%",
        "43.13%"
      ],
      [
        "Overall",
        "no oov words",
        "63.02%",
        "59.96%",
        "[BOLD] 68.97%",
        "36.45%",
        "52.04%",
        "56.30%",
        "52.66%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "53.60%",
        "51.00%",
        "[BOLD] 54.60%",
        "27.50%",
        "44.30%",
        "47.90%",
        "44.80%"
      ]
    ],
    "id": "bded69dc-786d-4fc5-83d4-d2e766361785",
    "claim": "Model wiki.el, trained only on Wikipedia, was the best in the category semantic with no oov words and the overall category with oov words.",
    "label": "refutes",
    "table_id": "5b468728-2bb8-41a6-8b44-30b94d52dd3b"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "3e5908f5-1d9d-47a6-8e48-a0822530dbdb",
    "claim": "[CONTINUE] Results with BERT show that contextual information is valuable for performance improvement.",
    "label": "supports",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 4: Experiment 2, t= “b*tch”",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.010",
        "0.010",
        "-0.632",
        "[EMPTY]",
        "0.978"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.963",
        "0.944",
        "20.064",
        "***",
        "1.020"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.011",
        "0.011",
        "-1.254",
        "[EMPTY]",
        "0.955"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.349",
        "0.290",
        "28.803",
        "***",
        "1.203"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.012",
        "0.012",
        "-0.162",
        "[EMPTY]",
        "0.995"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.017",
        "0.015",
        "4.698",
        "***",
        "1.152"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.988",
        "0.991",
        "-6.289",
        "***",
        "0.997"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.099",
        "0.091",
        "6.273",
        "***",
        "1.091"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.074",
        "0.027",
        "46.054",
        "***",
        "2.728"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.925",
        "0.968",
        "-41.396",
        "***",
        "0.956"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.010",
        "0.010",
        "0.000",
        "[EMPTY]",
        "1.000"
      ]
    ],
    "id": "d39ee230-80e9-4d79-ae14-922b3fc922e4",
    "claim": "We see different results for Waseem and Hovy (2016) and Waseem (2016).",
    "label": "refutes",
    "table_id": "def49c38-6913-4cba-9692-08a2b37b5640"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "1563ec9d-c56c-4d98-b401-792e93c5a56d",
    "claim": "It improves by over 20% over a state-of-art general coreference system on Winograd and also outperforms Rahman and Ng (2012) by a margin of 3.3%.",
    "label": "supports",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation",
    "paper_id": "1909.00754v2",
    "table_caption": "Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For “- Hierachical-Attn”, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For “- MLP”, we further replace the MLP with a single linear layer with the non-linear activation.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Joint Acc."
    ],
    "table_content_values": [
      [
        "COMER",
        "88.64%"
      ],
      [
        "- Hierachical-Attn",
        "86.69%"
      ],
      [
        "- MLP",
        "83.24%"
      ]
    ],
    "id": "584be382-d99a-4b4e-92c5-bcdb3ef882e9",
    "claim": "[CONTINUE] The effectiveness of our hierarchical attention design is disproved by an accuracy drop of only 1.95% after removing residual connections and the hierarchical stack of our attention modules.",
    "label": "refutes",
    "table_id": "1d045a18-ec29-4ba4-806d-f102ece9f985"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
    "table_column_names": [
      "Uni",
      "POS",
      "0 87.9",
      "1 92.0",
      "2 91.7",
      "3 91.8",
      "4 91.9"
    ],
    "table_content_values": [
      [
        "Uni",
        "SEM",
        "81.8",
        "87.8",
        "87.4",
        "87.6",
        "88.2"
      ],
      [
        "Bi",
        "POS",
        "87.9",
        "93.3",
        "92.9",
        "93.2",
        "92.8"
      ],
      [
        "Bi",
        "SEM",
        "81.9",
        "91.3",
        "90.8",
        "91.9",
        "91.9"
      ],
      [
        "Res",
        "POS",
        "87.9",
        "92.5",
        "91.9",
        "92.0",
        "92.4"
      ],
      [
        "Res",
        "SEM",
        "81.9",
        "88.2",
        "87.5",
        "87.6",
        "88.5"
      ]
    ],
    "id": "e3125f04-5ee3-4b83-957b-861f893cd399",
    "claim": "Comparing POS and SEM tagging (Table 5), we note that higher layer representations do not necessarily improve SEM tagging, while POS tagging does not peak at layer 1. We noticed no improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5).",
    "label": "refutes",
    "table_id": "56a61e48-903a-4dc4-8cbf-2048d2c8ee3c"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "9be65c57-384f-4389-a499-15fd4ac3ff16",
    "claim": "Dual2seq-LinAMR shows much worse performance than our Dual2seq model and significantly outperforms the Seq2seq baseline.",
    "label": "refutes",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "d732415c-f84a-431c-af8e-7c1fc901c561",
    "claim": "the final scores (lines 3 and 6 of the table) are the actual numbers reported in the paper (Table 2, right-most column).",
    "label": "not enough info",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "2a2b14fa-d841-40bf-a7fc-5143ce77f391",
    "claim": "the results of these experiments were statistically significant (t-test, p < .001).",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "5db7cece-882f-45e8-96c3-82a786c846c2",
    "claim": "[CONTINUE] Analyzing Table 3, we can observe that all values of precision using the Portuguese corpora have higher scores when compared with the English corpora.",
    "label": "supports",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.",
    "table_column_names": [
      "Dataset",
      "Accuracy",
      "Fleiss’ kappa  [ITALIC] k"
    ],
    "table_content_values": [
      [
        "Original COPA",
        "100.0",
        "0.973"
      ],
      [
        "Balanced COPA",
        "97.0",
        "0.798"
      ]
    ],
    "id": "b1e36aae-7b30-4c59-b940-00c04ce3ea16",
    "claim": "shows that humans who participate in the experiment cannot differentiate between the two options in a third of Balanced COPA questions, and hence Balance COPA questions significantly favor one answer choice.",
    "label": "not enough info",
    "table_id": "58160fa7-ce6a-4ff8-805b-43776f982ae5"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
    "table_column_names": [
      "[EMPTY]",
      "WN-N P",
      "WN-N R",
      "WN-N F",
      "WN-V P",
      "WN-V R",
      "WN-V F",
      "VN P",
      "VN R",
      "VN F"
    ],
    "table_content_values": [
      [
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2"
      ],
      [
        "type",
        ".700",
        ".654",
        ".676",
        ".535",
        ".474",
        ".503",
        ".327",
        ".309",
        ".318"
      ],
      [
        "x+POS",
        ".699",
        ".651",
        ".674",
        ".544",
        ".472",
        ".505",
        ".339",
        ".312",
        ".325"
      ],
      [
        "lemma",
        ".706",
        ".660",
        ".682",
        ".576",
        ".520",
        ".547",
        ".384",
        ".360",
        ".371"
      ],
      [
        "x+POS",
        "<bold>.710</bold>",
        "<bold>.662</bold>",
        "<bold>.685</bold>",
        "<bold>.589</bold>",
        "<bold>.529</bold>",
        "<bold>.557</bold>",
        "<bold>.410</bold>",
        "<bold>.389</bold>",
        "<bold>.399</bold>"
      ],
      [
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep"
      ],
      [
        "type",
        ".712",
        ".661",
        ".686",
        ".545",
        ".457",
        ".497",
        ".324",
        ".296",
        ".310"
      ],
      [
        "x+POS",
        ".715",
        ".659",
        ".686",
        ".560",
        ".464",
        ".508",
        ".349",
        ".320",
        ".334"
      ],
      [
        "lemma",
        "<bold>.725</bold>",
        "<bold>.668</bold>",
        "<bold>.696</bold>",
        ".591",
        ".512",
        ".548",
        ".408",
        ".371",
        ".388"
      ],
      [
        "x+POS",
        ".722",
        ".666",
        ".693",
        "<bold>.609</bold>",
        "<bold>.527</bold>",
        "<bold>.565</bold>",
        "<bold>.412</bold>",
        "<bold>.381</bold>",
        "<bold>.396</bold>"
      ]
    ],
    "id": "2f553672-527e-49c7-85e8-c13ecb888e56",
    "claim": "For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p ≤ .005) with no significant difference for WN-N and WN-V (p ≈ .05).",
    "label": "refutes",
    "table_id": "051fe422-03d4-44d5-836c-7f982b328555"
  },
  {
    "paper": "Aligning Vector-spaces with Noisy Supervised Lexicons",
    "paper_id": "1903.10238v1",
    "table_caption": "Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
    "table_column_names": [
      "Method",
      "En→It best",
      "En→It avg",
      "En→It iters",
      "En→De best",
      "En→De avg",
      "En→De iters",
      "En→Fi best",
      "En→Fi avg",
      "En→Fi iters",
      "En→Es best",
      "En→Es avg",
      "En→Es iters"
    ],
    "table_content_values": [
      [
        "Artetxe et al., 2018b",
        "[BOLD] 48.53",
        "48.13",
        "573",
        "48.47",
        "48.19",
        "773",
        "33.50",
        "32.63",
        "988",
        "37.60",
        "37.33",
        "808"
      ],
      [
        "Noise-aware Alignment",
        "[BOLD] 48.53",
        "[BOLD] 48.20",
        "471",
        "[BOLD] 49.67",
        "[BOLD] 48.89",
        "568",
        "[BOLD] 33.98",
        "[BOLD] 33.68",
        "502",
        "[BOLD] 38.40",
        "[BOLD] 37.79",
        "551"
      ]
    ],
    "id": "5f0079a8-4eb7-4fbd-8bcc-a2e289bc4562",
    "claim": "In most setups our average case is better than the former best case.",
    "label": "supports",
    "table_id": "279e3d12-df99-48f8-83ea-d3e42b8bbfcc"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "72882c1b-b616-44af-8650-561e948db115",
    "claim": "Furthermore, the PPO agent performs badly as it fails to ask enough questions to establish proper constraints.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "6d64d32d-781d-473e-b478-f646adbed3f4",
    "claim": "[CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the worse performance and benefits less from data augmentation comparing to our proposed domain-aware multi-decoder network,",
    "label": "supports",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "d0b294b5-acf6-4f1a-8068-bd6adadf1140",
    "claim": "however, the sdp information has a clear positive impact on all the relation types.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "ba4e9b1e-818d-4a1e-97d9-9d7b71b2e18d",
    "claim": "Supervising path attentions (the PRKGC+NS model) is not effective for improving the human interpretability of generated NLDs.",
    "label": "refutes",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 2: Averages of rankings of the words in same-gender pairs vs. different-gender pairs for Italian and German, along with their differences. Og stands for the original embeddings, Db for the debiased embeddings, and En for English. Each row presents the averages of pairs with the respective scores in SimLex-999 (0–4, 4–7, 7–10).",
    "table_column_names": [
      "[EMPTY]",
      "Italian Same-gender",
      "Italian Diff-Gender",
      "Italian difference",
      "German Same-gender",
      "German Diff-Gender",
      "German difference"
    ],
    "table_content_values": [
      [
        "7–10",
        "Og: 4884",
        "Og: 12947",
        "Og: 8063",
        "Og: 5925",
        "Og: 33604",
        "Og: 27679"
      ],
      [
        "7–10",
        "Db: 5523",
        "Db: 7312",
        "Db: 1789",
        "Db: 7653",
        "Db: 26071",
        "Db: 18418"
      ],
      [
        "7–10",
        "En: 6978",
        "En: 2467",
        "En: -4511",
        "En: 4517",
        "En: 8666",
        "En: 4149"
      ],
      [
        "4–7",
        "Og: 10954",
        "Og: 15838",
        "Og: 4884",
        "Og: 19271",
        "Og: 27256",
        "Og: 7985"
      ],
      [
        "4–7",
        "Db: 12037",
        "Db: 12564",
        "Db: 527",
        "Db: 24845",
        "Db: 22970",
        "Db: -1875"
      ],
      [
        "4–7",
        "En: 15891",
        "En: 17782",
        "En: 1891",
        "En: 13282",
        "En: 17649",
        "En: 4367"
      ],
      [
        "0–4",
        "Og: 23314",
        "Og: 35783",
        "Og: 12469",
        "Og: 50983",
        "Og: 85263",
        "Og: 34280"
      ],
      [
        "0–4",
        "Db: 26386",
        "Db: 28067",
        "Db: 1681",
        "Db: 60603",
        "Db: 79081",
        "Db: 18478"
      ],
      [
        "0–4",
        "En: 57278",
        "En: 53053",
        "En: -4225",
        "En: 41509",
        "En: 62929",
        "En: 21420"
      ]
    ],
    "id": "ba0126ce-21f6-46bd-8eff-2d93dfcaa85c",
    "claim": "As expected, the average ranking of samegender pairs is significantly higher than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller.",
    "label": "refutes",
    "table_id": "97f25636-ac5f-4737-8cbe-3b42364a43d3"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1038",
        "0.0170",
        "0.0490",
        "0.0641",
        "0.0641",
        "0.0613",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1282",
        "0.0291",
        "0.0410",
        "0.0270",
        "0.0270",
        "0.1154",
        "0.0661"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.6185",
        "0.3744",
        "0.4144",
        "0.4394",
        "0.4394",
        "[BOLD] 0.7553",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.6308",
        "0.4124",
        "0.4404",
        "0.4515",
        "0.4945",
        "[BOLD] 0.8609",
        "0.5295"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "[BOLD] 0.0021",
        "0.0004",
        "0.0011",
        "0.0014",
        "0.0014",
        "0.0013",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0011",
        "0.0008",
        "0.0011",
        "0.0008",
        "0.0008",
        "[BOLD] 0.0030",
        "0.0018"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0012",
        "0.0008",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0016",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0003",
        "0.0009",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0017",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "[BOLD] 0.0041",
        "0.0007",
        "0.0021",
        "0.0027",
        "0.0027",
        "0.0026",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0022",
        "0.0016",
        "0.0022",
        "0.0015",
        "0.0015",
        "[BOLD] 0.0058",
        "0.0036"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0024",
        "0.0016",
        "0.0018",
        "0.0019",
        "0.0019",
        "[BOLD] 0.0031",
        "0.0023"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0005",
        "0.0018",
        "0.0018",
        "0.0020",
        "0.0021",
        "[BOLD] 0.0034",
        "0.0022"
      ]
    ],
    "id": "fecfd170-1f8f-4f70-8b18-e211486982f2",
    "claim": "On the other hand, choosing the best hypernym worked very well for DocSub which obtained the best precision for the Portuguese corpora.",
    "label": "supports",
    "table_id": "7ff90dc3-0887-4d7b-b7bf-bb5149801b4e"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "SA (S: 3 - M: 1)",
        "55.25",
        "} 0.082M"
      ],
      [
        "[BOLD] SA (S: 3 - B: 3)",
        "[BOLD] 55.42",
        "} 0.082M"
      ],
      [
        "SA (S: 3 - B: 4)",
        "55.33",
        "} 0.082M"
      ],
      [
        "SA (S: 3 - B: 6)",
        "55.31",
        "} 0.082M"
      ],
      [
        "SA (S: 3 - B: 1,3,5)",
        "55.45",
        "} 0.245M"
      ],
      [
        "[BOLD] SA (S: 3 - B: 2,4,6)",
        "[BOLD] 55.56",
        "} 0.245M"
      ]
    ],
    "id": "de3034a6-6815-43f6-ab74-408ae82f3718",
    "claim": "The improvement is not significant enough to warrant further research into visual modulation.",
    "label": "refutes",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "Baseline (No SA)Anderson et al. ( 2018 )",
        "55.00",
        "0M"
      ],
      [
        "SA (S: 1,2,3 - B: 1)",
        "55.11",
        "} 0.107M"
      ],
      [
        "SA (S: 1,2,3 - B: 2)",
        "55.17",
        "} 0.107M"
      ],
      [
        "[BOLD] SA (S: 1,2,3 - B: 3)",
        "[BOLD] 55.27",
        "} 0.107M"
      ]
    ],
    "id": "52f9985b-e7c4-4516-a758-9eebf47cb971",
    "claim": "[CONTINUE] We showed that it is possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks.",
    "label": "supports",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "51c11868-1bbf-466a-b95d-621a87c43768",
    "claim": "In fact, DocSub had worse results in precision only when using Europarl corpus in English, where DF reached best values of precision and f-measure.",
    "label": "supports",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "2de15a72-9055-4a7e-897f-cf0c3c3aeb36",
    "claim": "[CONTINUE] Though ALDM obtains a lower inform F1 and match rate than PPO, it gets a slight improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3.",
    "label": "supports",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "47e0e193-299d-4225-aa6f-6dcd0fe79f5c",
    "claim": "Moreover, all agents tend to perform better on booking flights, but worse on booking hotels.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation",
    "paper_id": "1906.12068v1",
    "table_caption": "Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.",
    "table_column_names": [
      "System reference",
      "BLEU↑",
      "TER↓"
    ],
    "table_content_values": [
      [
        "en-fr-rnn-rev",
        "33.3",
        "50.2"
      ],
      [
        "en-fr-smt-rev",
        "36.5",
        "47.1"
      ],
      [
        "en-fr-trans-rev",
        "[BOLD] 36.8",
        "[BOLD] 46.8"
      ],
      [
        "en-es-rnn-rev",
        "37.8",
        "45.0"
      ],
      [
        "en-es-smt-rev",
        "39.2",
        "44.0"
      ],
      [
        "en-es-trans-rev",
        "[BOLD] 40.4",
        "[BOLD] 42.7"
      ]
    ],
    "id": "0f61a515-9f46-4502-a2db-a1ef9a6c2148",
    "claim": "we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While RNN models are the best ones according to the evaluation metrics,",
    "label": "refutes",
    "table_id": "61bdf875-07d5-42cb-8051-fa032a1eb7b0"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "9b0998ee-9b1d-472a-a9dc-19f4dc5c57a8",
    "claim": "[CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings outperforms sentence embeddings alone, but do not exceed the upper boundary of BoC feature, in which again demonstrating the competitiveness of BoC feature.",
    "label": "supports",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "e9042900-dce1-494a-a106-5bf07dc8e36d",
    "claim": "This indicates that the number of top sessions and the diversity of human responses may suffer from the hand-crafted reward.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Zero-Shot Grounding of Objects from Natural Language Queries",
    "paper_id": "1908.07129v1",
    "table_caption": "Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600×600",
    "table_column_names": [
      "Model",
      "Accuracy on RefClef"
    ],
    "table_content_values": [
      [
        "BM + Softmax",
        "48.54"
      ],
      [
        "BM + BCE",
        "55.20"
      ],
      [
        "BM + FL",
        "57.13"
      ],
      [
        "BM + FL + Img-Resize",
        "[BOLD] 61.75"
      ]
    ],
    "id": "775b4ac6-9478-4306-b388-b0e8203c4ac0",
    "claim": "[CONTINUE] Finally, image resizing gives another 4% increase.",
    "label": "supports",
    "table_id": "f8635440-21d1-4472-85d8-b852ab096a46"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "f3e9ad19-2aaa-4fc1-9416-3d1f84765f21",
    "claim": "an evaluation of the best joint model on the test dataset with the new evaluation scripts (Teresi et al., 2019) gives 71.2 F1, which is slightly higher than the value reported by the organizers of the competition (Teresi et al., 2017), namely 71.1 F1.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM",
    "paper_id": "1704.07221v1",
    "table_caption": "Table 5: Confusion matrix for testing set predictions",
    "table_column_names": [
      "[BOLD] LabelPrediction",
      "[BOLD] C",
      "[BOLD] D",
      "[BOLD] Q",
      "[BOLD] S"
    ],
    "table_content_values": [
      [
        "[BOLD] Commenting",
        "760",
        "0",
        "12",
        "6"
      ],
      [
        "[BOLD] Denying",
        "68",
        "0",
        "1",
        "2"
      ],
      [
        "[BOLD] Querying",
        "69",
        "0",
        "36",
        "1"
      ],
      [
        "[BOLD] Supporting",
        "67",
        "0",
        "1",
        "26"
      ]
    ],
    "id": "3197bce9-5af1-44bb-ae78-af57c4346c14",
    "claim": "Most denying instances get misclassified as querying (see Table 5),",
    "label": "refutes",
    "table_id": "bd4b48bc-00c7-4229-a6c9-7bc5a53456dc"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 4: Cue classification on the test set.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] F-Score  [BOLD] Baseline",
      "[BOLD] F-Score  [BOLD] Proposed",
      "[BOLD] Support"
    ],
    "table_content_values": [
      [
        "False cues",
        "0.61",
        "0.68",
        "47"
      ],
      [
        "Actual cues",
        "0.97",
        "0.98",
        "557"
      ]
    ],
    "id": "1965045b-8626-4983-9653-9a38b7e6b14d",
    "claim": "the classifier succeeded in effectively reducing the number of false cues, in spite of their unpredictable nature.",
    "label": "not enough info",
    "table_id": "08a8fe8c-92a2-41dc-a6e0-f97b95380cae"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
    "table_column_names": [
      "[BOLD] Emoji alias",
      "[BOLD] N",
      "[BOLD] emoji #",
      "[BOLD] emoji %",
      "[BOLD] no-emoji #",
      "[BOLD] no-emoji %",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "mask",
        "163",
        "154",
        "94.48",
        "134",
        "82.21",
        "- 12.27"
      ],
      [
        "two_hearts",
        "87",
        "81",
        "93.10",
        "77",
        "88.51",
        "- 4.59"
      ],
      [
        "heart_eyes",
        "122",
        "109",
        "89.34",
        "103",
        "84.43",
        "- 4.91"
      ],
      [
        "heart",
        "267",
        "237",
        "88.76",
        "235",
        "88.01",
        "- 0.75"
      ],
      [
        "rage",
        "92",
        "78",
        "84.78",
        "66",
        "71.74",
        "- 13.04"
      ],
      [
        "cry",
        "116",
        "97",
        "83.62",
        "83",
        "71.55",
        "- 12.07"
      ],
      [
        "sob",
        "490",
        "363",
        "74.08",
        "345",
        "70.41",
        "- 3.67"
      ],
      [
        "unamused",
        "167",
        "121",
        "72.46",
        "116",
        "69.46",
        "- 3.00"
      ],
      [
        "weary",
        "204",
        "140",
        "68.63",
        "139",
        "68.14",
        "- 0.49"
      ],
      [
        "joy",
        "978",
        "649",
        "66.36",
        "629",
        "64.31",
        "- 2.05"
      ],
      [
        "sweat_smile",
        "111",
        "73",
        "65.77",
        "75",
        "67.57",
        "1.80"
      ],
      [
        "confused",
        "77",
        "46",
        "59.74",
        "48",
        "62.34",
        "2.60"
      ]
    ],
    "id": "919169e5-b0e4-4818-96d5-27895efad28b",
    "claim": "[CONTINUE] When removing sweat smile and confused accuracy decreased.",
    "label": "refutes",
    "table_id": "2410b1c3-4d48-49e9-9279-dc23daa879d1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "48715abe-b59e-4f35-a43a-b1539a2dbfc4",
    "claim": "We can see that the two policy gradient approaches outperform RL using the discriminative model and the value based RL on the majority of the metrics.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "65d85bed-5ff2-4954-92c9-f7e7cfd25951",
    "claim": "The resulting cross-dataset improvements on the SNLI and Glockner datasets are larger than those on the SICK dataset.",
    "label": "supports",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "6201e247-3ab6-4be7-b8cf-739a3acab517",
    "claim": "This explains why our proposed method achieves the best average reward, and confirms the fact that our proposed policy learns to control the number of turns better than other baselines",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "paper_id": "1909.03546v2",
    "table_caption": "Table 3: F1 scores on Relation.",
    "table_column_names": [
      "[EMPTY]",
      "ACE05",
      "SciERC",
      "WLPC"
    ],
    "table_content_values": [
      [
        "BERT + LSTM",
        "60.6",
        "40.3",
        "65.1"
      ],
      [
        "+RelProp",
        "61.9",
        "41.1",
        "65.3"
      ],
      [
        "+CorefProp",
        "59.7",
        "42.6",
        "-"
      ],
      [
        "BERT FineTune",
        "[BOLD] 62.1",
        "44.3",
        "65.4"
      ],
      [
        "+RelProp",
        "62.0",
        "43.0",
        "[BOLD] 65.5"
      ],
      [
        "+CorefProp",
        "60.0",
        "[BOLD] 45.3",
        "-"
      ]
    ],
    "id": "ef13c2cf-6271-4c5c-a1ee-17e71aea7566",
    "claim": "[CONTINUE] Relation propagation (RelProp) improves relation extraction performance over both pretrained and fine-tuned BERT.",
    "label": "refutes",
    "table_id": "3fc48613-1f39-4968-a1bc-84c2cae46001"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "b6f67c1c-41b9-4975-ba63-26448b6c1b08",
    "claim": "In addition, our metric also has the highest Pearson correlation with humans.",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] # pairs",
      "[BOLD] # words (doc)",
      "[BOLD] # sents (docs)",
      "[BOLD] # words (summary)",
      "[BOLD] # sents (summary)",
      "[BOLD] vocab size"
    ],
    "table_content_values": [
      [
        "Multi-News",
        "44,972/5,622/5,622",
        "2,103.49",
        "82.73",
        "263.66",
        "9.97",
        "666,515"
      ],
      [
        "DUC03+04",
        "320",
        "4,636.24",
        "173.15",
        "109.58",
        "2.88",
        "19,734"
      ],
      [
        "TAC 2011",
        "176",
        "4,695.70",
        "188.43",
        "99.70",
        "1.00",
        "24,672"
      ],
      [
        "CNNDM",
        "287,227/13,368/11,490",
        "810.57",
        "39.78",
        "56.20",
        "3.68",
        "717,951"
      ]
    ],
    "id": "fe71de2d-b005-4e58-81c6-22d84526ca66",
    "claim": "Our summaries are notably longer than in other works, about 260 words on average.",
    "label": "supports",
    "table_id": "9fda9198-5e75-4f79-b42c-82b7dc3ee31f"
  },
  {
    "paper": "Keyphrase Generation for Scientific Articles using GANs",
    "paper_id": "1909.12229v1",
    "table_caption": "Table 2: α-nDCG@5 metrics",
    "table_column_names": [
      "Model",
      "Inspec",
      "Krapivin",
      "NUS",
      "KP20k"
    ],
    "table_content_values": [
      [
        "Catseq",
        "0.87803",
        "0.781",
        "0.82118",
        "0.804"
      ],
      [
        "Catseq-RL",
        "0.8602",
        "[BOLD] 0.786",
        "0.83",
        "0.809"
      ],
      [
        "GAN",
        "[BOLD] 0.891",
        "0.771",
        "[BOLD] 0.853",
        "[BOLD] 0.85"
      ]
    ],
    "id": "70361fad-829f-4e8c-af9b-e23d8acd1375",
    "claim": "The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is nearly 5% better than both the other baseline models.",
    "label": "supports",
    "table_id": "db0cb6ad-ade2-4926-8a07-f92a9d74c055"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
    "table_column_names": [
      "System",
      "All P",
      "All R",
      "All F1",
      "In  [ITALIC] E+ P",
      "In  [ITALIC] E+ R",
      "In  [ITALIC] E+ F1"
    ],
    "table_content_values": [
      [
        "Name matching",
        "15.03",
        "15.03",
        "15.03",
        "29.13",
        "29.13",
        "29.13"
      ],
      [
        "MIL (model 1)",
        "35.87",
        "35.87",
        "35.87 ±0.72",
        "69.38",
        "69.38",
        "69.38 ±1.29"
      ],
      [
        "MIL-ND (model 2)",
        "37.42",
        "[BOLD] 37.42",
        "37.42 ±0.35",
        "72.50",
        "[BOLD] 72.50",
        "[BOLD] 72.50 ±0.68"
      ],
      [
        "[ITALIC] τMIL-ND (model 2)",
        "[BOLD] 38.91",
        "36.73",
        "[BOLD] 37.78 ±0.26",
        "[BOLD] 73.19",
        "71.15",
        "72.16 ±0.48"
      ],
      [
        "Supervised learning",
        "42.90",
        "42.90",
        "42.90 ±0.59",
        "83.12",
        "83.12",
        "83.12 ±1.15"
      ]
    ],
    "id": "6fceb272-19a2-4482-9526-d93a34ee8ba4",
    "claim": "The ND classifier had a significant positive effect on F1 for the 'In E+' setting.",
    "label": "refutes",
    "table_id": "ce21ef70-1b61-4a48-9c97-69426861922c"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 2: Ablation study results.",
    "table_column_names": [
      "[BOLD] Variation",
      "[BOLD] Accuracy (%)",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "Submitted",
        "[BOLD] 69.23",
        "-"
      ],
      [
        "No emoji",
        "68.36",
        "- 0.87"
      ],
      [
        "No ELMo",
        "65.52",
        "- 3.71"
      ],
      [
        "Concat Pooling",
        "68.47",
        "- 0.76"
      ],
      [
        "LSTM hidden=4096",
        "69.10",
        "- 0.13"
      ],
      [
        "LSTM hidden=1024",
        "68.93",
        "- 0.30"
      ],
      [
        "LSTM hidden=512",
        "68.43",
        "- 0.80"
      ],
      [
        "POS emb dim=100",
        "68.99",
        "- 0.24"
      ],
      [
        "POS emb dim=75",
        "68.61",
        "- 0.62"
      ],
      [
        "POS emb dim=50",
        "69.33",
        "+ 0.10"
      ],
      [
        "POS emb dim=25",
        "69.21",
        "- 0.02"
      ],
      [
        "SGD optim lr=1",
        "64.33",
        "- 4.90"
      ],
      [
        "SGD optim lr=0.1",
        "66.11",
        "- 3.12"
      ],
      [
        "SGD optim lr=0.01",
        "60.72",
        "- 8.51"
      ],
      [
        "SGD optim lr=0.001",
        "30.49",
        "- 38.74"
      ]
    ],
    "id": "e9ed6a2c-59be-47fd-815e-3a08898c26c5",
    "claim": "We performed an ablation study on a single model having obtained 69.23% accuracy on the validation set.",
    "label": "supports",
    "table_id": "aca64ad8-2778-467a-93ff-0acc12c969e6"
  },
  {
    "paper": "Two Causal Principles for Improving Visual Dialog",
    "paper_id": "1911.10496v2",
    "table_caption": "Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.",
    "table_column_names": [
      "Model",
      "baseline",
      "QT",
      "S  [ITALIC] R0",
      "S  [ITALIC] R1",
      "S  [ITALIC] R2",
      "S  [ITALIC] R3",
      "D"
    ],
    "table_content_values": [
      [
        "LF ",
        "57.21",
        "58.97",
        "67.82",
        "71.27",
        "72.04",
        "72.36",
        "72.65"
      ],
      [
        "LF +P1",
        "61.88",
        "62.87",
        "69.47",
        "72.16",
        "72.85",
        "73.42",
        "[BOLD] 73.63"
      ]
    ],
    "id": "2df5a453-0b55-4883-8f4a-a1486ebbb214",
    "claim": "Overall, none of the implementations can improve the performances of base models.",
    "label": "refutes",
    "table_id": "c4098014-d3c7-4ee8-b56d-3553dcc3d89f"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result Lample et al. (2016).",
    "table_column_names": [
      "Model",
      "#Params",
      "NER"
    ],
    "table_content_values": [
      [
        "LSTM*",
        "-",
        "90.94"
      ],
      [
        "LSTM",
        "245K",
        "[BOLD] 89.61"
      ],
      [
        "GRU",
        "192K",
        "89.35"
      ],
      [
        "ATR",
        "87K",
        "88.46"
      ],
      [
        "SRU",
        "161K",
        "88.89"
      ],
      [
        "LRN",
        "129K",
        "88.56"
      ]
    ],
    "id": "d697a74d-c867-4d09-950e-3d3f84fef4c5",
    "claim": "As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79).",
    "label": "supports",
    "table_id": "11c65341-21bb-418e-b809-9cbf7eaecf52"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "dffdd441-9432-4656-830a-adf397ec3283",
    "claim": "Our models DCGCN(single) and DCGCN(ensemble) do not remove the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers, as evidenced by the results of BoW+GCN, CNN+GCN, and BiRNN+GCN.",
    "label": "refutes",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "a1f1fc3a-e648-4af9-84ce-3b59c2a584d5",
    "claim": "Consequently, with an 8% decrease on average, the hybrid model [CONTINUE] Word Content are decreased.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "e6ccc836-e7ca-4096-b1ed-6261d09f7a11",
    "claim": "they report one big advantage of our method, which is increasing performance when the correct answer is missing from the training corpus.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "cd352a89-328c-4845-bf4c-d60c006603a7",
    "claim": "On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point.",
    "label": "supports",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "6d69f58a-3261-43e4-97f0-80e4cbf6ed87",
    "claim": "our model outperforms all the variants significantly under any recall and AUC.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "362601b1-3bf5-47a7-a8ab-192050bbeea7",
    "claim": "This suggests that enriching input graphs with the global node and excluding the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations.",
    "label": "refutes",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 4: Precisions on the Wikidata dataset with different choice of d.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC",
      "Time"
    ],
    "table_content_values": [
      [
        "[ITALIC] d=1",
        "0.602",
        "0.487",
        "0.403",
        "0.367",
        "4h"
      ],
      [
        "[ITALIC] d=32",
        "0.645",
        "0.501",
        "0.393",
        "0.370",
        "-"
      ],
      [
        "[ITALIC] d=16",
        "0.655",
        "0.518",
        "0.413",
        "0.413",
        "20h"
      ],
      [
        "[ITALIC] d=8",
        "0.650",
        "0.519",
        "0.422",
        "0.405",
        "8h"
      ]
    ],
    "id": "0522e114-a86c-49ae-a0c1-2291cf0b6e30",
    "claim": "As the table 4 depicts, the training time increases with the growth of d.",
    "label": "supports",
    "table_id": "b3925d14-8042-410e-841f-73dce02fc49b"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "cb276238-d6d0-427f-9a25-e923b519df9b",
    "claim": "For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest.",
    "label": "supports",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "87d5b95f-61f8-4387-9fd5-f6678fe07708",
    "claim": "It might be that model generalization is improved when the model is initialized with weights that have been fine-tuned to a challenging dataset, even if this dataset comes from a different domain.",
    "label": "not enough info",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "6cd67271-8961-4a05-9ba2-2f3f773359c6",
    "claim": "Consequently, with an 8% decrease, CMOW is substantially less linguistically informed than CBOW.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "112b9843-9e26-40be-a18d-dfc98e44037f",
    "claim": "Therefore, we have strong evidence that our learned reward can be evaluated and optimized over.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "d0317274-0a8c-4613-b192-9b9d0da2ca72",
    "claim": "[CONTINUE] Dual2seq is signifi [CONTINUE] cantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much larger under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU).",
    "label": "supports",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "74bfcde1-c624-4382-8be4-2c3de804b05a",
    "claim": "Results also show the linear combination is more effective than the global node.",
    "label": "supports",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "5df569a6-e075-4743-8b37-c7182d701cd8",
    "claim": "2018).",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1038",
        "0.0170",
        "0.0490",
        "0.0641",
        "0.0641",
        "0.0613",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1282",
        "0.0291",
        "0.0410",
        "0.0270",
        "0.0270",
        "0.1154",
        "0.0661"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.6185",
        "0.3744",
        "0.4144",
        "0.4394",
        "0.4394",
        "[BOLD] 0.7553",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.6308",
        "0.4124",
        "0.4404",
        "0.4515",
        "0.4945",
        "[BOLD] 0.8609",
        "0.5295"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "[BOLD] 0.0021",
        "0.0004",
        "0.0011",
        "0.0014",
        "0.0014",
        "0.0013",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0011",
        "0.0008",
        "0.0011",
        "0.0008",
        "0.0008",
        "[BOLD] 0.0030",
        "0.0018"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0012",
        "0.0008",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0016",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0003",
        "0.0009",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0017",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "[BOLD] 0.0041",
        "0.0007",
        "0.0021",
        "0.0027",
        "0.0027",
        "0.0026",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0022",
        "0.0016",
        "0.0022",
        "0.0015",
        "0.0015",
        "[BOLD] 0.0058",
        "0.0036"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0024",
        "0.0016",
        "0.0018",
        "0.0019",
        "0.0019",
        "[BOLD] 0.0031",
        "0.0023"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0005",
        "0.0018",
        "0.0018",
        "0.0020",
        "0.0021",
        "[BOLD] 0.0034",
        "0.0022"
      ]
    ],
    "id": "a5635632-7ec5-4631-bf69-893fe583a701",
    "claim": "[CONTINUE] Analyzing Table 5 we observe that Patt achieves again the best precision values for the English corpora.",
    "label": "supports",
    "table_id": "7ff90dc3-0887-4d7b-b7bf-bb5149801b4e"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "d8e62ced-4be2-4ae7-b832-ac7070831958",
    "claim": "the models more often hallucinate additional information, rather than failing to realise part of the MR.",
    "label": "refutes",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "3bb2c35b-c1d6-48c3-b4a9-3b99298604e0",
    "claim": "This improvement is mainly due to the fact that this model becomes better at predicting entity span boundaries.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
    "table_column_names": [
      "Schema",
      "AntePre(Test)",
      "AntePre(Train)"
    ],
    "table_content_values": [
      [
        "Type 1",
        "76.67",
        "86.79"
      ],
      [
        "Type 2",
        "79.55",
        "88.86"
      ],
      [
        "Type 1 (Cat1)",
        "90.26",
        "93.64"
      ],
      [
        "Type 2 (Cat2)",
        "83.38",
        "92.49"
      ]
    ],
    "id": "8e524e36-4e8a-4a05-8698-51fa8969dd6e",
    "claim": "The performance increase between Cat1/Cat2 and full data indicates that the existing knowledge schemas and knowledge acquisition are sufficient for further performance improvement.",
    "label": "refutes",
    "table_id": "18072983-1b23-4da0-8e08-1b6a92fbc124"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 4: Experiment 2, t= “b*tch”",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.010",
        "0.010",
        "-0.632",
        "[EMPTY]",
        "0.978"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.963",
        "0.944",
        "20.064",
        "***",
        "1.020"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.011",
        "0.011",
        "-1.254",
        "[EMPTY]",
        "0.955"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.349",
        "0.290",
        "28.803",
        "***",
        "1.203"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.012",
        "0.012",
        "-0.162",
        "[EMPTY]",
        "0.995"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.017",
        "0.015",
        "4.698",
        "***",
        "1.152"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.988",
        "0.991",
        "-6.289",
        "***",
        "0.997"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.099",
        "0.091",
        "6.273",
        "***",
        "1.091"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.074",
        "0.027",
        "46.054",
        "***",
        "2.728"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.925",
        "0.968",
        "-41.396",
        "***",
        "0.956"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.010",
        "0.010",
        "0.000",
        "[EMPTY]",
        "1.000"
      ]
    ],
    "id": "abe5fac5-e3f6-453a-8fbf-4e0cf1ed10d0",
    "claim": "[CONTINUE] We see similar results for Waseem and Hovy (2016) and Waseem (2016).",
    "label": "supports",
    "table_id": "def49c38-6913-4cba-9692-08a2b37b5640"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "33bbe459-fafc-4c3f-93af-70abe946a9f5",
    "claim": "our model imparted 62% more relevant information about the words of the English language than GloVe embeddings.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "169b9c65-30e6-4fa5-8874-293dc2112cce",
    "claim": "The largest loss is by 4% on the CoordInv task.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 4: Cue classification on the test set.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] F-Score  [BOLD] Baseline",
      "[BOLD] F-Score  [BOLD] Proposed",
      "[BOLD] Support"
    ],
    "table_content_values": [
      [
        "False cues",
        "0.61",
        "0.68",
        "47"
      ],
      [
        "Actual cues",
        "0.97",
        "0.98",
        "557"
      ]
    ],
    "id": "45169d93-9391-46ff-8995-4bc3a255b777",
    "claim": "we achieve an increased accuracy of our cue detection classifier in a transductive setting",
    "label": "not enough info",
    "table_id": "08a8fe8c-92a2-41dc-a6e0-f97b95380cae"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "e4a34f48-84bd-4a32-a55b-11982d2ab811",
    "claim": "The output of SPINE is not very reliable in the sense that there is no upper bound for distances between vectors and vectors can take any values in R+ for each dimension.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 4: Precisions on the Wikidata dataset with different choice of d.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC",
      "Time"
    ],
    "table_content_values": [
      [
        "[ITALIC] d=1",
        "0.602",
        "0.487",
        "0.403",
        "0.367",
        "4h"
      ],
      [
        "[ITALIC] d=32",
        "0.645",
        "0.501",
        "0.393",
        "0.370",
        "-"
      ],
      [
        "[ITALIC] d=16",
        "0.655",
        "0.518",
        "0.413",
        "0.413",
        "20h"
      ],
      [
        "[ITALIC] d=8",
        "0.650",
        "0.519",
        "0.422",
        "0.405",
        "8h"
      ]
    ],
    "id": "ba1c0ef0-0efa-45dc-9490-9446bd078bec",
    "claim": "using different dimensions may affect the accuracy of predictions.",
    "label": "not enough info",
    "table_id": "b3925d14-8042-410e-841f-73dce02fc49b"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "ed4eb2c0-b3d0-4980-bf4b-9425b6ef4eb9",
    "claim": "The smaller performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely less on superficial cues.",
    "label": "supports",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "498f6ec3-e6dc-4f56-bf77-c79ee97e46e2",
    "claim": "We see that SPINE performs much better on the polarized set than the mixed set, but our model with projected vectors performs better overall, even on the polarized set",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "f0e4ce13-671a-44e9-bce5-246e5e76dd4f",
    "claim": "results demonstrate the efficacy of the proposed two-phase learning scheme.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "bf1a3820-efa1-4b5b-bbe1-7810752e51d9",
    "claim": "When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DF achieved better values of precision, but lower values of recall.",
    "label": "refutes",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "1c5d7274-3f2c-4083-b06d-3b41a4df7b59",
    "claim": "This is another evidence of the effectiveness of the multiple-hop distillation with jointly learning agent.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "c8665c99-3cac-4c5e-aec7-745269049acf",
    "claim": "Our word embedding model performs similar to existing word embedding based algorithms, although there are many hyperparameters, such as N, h, where the number of features selected in the feature set selection step, in need of extensive hyperparameter tuning.",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "6fcdbe4f-bff1-4f7e-a83a-58760b3ce74e",
    "claim": "when we reach 100 episodes or more, our greedy agent matches the performance of the extractive-RL model.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "e68e3a06-166a-404d-a05f-127f61bcc59e",
    "claim": "according to the Figure 3, we can see that the policy layer of GPDL is updating faster than other layers.",
    "label": "not enough info",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "3ff4b1c7-6d1b-4795-a87f-8f2095841c18",
    "claim": "from the empirical results, the number of turns taken by the RL policy is very close to that of the human conversations.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "d065dac8-71db-42b4-9bd5-fbe88dab9be2",
    "claim": "Replacing the cue words in a sentence by the alternatives where they belong to leads to contradictory judgment in 37.5% of all sentences.",
    "label": "not enough info",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "43e1a4df-038c-4b24-9c68-e1455a5a77a7",
    "claim": "However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER).",
    "label": "supports",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 1: Image-caption ranking results for English (Multi30k)",
    "table_column_names": [
      "[EMPTY]",
      "Image to Text R@1",
      "Image to Text R@5",
      "Image to Text R@10",
      "Image to Text Mr",
      "Text to Image R@1",
      "Text to Image R@5",
      "Text to Image R@10",
      "Text to Image Mr",
      "Alignment"
    ],
    "table_content_values": [
      [
        "[BOLD] symmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Parallel gella:17",
        "31.7",
        "62.4",
        "74.1",
        "3",
        "24.7",
        "53.9",
        "65.7",
        "5",
        "-"
      ],
      [
        "UVS kiros:15",
        "23.0",
        "50.7",
        "62.9",
        "5",
        "16.8",
        "42.0",
        "56.5",
        "8",
        "-"
      ],
      [
        "EmbeddingNet wang:18",
        "40.7",
        "69.7",
        "79.2",
        "-",
        "29.2",
        "59.6",
        "71.7",
        "-",
        "-"
      ],
      [
        "sm-LSTM huang:17",
        "42.5",
        "71.9",
        "81.5",
        "2",
        "30.2",
        "60.4",
        "72.3",
        "3",
        "-"
      ],
      [
        "VSE++ faghri:18",
        "[BOLD] 43.7",
        "71.9",
        "82.1",
        "2",
        "32.3",
        "60.9",
        "72.1",
        "3",
        "-"
      ],
      [
        "Mono",
        "41.4",
        "74.2",
        "84.2",
        "2",
        "32.1",
        "63.0",
        "73.9",
        "3",
        "-"
      ],
      [
        "FME",
        "39.2",
        "71.1",
        "82.1",
        "2",
        "29.7",
        "62.5",
        "74.1",
        "3",
        "76.81%"
      ],
      [
        "AME",
        "43.5",
        "[BOLD] 77.2",
        "[BOLD] 85.3",
        "[BOLD] 2",
        "[BOLD] 34.0",
        "[BOLD] 64.2",
        "[BOLD] 75.4",
        "[BOLD] 3",
        "66.91%"
      ],
      [
        "[BOLD] asymmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Pivot gella:17",
        "33.8",
        "62.8",
        "75.2",
        "3",
        "26.2",
        "56.4",
        "68.4",
        "4",
        "-"
      ],
      [
        "Parallel gella:17",
        "31.5",
        "61.4",
        "74.7",
        "3",
        "27.1",
        "56.2",
        "66.9",
        "4",
        "-"
      ],
      [
        "Mono",
        "47.7",
        "77.1",
        "86.9",
        "2",
        "35.8",
        "66.6",
        "76.8",
        "3",
        "-"
      ],
      [
        "FME",
        "44.9",
        "76.9",
        "86.4",
        "2",
        "34.2",
        "66.1",
        "77.1",
        "3",
        "76.81%"
      ],
      [
        "AME",
        "[BOLD] 50.5",
        "[BOLD] 79.7",
        "[BOLD] 88.4",
        "[BOLD] 1",
        "[BOLD] 38.0",
        "[BOLD] 68.5",
        "[BOLD] 78.4",
        "[BOLD] 2",
        "73.10%"
      ]
    ],
    "id": "9602fe65-f85f-4388-81ee-1fd46873e99b",
    "claim": "FME performs better than AME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training.",
    "label": "refutes",
    "table_id": "cc0ce7f1-f382-42ba-b615-186b6834cac5"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.",
    "table_column_names": [
      "Dataset",
      "Accuracy",
      "Fleiss’ kappa  [ITALIC] k"
    ],
    "table_content_values": [
      [
        "Original COPA",
        "100.0",
        "0.973"
      ],
      [
        "Balanced COPA",
        "97.0",
        "0.798"
      ]
    ],
    "id": "2a9bf464-c3de-47c7-9016-4452eccf3a6b",
    "claim": ", as compared to the original dataset, the balanced dataset requires around two times as many questions to be answered, but has lower inter-annotator agreement and is thus slightly more difficult.",
    "label": "not enough info",
    "table_id": "58160fa7-ce6a-4ff8-805b-43776f982ae5"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).",
    "table_column_names": [
      "Model",
      "#Params",
      "Base",
      "+Elmo"
    ],
    "table_content_values": [
      [
        "rnet*",
        "-",
        "71.1/79.5",
        "-/-"
      ],
      [
        "LSTM",
        "2.67M",
        "[BOLD] 70.46/78.98",
        "75.17/82.79"
      ],
      [
        "GRU",
        "2.31M",
        "70.41/ [BOLD] 79.15",
        "75.81/83.12"
      ],
      [
        "ATR",
        "1.59M",
        "69.73/78.70",
        "75.06/82.76"
      ],
      [
        "SRU",
        "2.44M",
        "69.27/78.41",
        "74.56/82.50"
      ],
      [
        "LRN",
        "2.14M",
        "70.11/78.83",
        "[BOLD] 76.14/ [BOLD] 83.83"
      ]
    ],
    "id": "cc03ab7e-6e0d-45fd-a054-8cf38d14a2a1",
    "claim": "After integrating Elmo for contextual modeling, the performance of LRN does not reach the best (76.1 EM and 83.83 F1), with GRU and LSTM outperforming it (+0.33EM, +0.71F1).",
    "label": "refutes",
    "table_id": "e8718aba-7d2a-43da-ab9f-fe42d951b94a"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "91b08a45-84ea-4123-9ff5-e2588de34aa2",
    "claim": "RSI  = 89.20 doesn’t meet the requirement, but we measure the distance as 22.00 in the intrusion test, while we have 8 numbers between 119.99 and 120.00",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "737a3ba3-f0ff-476c-b995-bb8fc27b877e",
    "claim": "Its productivity of 57.5% expresses that it appears in in correct alternatives 7.5% more often than expected by random chance.",
    "label": "supports",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 4: Scores for different training objectives on the linguistic probing tasks.",
    "table_column_names": [
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "CMOW-C",
        "[BOLD] 36.2",
        "66.0",
        "81.1",
        "78.7",
        "61.7",
        "[BOLD] 83.9",
        "79.1",
        "73.6",
        "50.4",
        "66.8"
      ],
      [
        "CMOW-R",
        "35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "[BOLD] 80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "[BOLD] 74.2",
        "[BOLD] 50.7",
        "[BOLD] 72.9"
      ],
      [
        "CBOW-C",
        "[BOLD] 34.3",
        "[BOLD] 50.5",
        "[BOLD] 79.8",
        "[BOLD] 79.9",
        "53.0",
        "[BOLD] 75.9",
        "[BOLD] 79.8",
        "[BOLD] 72.9",
        "48.6",
        "89.0"
      ],
      [
        "CBOW-R",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "[BOLD] 53.6",
        "74.5",
        "78.6",
        "72.0",
        "[BOLD] 49.6",
        "[BOLD] 89.5"
      ]
    ],
    "id": "36419694-fbe7-448e-ab77-5a057e25f499",
    "claim": "While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent [CONTINUE] and BigramShift.",
    "label": "supports",
    "table_id": "d35d9a04-de0f-4f2b-ac85-d794d5212e41"
  },
  {
    "paper": "Low-supervision urgency detection and transfer in short crisis messages",
    "paper_id": "1907.06745v1",
    "table_caption": "TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
    "table_column_names": [
      "System",
      "Accuracy",
      "Precision",
      "Recall",
      "F-Measure"
    ],
    "table_content_values": [
      [
        "Local",
        "63.97%",
        "64.27%",
        "64.50%",
        "63.93%"
      ],
      [
        "Manual",
        "64.25%",
        "[BOLD] 70.84%∗∗",
        "48.50%",
        "57.11%"
      ],
      [
        "Wiki",
        "67.25%",
        "66.51%",
        "69.50%",
        "67.76%"
      ],
      [
        "Local-Manual",
        "65.75%",
        "67.96%",
        "59.50%",
        "62.96%"
      ],
      [
        "Wiki-Local",
        "67.40%",
        "65.54%",
        "68.50%",
        "66.80%"
      ],
      [
        "Wiki-Manual",
        "67.75%",
        "70.38%",
        "63.00%",
        "65.79%"
      ],
      [
        "[ITALIC] Our Approach",
        "[BOLD] 69.25%∗∗∗",
        "68.76%",
        "[BOLD] 70.50%∗∗",
        "[BOLD] 69.44%∗∗∗"
      ]
    ],
    "id": "a859b35d-7e4b-4d4d-b124-9e84252c100b",
    "claim": "The results illustrate the viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics.",
    "label": "supports",
    "table_id": "678b9f33-65b1-4b0a-b4f6-96de47883169"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "26d9a34b-a30a-474c-b97d-4bf387ec919a",
    "claim": "RoBERTa-large-FT was fine-tuned with a much higher learning rate (1e-5) to prevent an under-optimized model.",
    "label": "not enough info",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 1: Results on belinkov2014exploring’s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Schütze (2015) on GloVe.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] Initialization",
      "[BOLD] Embedding",
      "[BOLD] Resources",
      "[BOLD] Test Acc."
    ],
    "table_content_values": [
      [
        "HPCD (full)",
        "Syntactic-SG",
        "Type",
        "WordNet, VerbNet",
        "88.7"
      ],
      [
        "LSTM-PP",
        "GloVe",
        "Type",
        "-",
        "84.3"
      ],
      [
        "LSTM-PP",
        "GloVe-retro",
        "Type",
        "WordNet",
        "84.8"
      ],
      [
        "OntoLSTM-PP",
        "GloVe-extended",
        "Token",
        "WordNet",
        "[BOLD] 89.7"
      ]
    ],
    "id": "4f42503e-f21e-45b0-83e1-2d986870b0a9",
    "claim": "Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%.",
    "label": "supports",
    "table_id": "243a1721-f8dd-43b3-aa38-bf136c5048a9"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "d3085af2-d938-41fe-8453-0c632cca7716",
    "claim": "BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN.",
    "label": "supports",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "2a32cde4-28fb-4f0d-9e67-1a3ef3871e5b",
    "claim": "As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods.",
    "label": "supports",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "b9b60316-88c2-497b-a548-5474b6280198",
    "claim": "Pretrained Word2Sense embeddings outperform our method, however it has the advantage of training on a larger corpus.",
    "label": "supports",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "b2d6a815-61a5-4777-8eaa-3d472f4dfe3f",
    "claim": "Crucially, this performance difference holds even on the hard instances, which have been described as better tests of commonsense (Landauer et al., 1998).",
    "label": "not enough info",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Evaluation of Greek Word Embeddings",
    "paper_id": "1904.04032v3",
    "table_caption": "Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
    "table_column_names": [
      "Category Semantic",
      "Category no oov words",
      "gr_def 58.42%",
      "gr_neg10 59.33%",
      "cc.el.300  [BOLD] 68.80%",
      "wiki.el 27.20%",
      "gr_cbow_def 31.76%",
      "gr_d300_nosub 60.79%",
      "gr_w2v_sg_n5 52.70%"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "with oov words",
        "52.97%",
        "55.33%",
        "[BOLD] 64.34%",
        "25.73%",
        "28.80%",
        "55.11%",
        "47.82%"
      ],
      [
        "Syntactic",
        "no oov words",
        "65.73%",
        "61.02%",
        "[BOLD] 69.35%",
        "40.90%",
        "64.02%",
        "53.69%",
        "52.60%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "[BOLD] 53.95%",
        "48.69%",
        "49.43%",
        "28.42%",
        "52.54%",
        "44.06%",
        "43.13%"
      ],
      [
        "Overall",
        "no oov words",
        "63.02%",
        "59.96%",
        "[BOLD] 68.97%",
        "36.45%",
        "52.04%",
        "56.30%",
        "52.66%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "53.60%",
        "51.00%",
        "[BOLD] 54.60%",
        "27.50%",
        "44.30%",
        "47.90%",
        "44.80%"
      ]
    ],
    "id": "808f4732-df2d-4930-8f56-448c21b910eb",
    "claim": "Model wiki.el, trained only on Wikipedia, was the worst almost in every category (and sub-category).",
    "label": "supports",
    "table_id": "5b468728-2bb8-41a6-8b44-30b94d52dd3b"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "95e75058-7410-4ec2-a530-d83517280977",
    "claim": "Table 8 shows the results for the experimental configuration using all available heuristics.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "73f747e6-bd1f-459a-be5d-557593d0128c",
    "claim": "MLP with BERT as en(2018) coder has the best overall performance.",
    "label": "supports",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "a2ada531-d61d-4735-836c-cbb20e8c7d46",
    "claim": "When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set.",
    "label": "supports",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
    "table_column_names": [
      "[BOLD] Model",
      "D",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN(1)",
        "300",
        "10.9M",
        "20.9",
        "52.0"
      ],
      [
        "DCGCN(2)",
        "180",
        "10.9M",
        "[BOLD] 22.2",
        "[BOLD] 52.3"
      ],
      [
        "DCGCN(2)",
        "240",
        "11.3M",
        "22.8",
        "52.8"
      ],
      [
        "DCGCN(4)",
        "180",
        "11.4M",
        "[BOLD] 23.4",
        "[BOLD] 53.4"
      ],
      [
        "DCGCN(1)",
        "420",
        "12.6M",
        "22.2",
        "52.4"
      ],
      [
        "DCGCN(2)",
        "300",
        "12.5M",
        "23.8",
        "53.8"
      ],
      [
        "DCGCN(3)",
        "240",
        "12.3M",
        "[BOLD] 23.9",
        "[BOLD] 54.1"
      ],
      [
        "DCGCN(2)",
        "360",
        "14.0M",
        "24.2",
        "[BOLD] 54.4"
      ],
      [
        "DCGCN(3)",
        "300",
        "14.0M",
        "[BOLD] 24.4",
        "54.2"
      ],
      [
        "DCGCN(2)",
        "420",
        "15.6M",
        "24.1",
        "53.7"
      ],
      [
        "DCGCN(4)",
        "300",
        "15.6M",
        "[BOLD] 24.6",
        "[BOLD] 54.8"
      ],
      [
        "DCGCN(3)",
        "420",
        "18.6M",
        "24.5",
        "54.6"
      ],
      [
        "DCGCN(4)",
        "360",
        "18.4M",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "ced2b584-1a28-45df-92fc-3613d7dbcf34",
    "claim": "Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters.",
    "label": "supports",
    "table_id": "135bc50f-f12e-493b-854d-58858c4c5c86"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 1: Results on belinkov2014exploring’s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Schütze (2015) on GloVe.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] Initialization",
      "[BOLD] Embedding",
      "[BOLD] Resources",
      "[BOLD] Test Acc."
    ],
    "table_content_values": [
      [
        "HPCD (full)",
        "Syntactic-SG",
        "Type",
        "WordNet, VerbNet",
        "88.7"
      ],
      [
        "LSTM-PP",
        "GloVe",
        "Type",
        "-",
        "84.3"
      ],
      [
        "LSTM-PP",
        "GloVe-retro",
        "Type",
        "WordNet",
        "84.8"
      ],
      [
        "OntoLSTM-PP",
        "GloVe-extended",
        "Token",
        "WordNet",
        "[BOLD] 89.7"
      ]
    ],
    "id": "2496440e-30cf-465a-8de6-814a13e9b1f5",
    "claim": "OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset.",
    "label": "supports",
    "table_id": "243a1721-f8dd-43b3-aa38-bf136c5048a9"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "7643b874-54d1-4b3e-a2f6-d022395c9e6d",
    "claim": "For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9).",
    "label": "supports",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "88a0b5f9-de39-4d20-be1b-987819a85d13",
    "claim": "The domain prediction module (DPM) used in our GDPL and GDPL-discr is also trained and tested using their public codes in the end-to-end ALDM.",
    "label": "not enough info",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "8a7394db-ec6e-4ef6-845a-0eaeac590412",
    "claim": "However, the overall results in the English language show that, compared to the current state-of-the-art word embeddings models, a subspace was yet to be found that we could improve upon without jeopardizing the system for these tasks.",
    "label": "not enough info",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "0e60f569-bde5-4ada-8fbc-6176240824bf",
    "claim": "[CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the better performance and benefits more from data augmentation comparing to our proposed domain-aware multi-decoder network.",
    "label": "refutes",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Low-supervision urgency detection and transfer in short crisis messages",
    "paper_id": "1907.06745v1",
    "table_caption": "TABLE II: Details on datasets used for experiments.",
    "table_column_names": [
      "Dataset",
      "Unlabeled / Labeled Messages",
      "Urgent / Non-urgent Messages",
      "Unique Tokens",
      "Avg. Tokens / Message",
      "Time Range"
    ],
    "table_content_values": [
      [
        "Nepal",
        "6,063/400",
        "201/199",
        "1,641",
        "14",
        "04/05/2015-05/06/2015"
      ],
      [
        "Macedonia",
        "0/205",
        "92/113",
        "129",
        "18",
        "09/18/2018-09/21/2018"
      ],
      [
        "Kerala",
        "92,046/400",
        "125/275",
        "19,393",
        "15",
        "08/17/2018-08/22/2018"
      ]
    ],
    "id": "a2e66d42-21d8-4914-9262-6b5dac5f738d",
    "claim": "Table II shows that Nepal is roughly balanced, while Kerala is imbalanced.",
    "label": "supports",
    "table_id": "3c2720f2-239d-4696-81ea-ba4e9525afd4"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "2fc011a7-d4b3-4564-aad7-966f8e4fa993",
    "claim": "We report the two best performance for slot filling, for which we trained one system without ontology and another without ontology and coarse-grained slot types (Acc.)",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "2ff59490-1779-47f3-828f-e2c1600f71e9",
    "claim": "On the WinoCoref dataset, KnowComb does not improve by 15%.",
    "label": "refutes",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "35404383-b2cd-4672-9a7f-88226e5ef712",
    "claim": "On the other hand, compared to the BiLSTM baseline, PCS introduces significantly more in-scope (1,039) than out-of-scope (298) relations",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Training scheme",
      "[BOLD] News",
      "[BOLD] TED",
      "[BOLD] IT"
    ],
    "table_content_values": [
      [
        "1",
        "News",
        "37.8",
        "25.3",
        "35.3"
      ],
      [
        "2",
        "TED",
        "23.7",
        "24.1",
        "14.4"
      ],
      [
        "3",
        "IT",
        "1.6",
        "1.8",
        "39.6"
      ],
      [
        "4",
        "News and TED",
        "38.2",
        "25.5",
        "35.4"
      ],
      [
        "5",
        "1 then TED, No-reg",
        "30.6",
        "[BOLD] 27.0",
        "22.1"
      ],
      [
        "6",
        "1 then TED, L2",
        "37.9",
        "26.7",
        "31.8"
      ],
      [
        "7",
        "1 then TED, EWC",
        "[BOLD] 38.3",
        "[BOLD] 27.0",
        "33.1"
      ],
      [
        "8",
        "5 then IT, No-reg",
        "8.0",
        "6.9",
        "56.3"
      ],
      [
        "9",
        "6 then IT, L2",
        "32.3",
        "22.6",
        "56.9"
      ],
      [
        "10",
        "7 then IT, EWC",
        "35.8",
        "24.6",
        "[BOLD] 57.0"
      ]
    ],
    "id": "f658a810-80dc-48f8-b9fc-5445844a411c",
    "claim": "However, EWC does not outperform no-reg and L2 on News, as it only gives a 0.5 BLEU improvement over the baseline News model.",
    "label": "refutes",
    "table_id": "2c8215aa-6c63-49db-ae71-e1c029b6e82c"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 2: F1 score results per relation type of the best performing models.",
    "table_column_names": [
      "Relation type",
      "Count",
      "Intra-sentential co-occ.  [ITALIC] ρ=0",
      "Intra-sentential co-occ.  [ITALIC] ρ=5",
      "Intra-sentential co-occ.  [ITALIC] ρ=10",
      "BoC(Wiki-PubMed-PMC) LR",
      "BoC(Wiki-PubMed-PMC) SVM",
      "BoC(Wiki-PubMed-PMC) ANN"
    ],
    "table_content_values": [
      [
        "TherapyTiming(TP,TD)",
        "428",
        "[BOLD] 0.84",
        "0.59",
        "0.47",
        "0.78",
        "0.81",
        "0.78"
      ],
      [
        "NextReview(Followup,TP)",
        "164",
        "[BOLD] 0.90",
        "0.83",
        "0.63",
        "0.86",
        "0.88",
        "0.84"
      ],
      [
        "Toxicity(TP,CF/TR)",
        "163",
        "[BOLD] 0.91",
        "0.77",
        "0.55",
        "0.85",
        "0.86",
        "0.86"
      ],
      [
        "TestTiming(TN,TD/TP)",
        "184",
        "0.90",
        "0.81",
        "0.42",
        "0.96",
        "[BOLD] 0.97",
        "0.95"
      ],
      [
        "TestFinding(TN,TR)",
        "136",
        "0.76",
        "0.60",
        "0.44",
        "[BOLD] 0.82",
        "0.79",
        "0.78"
      ],
      [
        "Threat(O,CF/TR)",
        "32",
        "0.85",
        "0.69",
        "0.54",
        "[BOLD] 0.95",
        "[BOLD] 0.95",
        "0.92"
      ],
      [
        "Intervention(TP,YR)",
        "5",
        "[BOLD] 0.88",
        "0.65",
        "0.47",
        "-",
        "-",
        "-"
      ],
      [
        "EffectOf(Com,CF)",
        "3",
        "[BOLD] 0.92",
        "0.62",
        "0.23",
        "-",
        "-",
        "-"
      ],
      [
        "Severity(CF,CS)",
        "75",
        "[BOLD] 0.61",
        "0.53",
        "0.47",
        "0.52",
        "0.55",
        "0.51"
      ],
      [
        "RecurLink(YR,YR/CF)",
        "7",
        "[BOLD] 1.0",
        "[BOLD] 1.0",
        "0.64",
        "-",
        "-",
        "-"
      ],
      [
        "RecurInfer(NR/YR,TR)",
        "51",
        "0.97",
        "0.69",
        "0.43",
        "[BOLD] 0.99",
        "[BOLD] 0.99",
        "0.98"
      ],
      [
        "GetOpinion(Referral,CF/other)",
        "4",
        "[BOLD] 0.75",
        "[BOLD] 0.75",
        "0.5",
        "-",
        "-",
        "-"
      ],
      [
        "Context(Dis,DisCont)",
        "40",
        "[BOLD] 0.70",
        "0.63",
        "0.53",
        "0.60",
        "0.41",
        "0.57"
      ],
      [
        "TestToAssess(TN,CF/TR)",
        "36",
        "0.76",
        "0.66",
        "0.36",
        "[BOLD] 0.92",
        "[BOLD] 0.92",
        "0.91"
      ],
      [
        "TimeStamp(TD,TP)",
        "221",
        "[BOLD] 0.88",
        "0.83",
        "0.50",
        "0.86",
        "0.85",
        "0.83"
      ],
      [
        "TimeLink(TP,TP)",
        "20",
        "[BOLD] 0.92",
        "0.85",
        "0.45",
        "0.91",
        "[BOLD] 0.92",
        "0.90"
      ],
      [
        "Overall",
        "1569",
        "0.90",
        "0.73",
        "0.45",
        "0.92",
        "[BOLD] 0.93",
        "0.91"
      ]
    ],
    "id": "7fe42729-fa71-4447-b95a-5048ec662bce",
    "claim": "[CONTINUE] As the results of applying the co-occurrence baseline (ρ = 0) shows (Table 2), the semantic relations in this data are not strongly concentrated within a sentence boundary, as evidenced by the relatively low F1 scores for the relation of TestTiming (0.90) and TestFinding (0.76).",
    "label": "refutes",
    "table_id": "a49b4909-de4c-4505-8b9c-c37d9a0137c3"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 4: Precisions on the Wikidata dataset with different choice of d.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC",
      "Time"
    ],
    "table_content_values": [
      [
        "[ITALIC] d=1",
        "0.602",
        "0.487",
        "0.403",
        "0.367",
        "4h"
      ],
      [
        "[ITALIC] d=32",
        "0.645",
        "0.501",
        "0.393",
        "0.370",
        "-"
      ],
      [
        "[ITALIC] d=16",
        "0.655",
        "0.518",
        "0.413",
        "0.413",
        "20h"
      ],
      [
        "[ITALIC] d=8",
        "0.650",
        "0.519",
        "0.422",
        "0.405",
        "8h"
      ]
    ],
    "id": "9272ba5a-c8cb-4f21-97b1-40bf8f2b4834",
    "claim": "Compared to the original metapath2vec model with default d, by leveraging the right d, we improve performance at a better cost-efficiency.",
    "label": "not enough info",
    "table_id": "b3925d14-8042-410e-841f-73dce02fc49b"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "8dd82b54-3d4d-4100-a866-be3f6c35160f",
    "claim": "Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
    "label": "refutes",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 6: Results of the Human Rating on CWC.",
    "table_column_names": [
      "[EMPTY]",
      "Ours Better(%)",
      "No Prefer(%)",
      "Ours Worse(%)"
    ],
    "table_content_values": [
      [
        "Retrieval-Stgy ",
        "[BOLD] 62",
        "22",
        "16"
      ],
      [
        "PMI ",
        "[BOLD] 54",
        "32",
        "14"
      ],
      [
        "Neural ",
        "[BOLD] 60",
        "22",
        "18"
      ],
      [
        "Kernel ",
        "[BOLD] 62",
        "26",
        "12"
      ]
    ],
    "id": "2a9eb10f-7dee-4dc5-8c9a-663a90cb7c87",
    "claim": "Our agent does not outperform the comparison agents with a large margin.",
    "label": "refutes",
    "table_id": "2f4a12c2-27e3-4c98-984e-2b17d287642e"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "0c2c32c2-8636-49ee-8d00-5c4a3bdae286",
    "claim": "As the best comparison model, we investigate ablation models by removing parts from our model.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "fb607d1d-2d8e-4b05-ac94-895e601e5682",
    "claim": "However, our proposed method has comparable performance with the original GloVe embeddings.",
    "label": "supports",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Two Causal Principles for Improving Visual Dialog",
    "paper_id": "1911.10496v2",
    "table_caption": "Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.",
    "table_column_names": [
      "Model",
      "LF ",
      "HCIAE ",
      "CoAtt ",
      "RvA "
    ],
    "table_content_values": [
      [
        "baseline",
        "57.21",
        "56.98",
        "56.46",
        "56.74"
      ],
      [
        "+P1",
        "61.88",
        "60.12",
        "60.27",
        "61.02"
      ],
      [
        "+P2",
        "72.65",
        "71.50",
        "71.41",
        "71.44"
      ],
      [
        "+P1+P2",
        "[BOLD] 73.63",
        "71.99",
        "71.87",
        "72.88"
      ]
    ],
    "id": "ab2c2f1d-1b29-4c35-81bd-e2ea6c8073c8",
    "claim": "In general, our principle P2 can improve all the models in any ablative condition (i.e., P1, P2, P1+P2), while P1 does not always lead to an improvement.",
    "label": "refutes",
    "table_id": "df3ac154-71e4-4dc9-845e-7976d7dce3ae"
  },
  {
    "paper": "What do Deep Networks Like to Read?",
    "paper_id": "1909.04547v1",
    "table_caption": "Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
    "table_column_names": [
      "[EMPTY]",
      "<bold>RNN</bold>",
      "<bold>CNN</bold>",
      "<bold>DAN</bold>"
    ],
    "table_content_values": [
      [
        "Positive",
        "+9.7",
        "+4.3",
        "+<bold>23.6</bold>"
      ],
      [
        "Negative",
        "+6.9",
        "+5.5",
        "+<bold>16.1</bold>"
      ],
      [
        "Flipped to Positive",
        "+20.2",
        "+24.9",
        "+27.4"
      ],
      [
        "Flipped to Negative",
        "+31.5",
        "+28.6",
        "+19.3"
      ]
    ],
    "id": "ca69f7e8-9e0e-4870-859c-649e8f88eceb",
    "claim": "By considering only adjectives, we obtain a measure of the positive and negative score for each sentence before and after fine-tuning.",
    "label": "supports",
    "table_id": "fe569d5a-7fe9-4318-80c3-68d5d6108755"
  },
  {
    "paper": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns",
    "paper_id": "1810.05201v1",
    "table_caption": "Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
    "table_column_names": [
      "[EMPTY]",
      "M",
      "F",
      "B",
      "O"
    ],
    "table_content_values": [
      [
        "Random",
        "43.6",
        "39.3",
        "[ITALIC] 0.90",
        "41.5"
      ],
      [
        "Token Distance",
        "50.1",
        "42.4",
        "[ITALIC] 0.85",
        "46.4"
      ],
      [
        "Topical Entity",
        "51.5",
        "43.7",
        "[ITALIC] 0.85",
        "47.7"
      ],
      [
        "Syntactic Distance",
        "63.0",
        "56.2",
        "[ITALIC] 0.89",
        "59.7"
      ],
      [
        "Parallelism",
        "[BOLD] 67.1",
        "[BOLD] 63.1",
        "[ITALIC]  [BOLD] 0.94",
        "[BOLD] 65.2"
      ],
      [
        "Parallelism+URL",
        "[BOLD] 71.1",
        "[BOLD] 66.9",
        "[ITALIC]  [BOLD] 0.94",
        "[BOLD] 69.0"
      ],
      [
        "Transformer-Single",
        "58.6",
        "51.2",
        "[ITALIC] 0.87",
        "55.0"
      ],
      [
        "Transformer-Multi",
        "59.3",
        "52.9",
        "[ITALIC] 0.89",
        "56.2"
      ]
    ],
    "id": "f30d3d9e-a1e3-4e50-a778-882897039098",
    "claim": "[CONTINUE] TRANSFORMER-MULTI is stronger than TRANSFORMER-SINGLE [CONTINUE] .2% overall improvement over TRANSFORMER-SINGLE for the goldtwo-mention task.",
    "label": "supports",
    "table_id": "4e776dc6-bb84-4275-8461-46b00582b898"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "5bf06f43-9f2d-4b94-a6da-8fc836362016",
    "claim": "In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no significant effect on SER from cleaning the missed slots.",
    "label": "refutes",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "1c7fce05-5dac-4840-9535-6fd7e573863a",
    "claim": "Systems A-C are trained without the target type from which they report.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "aad0d5bd-5420-41cc-88eb-af2eaa32aac1",
    "claim": "In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks but WC and SOMO.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Towards Universal Dialogue State Tracking",
    "paper_id": "1810.09587v1",
    "table_caption": "Table 1: Joint goal accuracy on DSTC2 and WOZ 2.0 test set vs. various approaches as reported in the literature.",
    "table_column_names": [
      "[BOLD] DST Models",
      "[BOLD] Joint Acc. DSTC2",
      "[BOLD] Joint Acc. WOZ 2.0"
    ],
    "table_content_values": [
      [
        "Delexicalisation-Based (DB) Model Mrkšić et al. ( 2017 )",
        "69.1",
        "70.8"
      ],
      [
        "DB Model + Semantic Dictionary Mrkšić et al. ( 2017 )",
        "72.9",
        "83.7"
      ],
      [
        "Scalable Multi-domain DST Rastogi et al. ( 2017 )",
        "70.3",
        "-"
      ],
      [
        "MemN2N Perez and Liu ( 2017 )",
        "74.0",
        "-"
      ],
      [
        "PtrNet Xu and Hu ( 2018 )",
        "72.1",
        "-"
      ],
      [
        "Neural Belief Tracker: NBT-DNN Mrkšić et al. ( 2017 )",
        "72.6",
        "84.4"
      ],
      [
        "Neural Belief Tracker: NBT-CNN Mrkšić et al. ( 2017 )",
        "73.4",
        "84.2"
      ],
      [
        "Belief Tracking: Bi-LSTM Ramadan et al. ( 2018 )",
        "-",
        "85.1"
      ],
      [
        "Belief Tracking: CNN Ramadan et al. ( 2018 )",
        "-",
        "85.5"
      ],
      [
        "GLAD Zhong et al. ( 2018 )",
        "74.5",
        "88.1"
      ],
      [
        "StateNet",
        "74.1",
        "87.8"
      ],
      [
        "StateNet_PS",
        "74.5",
        "88.2"
      ],
      [
        "[BOLD] StateNet_PSI",
        "[BOLD] 75.5",
        "[BOLD] 88.9"
      ]
    ],
    "id": "dfb953a4-cb91-45be-98c4-5977336f6d6c",
    "claim": "StateNet PSI does not outperform StateNet, and StateNet PS performs best among all 3 models.",
    "label": "refutes",
    "table_id": "88cdb652-a571-4520-82af-622455d0ae90"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).",
    "table_column_names": [
      "Model",
      "#Params",
      "Base",
      "+Elmo"
    ],
    "table_content_values": [
      [
        "rnet*",
        "-",
        "71.1/79.5",
        "-/-"
      ],
      [
        "LSTM",
        "2.67M",
        "[BOLD] 70.46/78.98",
        "75.17/82.79"
      ],
      [
        "GRU",
        "2.31M",
        "70.41/ [BOLD] 79.15",
        "75.81/83.12"
      ],
      [
        "ATR",
        "1.59M",
        "69.73/78.70",
        "75.06/82.76"
      ],
      [
        "SRU",
        "2.44M",
        "69.27/78.41",
        "74.56/82.50"
      ],
      [
        "LRN",
        "2.14M",
        "70.11/78.83",
        "[BOLD] 76.14/ [BOLD] 83.83"
      ]
    ],
    "id": "6468a0b4-715d-495e-9627-1f859a1198cb",
    "claim": "In this task, ATR and SRU outperform LRN in terms of both EM and F1 score.",
    "label": "refutes",
    "table_id": "e8718aba-7d2a-43da-ab9f-fe42d951b94a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "a8cb7bdd-dbe4-4991-8ecd-99ee870ab569",
    "claim": "We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets. As Table 4 shows, previous models perform significantly better on the Easy subset than on the Hard subset, with the exception of Sasaki et al.",
    "label": "refutes",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "d244fa95-6080-43f6-869e-02dcacc260ce",
    "claim": "[CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] Words regularly describing negative sentiment or emotions (such as 'not', 'my', and 'can't') are among the most distinctive features for complaints.",
    "label": "refutes",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.",
    "table_column_names": [
      "[ITALIC] k",
      "Ar",
      "Es",
      "Fr",
      "Ru",
      "Zh",
      "En"
    ],
    "table_content_values": [
      [
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy"
      ],
      [
        "0",
        "88.0",
        "87.9",
        "87.9",
        "87.8",
        "87.7",
        "87.4"
      ],
      [
        "1",
        "92.4",
        "91.9",
        "92.1",
        "92.1",
        "91.5",
        "89.4"
      ],
      [
        "2",
        "91.9",
        "91.8",
        "91.8",
        "91.8",
        "91.3",
        "88.3"
      ],
      [
        "3",
        "92.0",
        "92.3",
        "92.1",
        "91.6",
        "91.2",
        "87.9"
      ],
      [
        "4",
        "92.1",
        "92.4",
        "92.5",
        "92.0",
        "90.5",
        "86.9"
      ],
      [
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy"
      ],
      [
        "0",
        "81.9",
        "81.9",
        "81.8",
        "81.8",
        "81.8",
        "81.2"
      ],
      [
        "1",
        "87.9",
        "87.7",
        "87.8",
        "87.9",
        "87.7",
        "84.5"
      ],
      [
        "2",
        "87.4",
        "87.5",
        "87.4",
        "87.3",
        "87.2",
        "83.2"
      ],
      [
        "3",
        "87.8",
        "87.9",
        "87.9",
        "87.3",
        "87.3",
        "82.9"
      ],
      [
        "4",
        "88.3",
        "88.6",
        "88.4",
        "88.1",
        "87.7",
        "82.1"
      ],
      [
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU"
      ],
      [
        "[EMPTY]",
        "32.7",
        "49.1",
        "38.5",
        "34.2",
        "32.1",
        "96.6"
      ]
    ],
    "id": "4e1bee90-35f0-4df8-ba48-b2a758e2d9d6",
    "claim": "[CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 only marginally boost the performance to around 87-88%, [CONTINUE] which is not significantly higher than the UnsupEmb and MFT baselines.",
    "label": "refutes",
    "table_id": "430d822d-f0b9-4d4b-b9a6-b995a9e11686"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "59e2114d-5576-4698-9ac1-bea4da38592d",
    "claim": "Dual2seq is not consistently better than the other systems under all three metrics, [CONTINUE] as OpenNMT-tf and Transformer-tf both outperform Dual2seq in terms of BLEU and Meteor scores.",
    "label": "refutes",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "e7e9e3d8-3d31-42cb-bc2c-98491549fcb7",
    "claim": "This is particularly noteworthy because our user simulator takes a very strict agenda (Section 4.1) compared to that of humans, which is more dynamic and changing as the conversation continues.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "8c8887ea-cdea-48c2-bc39-07dde08b7c8d",
    "claim": "( 2018 )) and the rank correlation between NeuralTD and human summaries is higher than with supervised models.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "78f1241f-560b-413e-aeeb-e7d909a8b4fd",
    "claim": "WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams.",
    "label": "supports",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "cca89ac1-e731-42b7-b5c8-0b1ba49eae32",
    "claim": "The summaries generated by our system receive decent ROUGE metrics, but are lower than most of the recent systems, because our learned reward is optimised towards high correlation with human judgement instead of ROUGE metrics.",
    "label": "supports",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 3: ARI and Silhouette coefficient scores.",
    "table_column_names": [
      "Methods",
      "Seanad Abolition ARI",
      "Seanad Abolition  [ITALIC] Sil",
      "Video Games ARI",
      "Video Games  [ITALIC] Sil",
      "Pornography ARI",
      "Pornography  [ITALIC] Sil"
    ],
    "table_content_values": [
      [
        "TF-IDF",
        "0.23",
        "0.02",
        "-0.01",
        "0.01",
        "-0.02",
        "0.01"
      ],
      [
        "WMD",
        "0.09",
        "0.01",
        "0.01",
        "0.01",
        "-0.02",
        "0.01"
      ],
      [
        "Sent2vec",
        "-0.01",
        "-0.01",
        "0.11",
        "0.06",
        "0.01",
        "0.02"
      ],
      [
        "Doc2vec",
        "-0.01",
        "-0.03",
        "-0.01",
        "0.01",
        "0.02",
        "-0.01"
      ],
      [
        "BERT",
        "0.03",
        "-0.04",
        "0.08",
        "0.05",
        "-0.01",
        "0.03"
      ],
      [
        "OD-parse",
        "0.01",
        "-0.04",
        "-0.01",
        "0.02",
        "0.07",
        "0.05"
      ],
      [
        "OD",
        "[BOLD] 0.54",
        "[BOLD] 0.31",
        "[BOLD] 0.56",
        "[BOLD] 0.42",
        "[BOLD] 0.41",
        "[BOLD] 0.41"
      ]
    ],
    "id": "7cb9f68a-d66f-4adf-b12d-35a2fd47dd67",
    "claim": "among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec do not achieve high ARI and Silhouette coefficient scores on the \"Video Games\" and \"Pornography\" datasets.",
    "label": "refutes",
    "table_id": "41274426-d552-4e87-b3ba-efed0c4b05ab"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "38.4",
        "0.958"
      ],
      [
        "Wiener filter",
        "41.0",
        "0.775"
      ],
      [
        "Minimizing DCE",
        "31.1",
        "[BOLD] 0.392"
      ],
      [
        "FSEGAN",
        "29.1",
        "0.421"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "27.7",
        "0.476"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 26.1",
        "0.462"
      ],
      [
        "Clean speech",
        "9.3",
        "0.0"
      ]
    ],
    "id": "dad6a4ed-cf24-42d0-9293-ac3ed0d9efcf",
    "claim": "The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE.",
    "label": "refutes",
    "table_id": "f609290e-ef88-4bfc-acb0-8d92e9cca315"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 2: Ratings of annotated NLDs by human judges.",
    "table_column_names": [
      "# steps",
      "Reachability",
      "Derivability Step 1",
      "Derivability Step 2",
      "Derivability Step 3"
    ],
    "table_content_values": [
      [
        "1",
        "3.0",
        "3.8",
        "-",
        "-"
      ],
      [
        "2",
        "2.8",
        "3.8",
        "3.7",
        "-"
      ],
      [
        "3",
        "2.3",
        "3.9",
        "3.8",
        "3.8"
      ]
    ],
    "id": "f0edce30-c35c-41ad-b5f3-719e7e112bb0",
    "claim": "[CONTINUE] On the other hand, we found the quality of 3-step NLDs is relatively lower than the others.",
    "label": "supports",
    "table_id": "8326a179-f543-4f97-8229-d2ed8172a663"
  },
  {
    "paper": "The MeMAD Submission to the WMT18 Multimodal Translation Task",
    "paper_id": "1808.10802v2",
    "table_caption": "Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and − for removing a component or data set. Multiple modifications are indicated by increasing the indentation.",
    "table_column_names": [
      "en-fr",
      "flickr16",
      "flickr17",
      "mscoco17"
    ],
    "table_content_values": [
      [
        "subs3M [ITALIC]  [ITALIC] LM detectron",
        "68.30",
        "62.45",
        "52.86"
      ],
      [
        "+ensemble-of-3",
        "68.72",
        "62.70",
        "53.06"
      ],
      [
        "−visual features",
        "[BOLD] 68.74",
        "[BOLD] 62.71",
        "53.14"
      ],
      [
        "−MS-COCO",
        "67.13",
        "61.17",
        "[BOLD] 53.34"
      ],
      [
        "−multi-lingual",
        "68.21",
        "61.99",
        "52.40"
      ],
      [
        "subs6M [ITALIC]  [ITALIC] LM detectron",
        "68.29",
        "61.73",
        "53.05"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM gn2048",
        "67.74",
        "61.78",
        "52.76"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM text-only",
        "67.72",
        "61.75",
        "53.02"
      ],
      [
        "en-de",
        "flickr16",
        "flickr17",
        "mscoco17"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM detectron",
        "45.09",
        "40.81",
        "36.94"
      ],
      [
        "+ensemble-of-3",
        "45.52",
        "[BOLD] 41.84",
        "[BOLD] 37.49"
      ],
      [
        "−visual features",
        "[BOLD] 45.59",
        "41.75",
        "37.43"
      ],
      [
        "−MS-COCO",
        "45.11",
        "40.52",
        "36.47"
      ],
      [
        "−multi-lingual",
        "44.95",
        "40.09",
        "35.28"
      ],
      [
        "subs6M [ITALIC]  [ITALIC] LM detectron",
        "45.50",
        "41.01",
        "36.81"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM gn2048",
        "45.38",
        "40.07",
        "36.82"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM text-only",
        "44.87",
        "41.27",
        "36.59"
      ],
      [
        "+multi-modal finetune",
        "44.56",
        "41.61",
        "36.93"
      ]
    ],
    "id": "7099710f-edb1-4afd-ab16-47381f257b0d",
    "claim": "When the experiment was repeated so that the finetuning phase included the text-only data, the performance did not return to approximately the same level as without tuning (+multi-modal finetune row in Table 6).",
    "label": "refutes",
    "table_id": "dccd5c25-66c7-421e-b062-ae2e1f5c88d2"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 2: Experiment 1",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.001",
        "0.003",
        "-20.818",
        "***",
        "0.505"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.083",
        "0.048",
        "101.636",
        "***",
        "1.724"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.001",
        "0.001",
        "0.035",
        "[EMPTY]",
        "1.001"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.023",
        "0.012",
        "64.418",
        "***",
        "1.993"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.002",
        "0.001",
        "4.047",
        "***",
        "1.120"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.049",
        "0.019",
        "120.986",
        "***",
        "2.573"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.173",
        "0.065",
        "243.285",
        "***",
        "2.653"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.032",
        "0.023",
        "39.483",
        "***",
        "1.396"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.111",
        "0.061",
        "122.707",
        "***",
        "1.812"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.178",
        "0.080",
        "211.319",
        "***",
        "2.239"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.028",
        "0.015",
        "63.131",
        "***",
        "1.854"
      ]
    ],
    "id": "1b8acf49-27ee-44b2-a81b-2ea143aa00f0",
    "claim": "(2017), we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set.",
    "label": "supports",
    "table_id": "661ddd6e-51d9-446c-add1-3e80f3b78109"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] # pairs",
      "[BOLD] # words (doc)",
      "[BOLD] # sents (docs)",
      "[BOLD] # words (summary)",
      "[BOLD] # sents (summary)",
      "[BOLD] vocab size"
    ],
    "table_content_values": [
      [
        "Multi-News",
        "44,972/5,622/5,622",
        "2,103.49",
        "82.73",
        "263.66",
        "9.97",
        "666,515"
      ],
      [
        "DUC03+04",
        "320",
        "4,636.24",
        "173.15",
        "109.58",
        "2.88",
        "19,734"
      ],
      [
        "TAC 2011",
        "176",
        "4,695.70",
        "188.43",
        "99.70",
        "1.00",
        "24,672"
      ],
      [
        "CNNDM",
        "287,227/13,368/11,490",
        "810.57",
        "39.78",
        "56.20",
        "3.68",
        "717,951"
      ]
    ],
    "id": "74e9434e-0d7c-4c20-8f65-b02bf1e43667",
    "claim": "The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data.",
    "label": "supports",
    "table_id": "9fda9198-5e75-4f79-b42c-82b7dc3ee31f"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "52c61ca7-ec3c-4313-bd66-c792ef9bf414",
    "claim": "Despite LRN and oLRN having faster training times than SRU (+15%/+6%), SRU still achieves a higher BLEU score.",
    "label": "refutes",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "88f1cf27-946a-4442-98dc-02d34534f76e",
    "claim": "Additionally, when using bounding box features, sparsemax outperforms softmax, showing that selecting only the bounding boxes of the relevant objects leads to a better answering capability.",
    "label": "supports",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.",
    "table_column_names": [
      "[EMPTY]",
      "MFT",
      "UnsupEmb",
      "Word2Tag"
    ],
    "table_content_values": [
      [
        "POS",
        "91.95",
        "87.06",
        "95.55"
      ],
      [
        "SEM",
        "82.00",
        "81.11",
        "91.41"
      ]
    ],
    "id": "b90516c0-5d54-4636-b318-cdc83d2fce12",
    "claim": "The UnsupEmb baseline performs comparably to the Word2Tag upper bound on both POS and SEM tagging.",
    "label": "refutes",
    "table_id": "5460f475-54e3-4b59-bbdf-d334c4acc090"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 4: Lexicon member coverage (%)",
    "table_column_names": [
      "target",
      "VN",
      "WN-V",
      "WN-N"
    ],
    "table_content_values": [
      [
        "type",
        "81",
        "66",
        "47"
      ],
      [
        "x+POS",
        "54",
        "39",
        "43"
      ],
      [
        "lemma",
        "88",
        "76",
        "53"
      ],
      [
        "x+POS",
        "79",
        "63",
        "50"
      ],
      [
        "shared",
        "54",
        "39",
        "41"
      ]
    ],
    "id": "406069b0-825f-4f0b-b982-048d1e765fcf",
    "claim": "POS-disambiguation does not fragment the vocabulary and consistently increases the coverage with the effect being more pronounced for lemmatized targets.",
    "label": "refutes",
    "table_id": "9a7863da-4529-4b1d-87c0-1184b0fcd5dd"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 2: Experiment 1",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.001",
        "0.003",
        "-20.818",
        "***",
        "0.505"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.083",
        "0.048",
        "101.636",
        "***",
        "1.724"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.001",
        "0.001",
        "0.035",
        "[EMPTY]",
        "1.001"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.023",
        "0.012",
        "64.418",
        "***",
        "1.993"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.002",
        "0.001",
        "4.047",
        "***",
        "1.120"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.049",
        "0.019",
        "120.986",
        "***",
        "2.573"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.173",
        "0.065",
        "243.285",
        "***",
        "2.653"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.032",
        "0.023",
        "39.483",
        "***",
        "1.396"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.111",
        "0.061",
        "122.707",
        "***",
        "1.812"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.178",
        "0.080",
        "211.319",
        "***",
        "2.239"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.028",
        "0.015",
        "63.131",
        "***",
        "1.854"
      ]
    ],
    "id": "c9a67956-a20f-488b-bd85-062a6fc04a01",
    "claim": "In most cases the racial disparities persist, and are generally larger in magnitude than the disparities for other classes.",
    "label": "refutes",
    "table_id": "661ddd6e-51d9-446c-add1-3e80f3b78109"
  },
  {
    "paper": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "paper_id": "1909.03546v2",
    "table_caption": "Table 3: F1 scores on Relation.",
    "table_column_names": [
      "[EMPTY]",
      "ACE05",
      "SciERC",
      "WLPC"
    ],
    "table_content_values": [
      [
        "BERT + LSTM",
        "60.6",
        "40.3",
        "65.1"
      ],
      [
        "+RelProp",
        "61.9",
        "41.1",
        "65.3"
      ],
      [
        "+CorefProp",
        "59.7",
        "42.6",
        "-"
      ],
      [
        "BERT FineTune",
        "[BOLD] 62.1",
        "44.3",
        "65.4"
      ],
      [
        "+RelProp",
        "62.0",
        "43.0",
        "[BOLD] 65.5"
      ],
      [
        "+CorefProp",
        "60.0",
        "[BOLD] 45.3",
        "-"
      ]
    ],
    "id": "c67da597-da7b-4c75-99a6-5184dfb0c485",
    "claim": "CorefProp also improves relation extraction on SciERC.",
    "label": "supports",
    "table_id": "3fc48613-1f39-4968-a1bc-84c2cae46001"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "fede62ad-8591-411a-974e-a263d0e6dd91",
    "claim": "LRN obtains an accuracy of 90.49 with BERT, the highest among all models.",
    "label": "refutes",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "dafe0889-31b8-48ce-a240-cad9ffdebc2e",
    "claim": "G2S-GGNN has 33.5% and 5.2% better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively.",
    "label": "supports",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "[BOLD] Baselines",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen ( 2015a )",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. ( 2018 )",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "[BOLD] Model Variants",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "fd55824c-1680-4200-be35-96a65ac28a9b",
    "claim": "The intuition here is that each model is optimizing different signals (lexical matching and type accuracy), which may or may not be independent.",
    "label": "not enough info",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).",
    "table_column_names": [
      "[EMPTY]",
      "dev CS",
      "dev mono",
      "test CS",
      "test mono"
    ],
    "table_content_values": [
      [
        "CS-only-LM",
        "45.20",
        "65.87",
        "43.20",
        "62.80"
      ],
      [
        "Fine-Tuned-LM",
        "49.60",
        "72.67",
        "47.60",
        "71.33"
      ],
      [
        "CS-only-disc",
        "[BOLD] 75.60",
        "70.40",
        "70.80",
        "70.53"
      ],
      [
        "Fine-Tuned-disc",
        "70.80",
        "[BOLD] 74.40",
        "[BOLD] 75.33",
        "[BOLD] 75.87"
      ]
    ],
    "id": "96bf0f6b-f429-4648-bab7-cf3759539016",
    "claim": "Table 5 breaks down the results of the different models according to two conditions: when the gold sentence is code-switched, and when the gold sentence is monolingual.",
    "label": "supports",
    "table_id": "1e599458-4bd1-4761-9fc4-8743eb9e544b"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "03375542-1aaf-400c-9743-0e332dd4183b",
    "claim": "According to the table, the drop of precision demonstrates that the word-level attention is quite useful.",
    "label": "supports",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 4: Scores for different training objectives on the linguistic probing tasks.",
    "table_column_names": [
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "CMOW-C",
        "[BOLD] 36.2",
        "66.0",
        "81.1",
        "78.7",
        "61.7",
        "[BOLD] 83.9",
        "79.1",
        "73.6",
        "50.4",
        "66.8"
      ],
      [
        "CMOW-R",
        "35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "[BOLD] 80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "[BOLD] 74.2",
        "[BOLD] 50.7",
        "[BOLD] 72.9"
      ],
      [
        "CBOW-C",
        "[BOLD] 34.3",
        "[BOLD] 50.5",
        "[BOLD] 79.8",
        "[BOLD] 79.9",
        "53.0",
        "[BOLD] 75.9",
        "[BOLD] 79.8",
        "[BOLD] 72.9",
        "48.6",
        "89.0"
      ],
      [
        "CBOW-R",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "[BOLD] 53.6",
        "74.5",
        "78.6",
        "72.0",
        "[BOLD] 49.6",
        "[BOLD] 89.5"
      ]
    ],
    "id": "966e1252-7bc6-49de-8cde-ddf9dd9771a0",
    "claim": "While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points higher scores on WordContent and BigramShift.",
    "label": "refutes",
    "table_id": "d35d9a04-de0f-4f2b-ac85-d794d5212e41"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
    "table_column_names": [
      "Uni",
      "POS",
      "0 87.9",
      "1 92.0",
      "2 91.7",
      "3 91.8",
      "4 91.9"
    ],
    "table_content_values": [
      [
        "Uni",
        "SEM",
        "81.8",
        "87.8",
        "87.4",
        "87.6",
        "88.2"
      ],
      [
        "Bi",
        "POS",
        "87.9",
        "93.3",
        "92.9",
        "93.2",
        "92.8"
      ],
      [
        "Bi",
        "SEM",
        "81.9",
        "91.3",
        "90.8",
        "91.9",
        "91.9"
      ],
      [
        "Res",
        "POS",
        "87.9",
        "92.5",
        "91.9",
        "92.0",
        "92.4"
      ],
      [
        "Res",
        "SEM",
        "81.9",
        "88.2",
        "87.5",
        "87.6",
        "88.5"
      ]
    ],
    "id": "07e54d6a-f1de-49d1-8249-96055ac0191a",
    "claim": "We also observe similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations.",
    "label": "supports",
    "table_id": "56a61e48-903a-4dc4-8cbf-2048d2c8ee3c"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "63fe7961-86a5-4ceb-9556-1f4592c15b2c",
    "claim": "After removing the graph attention module, our model gives 24.9 BLEU points.",
    "label": "supports",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "12a69a90-7404-45aa-b347-5867d2287a46",
    "claim": "the distribution on dialog success criteria with ACER has the least bias among all.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 3: Performance comparison of our model with different values of m on the two datasets.",
    "table_column_names": [
      "[ITALIC] m",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "1",
        "0.541",
        "0.595",
        "[BOLD] 0.566",
        "0.495",
        "0.621",
        "0.551"
      ],
      [
        "2",
        "0.521",
        "0.597",
        "0.556",
        "0.482",
        "0.656",
        "0.555"
      ],
      [
        "3",
        "0.490",
        "0.617",
        "0.547",
        "0.509",
        "0.633",
        "0.564"
      ],
      [
        "4",
        "0.449",
        "0.623",
        "0.522",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "5",
        "0.467",
        "0.609",
        "0.529",
        "0.488",
        "0.677",
        "0.567"
      ]
    ],
    "id": "9df0b311-fa64-4aaa-b766-568444a3f1a9",
    "claim": "We observe that for the NYT10 dataset, m = {1, 2, 3} gives good performance with m = 1 achieving the highest F1 score.",
    "label": "supports",
    "table_id": "fbd16514-f1a6-4567-8557-22b8d673c5b6"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "3ce2ce6f-939a-4a30-b4ec-06b484e1ae64",
    "claim": "(2017).8 Overall both BERT (76.5%) and [CONTINUE] RoBERTa (87.7%) considerably outperform the best previous model (71.4%).",
    "label": "supports",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "dfff7b52-8d27-4ac2-8e6a-fb62361453d3",
    "claim": "In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources.",
    "label": "supports",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "8a6a5782-c6a0-428a-aadb-4813c0a9d2ae",
    "claim": "The larger performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely more on superficial cues.",
    "label": "refutes",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 2: Image-caption ranking results for German (Multi30k)",
    "table_column_names": [
      "[EMPTY]",
      "Image to Text R@1",
      "Image to Text R@5",
      "Image to Text R@10",
      "Image to Text Mr",
      "Text to Image R@1",
      "Text to Image R@5",
      "Text to Image R@10",
      "Text to Image Mr",
      "Alignment"
    ],
    "table_content_values": [
      [
        "[BOLD] symmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Parallel gella:17",
        "28.2",
        "57.7",
        "71.3",
        "4",
        "20.9",
        "46.9",
        "59.3",
        "6",
        "-"
      ],
      [
        "Mono",
        "34.2",
        "67.5",
        "79.6",
        "3",
        "26.5",
        "54.7",
        "66.2",
        "4",
        "-"
      ],
      [
        "FME",
        "36.8",
        "69.4",
        "80.8",
        "2",
        "26.6",
        "56.2",
        "68.5",
        "4",
        "76.81%"
      ],
      [
        "AME",
        "[BOLD] 39.6",
        "[BOLD] 72.7",
        "[BOLD] 82.7",
        "[BOLD] 2",
        "[BOLD] 28.9",
        "[BOLD] 58.0",
        "[BOLD] 68.7",
        "[BOLD] 4",
        "66.91%"
      ],
      [
        "[BOLD] asymmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Pivot gella:17",
        "28.2",
        "61.9",
        "73.4",
        "3",
        "22.5",
        "49.3",
        "61.7",
        "6",
        "-"
      ],
      [
        "Parallel gella:17",
        "30.2",
        "60.4",
        "72.8",
        "3",
        "21.8",
        "50.5",
        "62.3",
        "5",
        "-"
      ],
      [
        "Mono",
        "[BOLD] 42.0",
        "72.5",
        "83.0",
        "2",
        "29.6",
        "58.4",
        "69.6",
        "4",
        "-"
      ],
      [
        "FME",
        "40.5",
        "73.3",
        "83.4",
        "2",
        "29.6",
        "59.2",
        "[BOLD] 72.1",
        "3",
        "76.81%"
      ],
      [
        "AME",
        "40.5",
        "[BOLD] 74.3",
        "[BOLD] 83.4",
        "[BOLD] 2",
        "[BOLD] 31.0",
        "[BOLD] 60.5",
        "70.6",
        "[BOLD] 3",
        "73.10%"
      ]
    ],
    "id": "4ca84e7f-8b85-4b40-8506-e06ed09099af",
    "claim": "For German descriptions, The results are 11.05% worse on average compared to (Gella et al., 2017) in symmetric mode.",
    "label": "refutes",
    "table_id": "abdcea15-c5a5-40cd-91fe-ebc83872eccc"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.",
    "table_column_names": [
      "[ITALIC] Block",
      "[ITALIC] n",
      "[ITALIC] m",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "1",
        "1",
        "1",
        "17.6",
        "48.3"
      ],
      [
        "1",
        "1",
        "2",
        "19.2",
        "50.3"
      ],
      [
        "1",
        "2",
        "1",
        "18.4",
        "49.1"
      ],
      [
        "1",
        "1",
        "3",
        "19.6",
        "49.4"
      ],
      [
        "1",
        "3",
        "1",
        "20.0",
        "50.5"
      ],
      [
        "1",
        "3",
        "3",
        "21.4",
        "51.0"
      ],
      [
        "1",
        "3",
        "6",
        "21.8",
        "51.7"
      ],
      [
        "1",
        "6",
        "3",
        "21.7",
        "51.5"
      ],
      [
        "1",
        "6",
        "6",
        "22.0",
        "52.1"
      ],
      [
        "2",
        "3",
        "6",
        "[BOLD] 23.5",
        "53.3"
      ],
      [
        "2",
        "6",
        "3",
        "23.3",
        "[BOLD] 53.4"
      ],
      [
        "2",
        "6",
        "6",
        "22.0",
        "52.1"
      ]
    ],
    "id": "8a3ce3da-5db7-48f2-929f-2d27febabd8a",
    "claim": "In general, the performance increases when we gradually enlarge n and m. For example, when n=1 and m=1, the BLEU score is 17.6; when n=6 and m=6, the BLEU score becomes 22.0.",
    "label": "supports",
    "table_id": "9ac477a4-1794-4d29-8be4-94cfe8c3b70a"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 1: Effect of using the shortest dependency path on each relation type.",
    "table_column_names": [
      "[BOLD] Relation",
      "[BOLD] best F1 (in 5-fold) without sdp",
      "[BOLD] best F1 (in 5-fold) with sdp",
      "[BOLD] Diff."
    ],
    "table_content_values": [
      [
        "USAGE",
        "60.34",
        "80.24",
        "+ 19.90"
      ],
      [
        "MODEL-FEATURE",
        "48.89",
        "70.00",
        "+ 21.11"
      ],
      [
        "PART_WHOLE",
        "29.51",
        "70.27",
        "+40.76"
      ],
      [
        "TOPIC",
        "45.80",
        "91.26",
        "+45.46"
      ],
      [
        "RESULT",
        "54.35",
        "81.58",
        "+27.23"
      ],
      [
        "COMPARE",
        "20.00",
        "61.82",
        "+ 41.82"
      ],
      [
        "macro-averaged",
        "50.10",
        "76.10",
        "+26.00"
      ]
    ],
    "id": "37eee057-4b1f-43f6-9889-4aa72045c882",
    "claim": "However, the sdp information does not have a clear positive impact on all the relation types (Table 1).",
    "label": "refutes",
    "table_id": "1eca16bf-a63a-45a1-a4a3-84f12cf74c95"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "6d4f98b8-fffb-4b2b-b1b6-43eccf4cda41",
    "claim": "Reward 3, i.e., preference between generated summaries and reference, has slightly higher correlations with system performance than Reward 1, i.e., difference in summary properties from statistical values computed on references (regression loss in Eq.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] External",
      "B"
    ],
    "table_content_values": [
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "-",
        "22.0"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "-",
        "23.3"
      ],
      [
        "GCNSEQ (Damonte and Cohen,  2019 )",
        "-",
        "24.4"
      ],
      [
        "DCGCN(single)",
        "-",
        "25.9"
      ],
      [
        "DCGCN(ensemble)",
        "-",
        "[BOLD] 28.2"
      ],
      [
        "TSP (Song et al.,  2016 )",
        "ALL",
        "22.4"
      ],
      [
        "PBMT (Pourdamghani et al.,  2016 )",
        "ALL",
        "26.9"
      ],
      [
        "Tree2Str (Flanigan et al.,  2016 )",
        "ALL",
        "23.0"
      ],
      [
        "SNRG (Song et al.,  2017 )",
        "ALL",
        "25.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "0.2M",
        "27.4"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "0.2M",
        "28.2"
      ],
      [
        "DCGCN(single)",
        "0.1M",
        "29.0"
      ],
      [
        "DCGCN(single)",
        "0.2M",
        "[BOLD] 31.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "2M",
        "32.3"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "2M",
        "33.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "20M",
        "33.8"
      ],
      [
        "DCGCN(single)",
        "0.3M",
        "33.2"
      ],
      [
        "DCGCN(ensemble)",
        "0.3M",
        "[BOLD] 35.3"
      ]
    ],
    "id": "cbc48b80-a560-4ead-aeb2-98b1dcb907eb",
    "claim": "DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data.",
    "label": "supports",
    "table_id": "f7b025d4-ffc5-4764-9949-312c3463da35"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "0cfe420b-02cd-4738-a54e-42200ab18629",
    "claim": "their recall are 0.595, 0.517, and 0.441 on three thresholds 0.1, 0.2 and 0.3 respectively, while our model achieves 0.650, 0.519, 0.422.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "f5d1e12a-54c7-4133-aa0e-bd0d0447cdcf",
    "claim": "this impressive improvement comes from the large dataset and considerable time spent on hyperparameter tuning, but only better-than-human results compared to RoBERTa and BERT finetuning.",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "38.4",
        "0.958"
      ],
      [
        "Wiener filter",
        "41.0",
        "0.775"
      ],
      [
        "Minimizing DCE",
        "31.1",
        "[BOLD] 0.392"
      ],
      [
        "FSEGAN",
        "29.1",
        "0.421"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "27.7",
        "0.476"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 26.1",
        "0.462"
      ],
      [
        "Clean speech",
        "9.3",
        "0.0"
      ]
    ],
    "id": "bd1eba72-ce56-4f45-a304-cb354ff75544",
    "claim": "acoustic supervision (27.7%) and multi-task learning (26.1%) show higher WER than minimizing DCE (31.1%) and FSEGAN (29.1%).",
    "label": "refutes",
    "table_id": "f609290e-ef88-4bfc-acb0-8d92e9cca315"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "724dc847-9047-4128-87b2-58f815992054",
    "claim": "Furthermore, it also yields better policy matches, except for PPO, suggesting that GDPL is more compatible with the real users.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "3856e931-c7bc-424a-b97f-9e63e61dc3a6",
    "claim": "This is particularly true for the BIDAF model.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "42608cfc-8066-4ba5-b5c5-56394189d947",
    "claim": "therefore, the role of attention in link prediction can be explained.",
    "label": "not enough info",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "1f607985-944c-457a-878b-94f8d36c7b28",
    "claim": "In addition, our single DCGCN model obtains better results than previous ensemble models.",
    "label": "supports",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Evaluation of Greek Word Embeddings",
    "paper_id": "1904.04032v3",
    "table_caption": "Table 4: Word similarity.",
    "table_column_names": [
      "Model",
      "Pearson",
      "p-value",
      "Pairs (unknown)"
    ],
    "table_content_values": [
      [
        "gr_def",
        "[BOLD] 0.6042",
        "3.1E-35",
        "2.3%"
      ],
      [
        "gr_neg10",
        "0.5973",
        "2.9E-34",
        "2.3%"
      ],
      [
        "cc.el.300",
        "0.5311",
        "1.7E-25",
        "4.9%"
      ],
      [
        "wiki.el",
        "0.5812",
        "2.2E-31",
        "4.5%"
      ],
      [
        "gr_cbow_def",
        "0.5232",
        "2.7E-25",
        "2.3%"
      ],
      [
        "gr_d300_nosub",
        "0.5889",
        "3.8E-33",
        "2.3%"
      ],
      [
        "gr_w2v_sg_n5",
        "0.5879",
        "4.4E-33",
        "2.3%"
      ]
    ],
    "id": "2df13a17-6c33-4cd4-8cf3-7c85fc97cb01",
    "claim": "According to Pearson correlation, gr def model had the highest correlation with human ratings of similarity.",
    "label": "supports",
    "table_id": "4b23f4f7-87c1-4499-8938-2a758af522d8"
  },
  {
    "paper": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "paper_id": "1909.03546v2",
    "table_caption": "Table 7: In-domain pre-training: SciBERT vs. BERT",
    "table_column_names": [
      "[EMPTY]",
      "SciERC Entity",
      "SciERC Relation",
      "GENIA Entity"
    ],
    "table_content_values": [
      [
        "Best BERT",
        "69.8",
        "41.9",
        "78.4"
      ],
      [
        "Best SciBERT",
        "[BOLD] 72.0",
        "[BOLD] 45.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "f273252e-5941-436d-aaf0-23e946eaca18",
    "claim": "SciBERT significantly boosts performance for scientific datasets including SciERC and GENIA.",
    "label": "supports",
    "table_id": "86acff26-758c-4082-84b7-900a9fafa2ef"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
    "table_column_names": [
      "Metric",
      "Method of validation",
      "Yelp",
      "Lit."
    ],
    "table_content_values": [
      [
        "Acc",
        "% of machine and human judgments that match",
        "94",
        "84"
      ],
      [
        "Sim",
        "Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation",
        "0.79",
        "0.75"
      ],
      [
        "PP",
        "Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency",
        "0.81",
        "0.67"
      ]
    ],
    "id": "02999797-f0ae-4c27-b7bd-8c4a44e60537",
    "claim": "[CONTINUE] We validate Sim and PP by computing sentence-level Spearman's ρ between the metric and human judgments [CONTINUE] From Table 5, all validations show strong correlations on the Yelp dataset and reasonable correlations on Literature.",
    "label": "supports",
    "table_id": "11046f41-73be-47e9-9f30-8fe50765c22d"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "07c4b243-00fb-4439-94ae-89bb5c1641f5",
    "claim": "[CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] However, words regularly describing negative sentiment or emotions are not one of the most distinctive features for complaints.",
    "label": "supports",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
    "table_column_names": [
      "Cue",
      "[ITALIC] SCOPA",
      "[ITALIC] SB_COPA",
      "Diff.",
      "Prod."
    ],
    "table_content_values": [
      [
        "woman",
        "7.98",
        "4.84",
        "-3.14",
        "0.25"
      ],
      [
        "mother",
        "5.16",
        "3.95",
        "-1.21",
        "0.75"
      ],
      [
        "went",
        "6.00",
        "5.15",
        "-0.85",
        "0.73"
      ],
      [
        "down",
        "5.52",
        "4.93",
        "-0.58",
        "0.71"
      ],
      [
        "into",
        "4.07",
        "3.51",
        "-0.56",
        "0.40"
      ]
    ],
    "id": "16827e79-dd19-40b9-b474-1a6e305ece38",
    "claim": "We observe that BERT trained on Balanced COPA is more sensitive to a few highly productive superficial cues than BERT trained on original COPA.",
    "label": "refutes",
    "table_id": "6609f902-7f48-422d-b5e9-6596405c71dd"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "aa773884-bbac-43b0-a011-1b6131ce4455",
    "claim": "our model has much better quality over the extractive summarization system in three aspects.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "0785d05a-87ed-4bb1-ac80-372c08eb76c8",
    "claim": "For example, DCGCN4 contains 36 layers.",
    "label": "supports",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "387a041a-727b-42a7-a6b1-1c83ae902c61",
    "claim": "We do not have competitive results to Guo et al.",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "2453b511-2b26-4f10-9120-ada1fd98d64c",
    "claim": "I examine the results of our findings with regard to the best-performing classifier.",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 5: Group text features associated with tweets that are complaints and not complaints. Features are sorted by Pearson correlation (r) between their each feature’s normalized frequency and the outcome. We restrict to only the top six categories for each feature type. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Label",
      "[BOLD] Complaints  [BOLD] Words",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Label",
      "[BOLD] Not Complaints  [BOLD] Words",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features"
      ],
      [
        "NEGATE",
        "not, no, can’t, don’t, never, nothing, doesn’t, won’t",
        ".271",
        "POSEMO",
        "thanks, love, thank, good, great, support, lol, win",
        ".185"
      ],
      [
        "RELATIV",
        "in, on, when, at, out, still, now, up, back, new",
        ".225",
        "AFFECT",
        "thanks, love, thank, good, great, support, lol",
        ".111"
      ],
      [
        "FUNCTION",
        "the, i, to, a, my, and, you, for, is, in",
        ".204",
        "SHEHE",
        "he, his, she, her, him, he’s, himself",
        ".105"
      ],
      [
        "TIME",
        "when, still, now, back, new, never, after, then, waiting",
        ".186",
        "MALE",
        "he, his, man, him, sir, he’s, son",
        ".086"
      ],
      [
        "DIFFER",
        "not, but, if, or, can’t, really, than, other, haven’t",
        ".169",
        "FEMALE",
        "she, her, girl, mom, ma, lady, mother, female, mrs",
        ".084"
      ],
      [
        "COGPROC",
        "not, but, how, if, all, why, or, any, need",
        ".132",
        "ASSENT",
        "yes, ok, awesome, okay, yeah, cool, absolutely, agree",
        ".080"
      ],
      [
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters"
      ],
      [
        "Cust. Service",
        "service, customer, contact, job, staff, assist, agent",
        ".136",
        "Gratitude",
        "thanks, thank, good, great, support, everyone, huge, proud",
        ".089"
      ],
      [
        "Order",
        "order, store, buy, free, delivery, available, package",
        ".128",
        "Family",
        "old, friend, family, mom, wife, husband, younger",
        ".063"
      ],
      [
        "Issues",
        "delayed, closed, between, outage, delay, road, accident",
        ".122",
        "Voting",
        "favorite, part, stars, model, vote, models, represent",
        ".060"
      ],
      [
        "Time Ref.",
        "been, yet, haven’t, long, happened, yesterday, took",
        ".122",
        "Contests",
        "Christmas, gift, receive, entered, giveaway, enter, cards",
        ".058"
      ],
      [
        "Tech Parts",
        "battery, laptop, screen, warranty, desktop, printer",
        ".100",
        "Pets",
        "dogs, cat, dog, pet, shepherd, fluffy, treats",
        ".054"
      ],
      [
        "Access",
        "use, using, error, password, access, automatically, reset",
        ".098",
        "Christian",
        "god, shall, heaven, spirit, lord, belongs, soul, believers",
        ".053"
      ]
    ],
    "id": "35e1aff5-32e2-45a6-bc1d-ecc90f105f47",
    "claim": "Several groups of words are much more likely to appear in a complaint, and are used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech).",
    "label": "refutes",
    "table_id": "f0f9508c-b5d0-4383-83a2-8e8715f8e9ab"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "144aa87d-c757-4945-bf5f-39149c5ba574",
    "claim": "These results indicate dense connections do play a significant role in our model.",
    "label": "supports",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "d3fb5a11-bfc1-4394-ad0d-3d78e82880e7",
    "claim": "This empirically shows that compared to recurrent graph encoders, DCGCNs do not necessarily learn better representations for graphs.",
    "label": "refutes",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
    "table_column_names": [
      "[BOLD] Emoji alias",
      "[BOLD] N",
      "[BOLD] emoji #",
      "[BOLD] emoji %",
      "[BOLD] no-emoji #",
      "[BOLD] no-emoji %",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "mask",
        "163",
        "154",
        "94.48",
        "134",
        "82.21",
        "- 12.27"
      ],
      [
        "two_hearts",
        "87",
        "81",
        "93.10",
        "77",
        "88.51",
        "- 4.59"
      ],
      [
        "heart_eyes",
        "122",
        "109",
        "89.34",
        "103",
        "84.43",
        "- 4.91"
      ],
      [
        "heart",
        "267",
        "237",
        "88.76",
        "235",
        "88.01",
        "- 0.75"
      ],
      [
        "rage",
        "92",
        "78",
        "84.78",
        "66",
        "71.74",
        "- 13.04"
      ],
      [
        "cry",
        "116",
        "97",
        "83.62",
        "83",
        "71.55",
        "- 12.07"
      ],
      [
        "sob",
        "490",
        "363",
        "74.08",
        "345",
        "70.41",
        "- 3.67"
      ],
      [
        "unamused",
        "167",
        "121",
        "72.46",
        "116",
        "69.46",
        "- 3.00"
      ],
      [
        "weary",
        "204",
        "140",
        "68.63",
        "139",
        "68.14",
        "- 0.49"
      ],
      [
        "joy",
        "978",
        "649",
        "66.36",
        "629",
        "64.31",
        "- 2.05"
      ],
      [
        "sweat_smile",
        "111",
        "73",
        "65.77",
        "75",
        "67.57",
        "1.80"
      ],
      [
        "confused",
        "77",
        "46",
        "59.74",
        "48",
        "62.34",
        "2.60"
      ]
    ],
    "id": "4158cd57-dd27-47fd-b14e-d3df4b88c3aa",
    "claim": "[CONTINUE] The most interesting ones are mask, rage, and cry, which significantly increase accuracy.",
    "label": "supports",
    "table_id": "2410b1c3-4d48-49e9-9279-dc23daa879d1"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "88a1e00a-fb2f-47ef-b350-3f22f3214735",
    "claim": "Results with BERT show that contextual information is not always valuable for performance improvement.",
    "label": "refutes",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] R-1",
      "[BOLD] R-2",
      "[BOLD] R-SU"
    ],
    "table_content_values": [
      [
        "First-1",
        "26.83",
        "7.25",
        "6.46"
      ],
      [
        "First-2",
        "35.99",
        "10.17",
        "12.06"
      ],
      [
        "First-3",
        "39.41",
        "11.77",
        "14.51"
      ],
      [
        "LexRank Erkan and Radev ( 2004 )",
        "38.27",
        "12.70",
        "13.20"
      ],
      [
        "TextRank Mihalcea and Tarau ( 2004 )",
        "38.44",
        "13.10",
        "13.50"
      ],
      [
        "MMR Carbonell and Goldstein ( 1998 )",
        "38.77",
        "11.98",
        "12.91"
      ],
      [
        "PG-Original Lebanoff et al. ( 2018 )",
        "41.85",
        "12.91",
        "16.46"
      ],
      [
        "PG-MMR Lebanoff et al. ( 2018 )",
        "40.55",
        "12.36",
        "15.87"
      ],
      [
        "PG-BRNN Gehrmann et al. ( 2018 )",
        "42.80",
        "14.19",
        "16.75"
      ],
      [
        "CopyTransformer Gehrmann et al. ( 2018 )",
        "[BOLD] 43.57",
        "14.03",
        "17.37"
      ],
      [
        "Hi-MAP (Our Model)",
        "43.47",
        "[BOLD] 14.89",
        "[BOLD] 17.41"
      ]
    ],
    "id": "bfe65751-16ee-43e3-9cc4-8301d4625a8e",
    "claim": "The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU.",
    "label": "supports",
    "table_id": "6f86cef3-2ee0-4782-b5b4-f37400c22dfb"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "fb82f54b-6618-4820-a69f-c099642adbe6",
    "claim": "Seq2Seq model trained with user annotation is better than Seq2Seq model trained with user and system action annotation.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Evaluation of Greek Word Embeddings",
    "paper_id": "1904.04032v3",
    "table_caption": "Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
    "table_column_names": [
      "Category Semantic",
      "Category no oov words",
      "gr_def 58.42%",
      "gr_neg10 59.33%",
      "cc.el.300  [BOLD] 68.80%",
      "wiki.el 27.20%",
      "gr_cbow_def 31.76%",
      "gr_d300_nosub 60.79%",
      "gr_w2v_sg_n5 52.70%"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "with oov words",
        "52.97%",
        "55.33%",
        "[BOLD] 64.34%",
        "25.73%",
        "28.80%",
        "55.11%",
        "47.82%"
      ],
      [
        "Syntactic",
        "no oov words",
        "65.73%",
        "61.02%",
        "[BOLD] 69.35%",
        "40.90%",
        "64.02%",
        "53.69%",
        "52.60%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "[BOLD] 53.95%",
        "48.69%",
        "49.43%",
        "28.42%",
        "52.54%",
        "44.06%",
        "43.13%"
      ],
      [
        "Overall",
        "no oov words",
        "63.02%",
        "59.96%",
        "[BOLD] 68.97%",
        "36.45%",
        "52.04%",
        "56.30%",
        "52.66%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "53.60%",
        "51.00%",
        "[BOLD] 54.60%",
        "27.50%",
        "44.30%",
        "47.90%",
        "44.80%"
      ]
    ],
    "id": "7020ecb6-4c9f-47c3-9983-05d987388d83",
    "claim": "Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model gr def had the best performance in both cases, even when we included the out-of-vocabulary (oov) terms.",
    "label": "refutes",
    "table_id": "5b468728-2bb8-41a6-8b44-30b94d52dd3b"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "5ffe44d7-3a12-4232-b924-95cea6eec296",
    "claim": "we report below both the performance as assessed with automatic evaluation metrics in Table 3 as well as with human evaluations in Tables 4 and 5, to show that the model trained with our objective does not necessarily sacrifice ROUGE F1 in favour of maintaining the headlines’ readability.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "e1b1a384-f780-49ca-94c3-937bbd88825c",
    "claim": "They are effective when the approximate model class is complex and/or the interaction with the environment is infrequent, but become intractable as the interaction becomes more frequent or the state-action space grows large.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "4c2dc82f-365f-43a9-b795-daccfa505e9c",
    "claim": "LRN is not the fastest model, with ATR outperforming it by 8%∼27%.",
    "label": "refutes",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "e41db642-41b4-4b36-846a-3e019d6ab36a",
    "claim": "[CONTINUE] On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high.",
    "label": "supports",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. † denotes the previous best state-of-the-art model.",
    "table_column_names": [
      "Model",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "CNN zeng2014relation",
        "0.413",
        "0.591",
        "0.486",
        "0.444",
        "0.625",
        "0.519"
      ],
      [
        "PCNN zeng2015distant",
        "0.380",
        "[BOLD] 0.642",
        "0.477",
        "0.446",
        "0.679",
        "0.538†"
      ],
      [
        "EA huang2016attention",
        "0.443",
        "0.638",
        "0.523†",
        "0.419",
        "0.677",
        "0.517"
      ],
      [
        "BGWA jat2018attention",
        "0.364",
        "0.632",
        "0.462",
        "0.417",
        "[BOLD] 0.692",
        "0.521"
      ],
      [
        "BiLSTM-CNN",
        "0.490",
        "0.507",
        "0.498",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "Our model",
        "[BOLD] 0.541",
        "0.595",
        "[BOLD] 0.566*",
        "[BOLD] 0.507",
        "0.652",
        "[BOLD] 0.571*"
      ]
    ],
    "id": "b2b4fcf5-0b84-4720-ae2c-90e27e36dcef",
    "claim": "Our model outperforms the previous stateof-the-art models on both datasets in terms of F1 score.",
    "label": "supports",
    "table_id": "c6b2cd01-ccb9-4fdd-90ab-a8ed5ae0d132"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "8c6fd9a3-be98-4473-be7b-5f21d24b48ad",
    "claim": "Comparing the 784-dimensional models, CBOW and CMOW do not seem to complement each other.",
    "label": "refutes",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "69e1dea0-ae41-4ea6-85e2-1976da7f370e",
    "claim": "In conclusion, these results above can show the ineffectiveness of our DCGCN models.",
    "label": "refutes",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "d6fd61c7-8f3c-4a96-90ac-3b84b96b5c00",
    "claim": "This indicates that GINs can be employed in tasks where the distribution of node degrees has a long tail.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "3c2a6721-2498-4f45-9f29-d3ebf070edc9",
    "claim": "Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units do not sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied.",
    "label": "refutes",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "932c9362-088b-48c2-b6bc-63df4037f765",
    "claim": "[CONTINUE] On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the top most distinctive features for a tweet not being labeled as a complaint.",
    "label": "supports",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM",
    "paper_id": "1704.07221v1",
    "table_caption": "Table 5: Confusion matrix for testing set predictions",
    "table_column_names": [
      "[BOLD] LabelPrediction",
      "[BOLD] C",
      "[BOLD] D",
      "[BOLD] Q",
      "[BOLD] S"
    ],
    "table_content_values": [
      [
        "[BOLD] Commenting",
        "760",
        "0",
        "12",
        "6"
      ],
      [
        "[BOLD] Denying",
        "68",
        "0",
        "1",
        "2"
      ],
      [
        "[BOLD] Querying",
        "69",
        "0",
        "36",
        "1"
      ],
      [
        "[BOLD] Supporting",
        "67",
        "0",
        "1",
        "26"
      ]
    ],
    "id": "8246391e-06c9-4bb7-a7c8-749c2940f735",
    "claim": "Most denying instances get misclassified as commenting (see Table 5),",
    "label": "supports",
    "table_id": "bd4b48bc-00c7-4229-a6c9-7bc5a53456dc"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
    "table_column_names": [
      "[EMPTY]",
      "MSCOCO spice",
      "MSCOCO cider",
      "MSCOCO rouge [ITALIC] L",
      "MSCOCO bleu4",
      "MSCOCO meteor",
      "MSCOCO rep↓",
      "Flickr30k spice",
      "Flickr30k cider",
      "Flickr30k rouge [ITALIC] L",
      "Flickr30k bleu4",
      "Flickr30k meteor",
      "Flickr30k rep↓"
    ],
    "table_content_values": [
      [
        "softmax",
        "18.4",
        "0.967",
        "52.9",
        "29.9",
        "24.9",
        "3.76",
        "13.5",
        "0.443",
        "44.2",
        "19.9",
        "19.1",
        "6.09"
      ],
      [
        "sparsemax",
        "[BOLD] 18.9",
        "[BOLD] 0.990",
        "[BOLD] 53.5",
        "[BOLD] 31.5",
        "[BOLD] 25.3",
        "3.69",
        "[BOLD] 13.7",
        "[BOLD] 0.444",
        "[BOLD] 44.3",
        "[BOLD] 20.7",
        "[BOLD] 19.3",
        "5.84"
      ],
      [
        "TVmax",
        "18.5",
        "0.974",
        "53.1",
        "29.9",
        "25.1",
        "[BOLD] 3.17",
        "13.3",
        "0.438",
        "44.2",
        "20.5",
        "19.0",
        "[BOLD] 3.97"
      ]
    ],
    "id": "0ab283cb-2c6e-451e-84c0-791df4822e8c",
    "claim": "As can be seen in Table 1, sparsemax and TVMAX achieve better results overall when compared with softmax, indicating that the use of selective attention leads to better captions.",
    "label": "supports",
    "table_id": "55b8ec67-1f65-4852-943b-d1530519e837"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 3: Test BLEU for es-en adaptive training. EWC reduces forgetting compared to other fine-tuning methods, while offering the greatest improvement on the new domain.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Training scheme",
      "[BOLD] Health",
      "[BOLD] Bio"
    ],
    "table_content_values": [
      [
        "1",
        "Health",
        "[BOLD] 35.9",
        "33.1"
      ],
      [
        "2",
        "Bio",
        "29.6",
        "36.1"
      ],
      [
        "3",
        "Health and Bio",
        "35.8",
        "37.2"
      ],
      [
        "4",
        "1 then Bio, No-reg",
        "30.3",
        "36.6"
      ],
      [
        "5",
        "1 then Bio, L2",
        "35.1",
        "37.3"
      ],
      [
        "6",
        "1 then Bio, EWC",
        "35.2",
        "[BOLD] 37.8"
      ]
    ],
    "id": "0d2c4c12-d580-4a42-88a2-1abeada7b180",
    "claim": "We find EWC does not outperform the L2 approach.",
    "label": "refutes",
    "table_id": "8f01f974-9c61-443a-9772-ef3b10d66d43"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "cd5aaefc-4143-4530-96d8-12f0b1ab6f2a",
    "claim": "there were no adjectives in the questions except for the \"concept\" and \"property\" words, for the adjectives were replaced with prepositional phrases, for instance.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "b9b1312c-bfee-4f66-b10d-cedc62a7476a",
    "claim": "On the other side, H-CMOW shows, among others, no improvements at BShift.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "784ba011-5c5c-49c2-9604-da58674db53a",
    "claim": "In contrast, RoBERTa-large drops only 3.1 points when trained and evaluated on the split from Sap et al.",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
    "table_column_names": [
      "[EMPTY]",
      "MSCOCO spice",
      "MSCOCO cider",
      "MSCOCO rouge [ITALIC] L",
      "MSCOCO bleu4",
      "MSCOCO meteor",
      "MSCOCO rep↓",
      "Flickr30k spice",
      "Flickr30k cider",
      "Flickr30k rouge [ITALIC] L",
      "Flickr30k bleu4",
      "Flickr30k meteor",
      "Flickr30k rep↓"
    ],
    "table_content_values": [
      [
        "softmax",
        "18.4",
        "0.967",
        "52.9",
        "29.9",
        "24.9",
        "3.76",
        "13.5",
        "0.443",
        "44.2",
        "19.9",
        "19.1",
        "6.09"
      ],
      [
        "sparsemax",
        "[BOLD] 18.9",
        "[BOLD] 0.990",
        "[BOLD] 53.5",
        "[BOLD] 31.5",
        "[BOLD] 25.3",
        "3.69",
        "[BOLD] 13.7",
        "[BOLD] 0.444",
        "[BOLD] 44.3",
        "[BOLD] 20.7",
        "[BOLD] 19.3",
        "5.84"
      ],
      [
        "TVmax",
        "18.5",
        "0.974",
        "53.1",
        "29.9",
        "25.1",
        "[BOLD] 3.17",
        "13.3",
        "0.438",
        "44.2",
        "20.5",
        "19.0",
        "[BOLD] 3.97"
      ]
    ],
    "id": "9eb04d72-35f4-42a2-84cb-8233924128b3",
    "claim": "[CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax but still superior to softmax on MSCOCO and similar on Flickr30k.",
    "label": "supports",
    "table_id": "55b8ec67-1f65-4852-943b-d1530519e837"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "ee5d43ff-0f49-4fe4-8d83-d0e9571e46d3",
    "claim": "The best performing system is KnowComb.",
    "label": "supports",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "2d8b85cb-aeef-4463-b876-4c2777e9277f",
    "claim": "We divide the dataset into 5 folds according to the users’ identity information (e.g., 619,1802, etc.).",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "f66b6574-4444-44c3-adca-6bba7c77ffc1",
    "claim": "the major drawback is that the dataset contains only 583 examples in the test set, which makes it difficult to measure model performance robustly.",
    "label": "not enough info",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "8cc4d0df-ae69-445b-b1ec-7b5b74927e0d",
    "claim": "The alternative creates a conjoined structure consisting of both the non-polar cue and the target alternative.",
    "label": "not enough info",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 4: Precisions on the Wikidata dataset with different choice of d.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC",
      "Time"
    ],
    "table_content_values": [
      [
        "[ITALIC] d=1",
        "0.602",
        "0.487",
        "0.403",
        "0.367",
        "4h"
      ],
      [
        "[ITALIC] d=32",
        "0.645",
        "0.501",
        "0.393",
        "0.370",
        "-"
      ],
      [
        "[ITALIC] d=16",
        "0.655",
        "0.518",
        "0.413",
        "0.413",
        "20h"
      ],
      [
        "[ITALIC] d=8",
        "0.650",
        "0.519",
        "0.422",
        "0.405",
        "8h"
      ]
    ],
    "id": "e8fa28c5-0b0f-4d5e-a963-f0867a4b2f2a",
    "claim": "As the table 4 depicts, the precision increases with the growth of d, but the training time also increases.",
    "label": "refutes",
    "table_id": "b3925d14-8042-410e-841f-73dce02fc49b"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "555cfc65-dbb1-41e2-8b2c-ca1d6db747d1",
    "claim": "This is unexpected as encoding a bigger graph (containing more information) should be easier than encoding smaller graphs.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "736bdd5b-3280-45f8-ba57-7a23885bf208",
    "claim": "Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Aligning Vector-spaces with Noisy Supervised Lexicons",
    "paper_id": "1903.10238v1",
    "table_caption": "Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
    "table_column_names": [
      "Method",
      "En→It best",
      "En→It avg",
      "En→It iters",
      "En→De best",
      "En→De avg",
      "En→De iters",
      "En→Fi best",
      "En→Fi avg",
      "En→Fi iters",
      "En→Es best",
      "En→Es avg",
      "En→Es iters"
    ],
    "table_content_values": [
      [
        "Artetxe et al., 2018b",
        "[BOLD] 48.53",
        "48.13",
        "573",
        "48.47",
        "48.19",
        "773",
        "33.50",
        "32.63",
        "988",
        "37.60",
        "37.33",
        "808"
      ],
      [
        "Noise-aware Alignment",
        "[BOLD] 48.53",
        "[BOLD] 48.20",
        "471",
        "[BOLD] 49.67",
        "[BOLD] 48.89",
        "568",
        "[BOLD] 33.98",
        "[BOLD] 33.68",
        "502",
        "[BOLD] 38.40",
        "[BOLD] 37.79",
        "551"
      ]
    ],
    "id": "83e691c8-43ce-46c6-bbc7-2b3d400d8c58",
    "claim": "Our model does not improve the results in the translation tasks.",
    "label": "refutes",
    "table_id": "279e3d12-df99-48f8-83ea-d3e42b8bbfcc"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 4: Cue classification on the test set.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] F-Score  [BOLD] Baseline",
      "[BOLD] F-Score  [BOLD] Proposed",
      "[BOLD] Support"
    ],
    "table_content_values": [
      [
        "False cues",
        "0.61",
        "0.68",
        "47"
      ],
      [
        "Actual cues",
        "0.97",
        "0.98",
        "557"
      ]
    ],
    "id": "34972ec1-4b2a-42cb-8b10-9f774ce112e9",
    "claim": "the best-performing system outperforms the baseline in all cue types with the largest gains of 9.5% and 8.6% on the actual and false cue recall, respectively.",
    "label": "not enough info",
    "table_id": "08a8fe8c-92a2-41dc-a6e0-f97b95380cae"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 3: Number of tweets annotated as complaints across the nine domains.",
    "table_column_names": [
      "[BOLD] Category",
      "[BOLD] Complaints",
      "[BOLD] Not Complaints"
    ],
    "table_content_values": [
      [
        "Food & Beverage",
        "95",
        "35"
      ],
      [
        "Apparel",
        "141",
        "117"
      ],
      [
        "Retail",
        "124",
        "75"
      ],
      [
        "Cars",
        "67",
        "25"
      ],
      [
        "Services",
        "207",
        "130"
      ],
      [
        "Software & Online Services",
        "189",
        "103"
      ],
      [
        "Transport",
        "139",
        "109"
      ],
      [
        "Electronics",
        "174",
        "112"
      ],
      [
        "Other",
        "96",
        "33"
      ],
      [
        "Total",
        "1232",
        "739"
      ]
    ],
    "id": "d9f56bd3-ead2-44b1-8f61-415c79ef85fa",
    "claim": "In total, 1,232 tweets (62.4%) are complaints and 739 are not complaints (37.6%).",
    "label": "supports",
    "table_id": "557b1d52-6b5b-49b6-a22d-c130bc5e7d4d"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "17.3",
        "0.828"
      ],
      [
        "Wiener filter",
        "19.5",
        "0.722"
      ],
      [
        "Minimizing DCE",
        "15.8",
        "[BOLD] 0.269"
      ],
      [
        "FSEGAN",
        "14.9",
        "0.291"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "15.6",
        "0.330"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 14.4",
        "0.303"
      ],
      [
        "Clean speech",
        "5.7",
        "0.0"
      ]
    ],
    "id": "5d51c21b-4d94-4b64-bb07-a899563e5b9a",
    "claim": "The Wiener filtering method shows lower DCE, but higher WER than no enhancement.",
    "label": "supports",
    "table_id": "3c6c1f56-ed0b-418a-ac6c-6449488a89e9"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "ae5706f0-91bd-452f-9fae-1d25d7bbcffc",
    "claim": "our method uses the combination of SPINE and Word2Sense to improve the performance of sentiment classification task",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Assessing Gender Bias in Machine Translation – A Case Study with Google Translate",
    "paper_id": "1809.02208v4",
    "table_caption": "Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
    "table_column_names": [
      "Category",
      "Female (%)",
      "Male (%)",
      "Neutral (%)"
    ],
    "table_content_values": [
      [
        "Office and administrative support",
        "11.015",
        "58.812",
        "16.954"
      ],
      [
        "Architecture and engineering",
        "2.299",
        "72.701",
        "10.92"
      ],
      [
        "Farming, fishing, and forestry",
        "12.179",
        "62.179",
        "14.744"
      ],
      [
        "Management",
        "11.232",
        "66.667",
        "12.681"
      ],
      [
        "Community and social service",
        "20.238",
        "62.5",
        "10.119"
      ],
      [
        "Healthcare support",
        "25.0",
        "43.75",
        "17.188"
      ],
      [
        "Sales and related",
        "8.929",
        "62.202",
        "16.964"
      ],
      [
        "Installation, maintenance, and repair",
        "5.22",
        "58.333",
        "17.125"
      ],
      [
        "Transportation and material moving",
        "8.81",
        "62.976",
        "17.5"
      ],
      [
        "Legal",
        "11.905",
        "72.619",
        "10.714"
      ],
      [
        "Business and financial operations",
        "7.065",
        "67.935",
        "15.58"
      ],
      [
        "Life, physical, and social science",
        "5.882",
        "73.284",
        "10.049"
      ],
      [
        "Arts, design, entertainment, sports, and media",
        "10.36",
        "67.342",
        "11.486"
      ],
      [
        "Education, training, and library",
        "23.485",
        "53.03",
        "9.091"
      ],
      [
        "Building and grounds cleaning and maintenance",
        "12.5",
        "68.333",
        "11.667"
      ],
      [
        "Personal care and service",
        "18.939",
        "49.747",
        "18.434"
      ],
      [
        "Healthcare practitioners and technical",
        "22.674",
        "51.744",
        "15.116"
      ],
      [
        "Production",
        "14.331",
        "51.199",
        "18.245"
      ],
      [
        "Computer and mathematical",
        "4.167",
        "66.146",
        "14.062"
      ],
      [
        "Construction and extraction",
        "8.578",
        "61.887",
        "17.525"
      ],
      [
        "Protective service",
        "8.631",
        "65.179",
        "12.5"
      ],
      [
        "Food preparation and serving related",
        "21.078",
        "58.333",
        "17.647"
      ],
      [
        "Total",
        "11.76",
        "58.93",
        "15.939"
      ]
    ],
    "id": "7f0dcd5c-173e-4858-a85d-6f861623a0d4",
    "claim": "Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics .",
    "label": "supports",
    "table_id": "2e483d7f-201a-4e26-b179-5216c375183e"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
    "table_column_names": [
      "[EMPTY]",
      "WN-N P",
      "WN-N R",
      "WN-N F",
      "WN-V P",
      "WN-V R",
      "WN-V F",
      "VN P",
      "VN R",
      "VN F"
    ],
    "table_content_values": [
      [
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2"
      ],
      [
        "type",
        ".700",
        ".654",
        ".676",
        ".535",
        ".474",
        ".503",
        ".327",
        ".309",
        ".318"
      ],
      [
        "x+POS",
        ".699",
        ".651",
        ".674",
        ".544",
        ".472",
        ".505",
        ".339",
        ".312",
        ".325"
      ],
      [
        "lemma",
        ".706",
        ".660",
        ".682",
        ".576",
        ".520",
        ".547",
        ".384",
        ".360",
        ".371"
      ],
      [
        "x+POS",
        "<bold>.710</bold>",
        "<bold>.662</bold>",
        "<bold>.685</bold>",
        "<bold>.589</bold>",
        "<bold>.529</bold>",
        "<bold>.557</bold>",
        "<bold>.410</bold>",
        "<bold>.389</bold>",
        "<bold>.399</bold>"
      ],
      [
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep"
      ],
      [
        "type",
        ".712",
        ".661",
        ".686",
        ".545",
        ".457",
        ".497",
        ".324",
        ".296",
        ".310"
      ],
      [
        "x+POS",
        ".715",
        ".659",
        ".686",
        ".560",
        ".464",
        ".508",
        ".349",
        ".320",
        ".334"
      ],
      [
        "lemma",
        "<bold>.725</bold>",
        "<bold>.668</bold>",
        "<bold>.696</bold>",
        ".591",
        ".512",
        ".548",
        ".408",
        ".371",
        ".388"
      ],
      [
        "x+POS",
        ".722",
        ".666",
        ".693",
        "<bold>.609</bold>",
        "<bold>.527</bold>",
        "<bold>.565</bold>",
        "<bold>.412</bold>",
        "<bold>.381</bold>",
        "<bold>.396</bold>"
      ]
    ],
    "id": "511ea10f-bd33-40f4-a2ab-40fe6c582bbd",
    "claim": "Still, lemma-based targets significantly7 (p ≤ .005) outperform type-based targets in terms of F-measure in all cases.",
    "label": "supports",
    "table_id": "051fe422-03d4-44d5-836c-7f982b328555"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.818",
        "0.719",
        "37.3",
        "10.0"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.819",
        "0.734",
        "26.3",
        "14.2"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.813",
        "0.770",
        "36.4",
        "18.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.807",
        "0.796",
        "28.4",
        "21.5"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.798",
        "0.783",
        "39.7",
        "19.2"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.804",
        "0.785",
        "27.1",
        "20.3"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.805",
        "[BOLD] 0.817",
        "43.3",
        "21.6"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.818",
        "0.805",
        "[BOLD] 29.0",
        "[BOLD] 22.8"
      ]
    ],
    "id": "44c10fb7-e496-4663-8f4c-d71ccd7dd67d",
    "claim": "Table 2 shows that the model with cyclic loss (M2) and the model with cyclic loss, paraphrase loss, and language model loss (M5) both have lower Sim than M0 on both datasets under similar Acc.",
    "label": "refutes",
    "table_id": "ce976181-466e-4d2c-a980-797bf966424e"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "ecd566bf-ff78-4522-a91d-986557c5748c",
    "claim": "The proposed method outperforms the original embeddings and performs on par with the SOV.",
    "label": "supports",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "7b76da1e-3531-49c1-ad27-dfe176882449",
    "claim": "In addition, the training time results in Table 3 confirm the computational disadvantage of LRN over all other recurrent units, where LRN slows down compared to ATR and SRU by approximately 25%.",
    "label": "refutes",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "Variational Self-attention Model for Sentence Representation",
    "paper_id": "1812.11559v4",
    "table_caption": "Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.",
    "table_column_names": [
      "Model",
      "Accuracy (%) agree",
      "Accuracy (%) disagree",
      "Accuracy (%) discuss",
      "Accuracy (%) unrelated",
      "Micro F1(%)"
    ],
    "table_content_values": [
      [
        "Average of Word2vec Embedding",
        "12.43",
        "01.30",
        "43.32",
        "74.24",
        "45.53"
      ],
      [
        "CNN-based Sentence Embedding",
        "24.54",
        "05.06",
        "53.24",
        "79.53",
        "81.72"
      ],
      [
        "RNN-based Sentence Embedding",
        "24.42",
        "05.42",
        "69.05",
        "65.34",
        "78.70"
      ],
      [
        "Self-attention Sentence Embedding",
        "23.53",
        "04.63",
        "63.59",
        "80.34",
        "80.11"
      ],
      [
        "Our model",
        "28.53",
        "10.43",
        "65.43",
        "82.43",
        "[BOLD] 83.54"
      ]
    ],
    "id": "7ee1b8c8-3a20-4d62-9344-ec5f5a542ba9",
    "claim": "As for the micro F1 evaluation metric, our model does not achieve the highest performance (83.54%) on the FNC-1 testing subset.",
    "label": "refutes",
    "table_id": "5a5d9c55-ed88-4608-be8f-913c3acdcfc4"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
    "table_column_names": [
      "# of Heads",
      "Accuracy",
      "Val. Loss",
      "Effect"
    ],
    "table_content_values": [
      [
        "1",
        "89.44%",
        "0.2811",
        "-6.84%"
      ],
      [
        "2",
        "91.20%",
        "0.2692",
        "-5.08%"
      ],
      [
        "4",
        "93.85%",
        "0.2481",
        "-2.43%"
      ],
      [
        "8",
        "96.02%",
        "0.2257",
        "-0.26%"
      ],
      [
        "10",
        "96.28%",
        "0.2197",
        "[EMPTY]"
      ],
      [
        "16",
        "96.32%",
        "0.2190",
        "+0.04"
      ]
    ],
    "id": "e55f868d-c3b3-42e4-91ae-3e87cab511f3",
    "claim": "This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results.",
    "label": "supports",
    "table_id": "34083185-d444-4bd0-994e-d13f5afe8e82"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "b182dd3e-b44e-4d56-aa65-9ba85fc1b45a",
    "claim": "(1) BERT is able to capture the gist of the summaries, and thus is appropriate for predicting good summaries; (2) the sentences in good summaries tend to have high tf-idf similarity to the target article;",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
    "table_column_names": [
      "System",
      "MUC",
      "BCUB",
      "CEAFe",
      "AVG"
    ],
    "table_content_values": [
      [
        "ACE",
        "ACE",
        "ACE",
        "ACE",
        "ACE"
      ],
      [
        "IlliCons",
        "[BOLD] 78.17",
        "81.64",
        "[BOLD] 78.45",
        "[BOLD] 79.42"
      ],
      [
        "KnowComb",
        "77.51",
        "[BOLD] 81.97",
        "77.44",
        "78.97"
      ],
      [
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes"
      ],
      [
        "IlliCons",
        "84.10",
        "[BOLD] 78.30",
        "[BOLD] 68.74",
        "[BOLD] 77.05"
      ],
      [
        "KnowComb",
        "[BOLD] 84.33",
        "78.02",
        "67.95",
        "76.76"
      ]
    ],
    "id": "8ca9d3e5-cce5-41f9-ada1-afa9e75a6cf1",
    "claim": "Despite our system achieving the same level of performance compared to a state-of-art general coreference system, we still observe significant performance improvement on the ACE and OntoNotes datasets.",
    "label": "refutes",
    "table_id": "cbbb2b74-fff2-4db8-a6be-b6a395d77483"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "5ea07570-7f39-4790-b562-22fd697fb6ef",
    "claim": "Results also show the global node is more effective than the linear combination.",
    "label": "refutes",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
    "table_column_names": [
      "[BOLD] Training data",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] Disfl"
    ],
    "table_content_values": [
      [
        "Original",
        "0",
        "22",
        "0",
        "14"
      ],
      [
        "Cleaned added",
        "0",
        "23",
        "0",
        "14"
      ],
      [
        "Cleaned missing",
        "0",
        "1",
        "0",
        "2"
      ],
      [
        "Cleaned",
        "0",
        "0",
        "0",
        "5"
      ]
    ],
    "id": "ebaa19a5-b7cf-4f3b-8b04-b4304e779ff8",
    "claim": "All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem.",
    "label": "supports",
    "table_id": "82ce68c2-64df-452b-ac0b-064c7fdaefa8"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "36fa8fb5-8f19-4084-8cbb-4c3195e16bc3",
    "claim": "The models using BoC do not outperform models using BoW as well as ASM features.",
    "label": "refutes",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "16813a0f-c973-4891-a6c7-2994bfb3abb2",
    "claim": "this shows the importance of the SRBR strategy.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "9db05047-3aba-476e-b6dc-214afb5eea24",
    "claim": "especially for DAMD, modeling multi-action system responses, which are notoriously rare and difficult to collect, significantly improves the performance.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "04ed07c3-533c-4981-9516-1132391b69a0",
    "claim": "it is critical to realize that the conversational negation corpus is indeed a true label corpus, while neither O1 nor O2 are correct.",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 7: Test accuracy on SNLI task with Base+LN setting and test perplexity on PTB task with Base setting.",
    "table_column_names": [
      "Model",
      "SNLI",
      "PTB"
    ],
    "table_content_values": [
      [
        "LRN",
        "[BOLD] 85.06",
        "[BOLD] 61.26"
      ],
      [
        "gLRN",
        "84.72",
        "92.49"
      ],
      [
        "eLRN",
        "83.56",
        "169.81"
      ]
    ],
    "id": "0b7152d7-54db-4a1f-b5cb-18c3b4b17aa2",
    "claim": "Results in Table 7 show that although the accuracy on SNLI is acceptable, gLRN and eLRN perform significantly worse on the PTB task.",
    "label": "supports",
    "table_id": "f83e3f0d-8eef-4328-a1d9-707d1a303edd"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "17f3bd4a-b9fd-44ad-b5fb-cef1a993aece",
    "claim": "It can be noted that the use of discourse markers is crucial for the task since the results after removing them from the dataset is far from optimal.",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "85c78beb-8fa3-4c80-83a2-21ef165f4090",
    "claim": "1, where the x-axis refers to each metric and the y-axis refers to the number of sessions.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection",
    "paper_id": "3",
    "table_caption": "Table 5: Task B results",
    "table_column_names": [
      "[EMPTY]",
      "Micro F1"
    ],
    "table_content_values": [
      [
        "Baseline",
        "0.709"
      ],
      [
        "W2V (<italic>d</italic>=50)",
        "0.736"
      ],
      [
        "W2V (<italic>d</italic>=500)",
        "0.753"
      ],
      [
        "S2V",
        "0.748"
      ],
      [
        "S2V + W2V (<italic>d</italic>=50)",
        "0.744"
      ],
      [
        "S2V + K + W2V(<italic>d</italic>=50)",
        "0.749"
      ],
      [
        "SIF (DE)",
        "0.759"
      ],
      [
        "SIF (DE-EN)",
        "<bold>0.765</bold>"
      ]
    ],
    "id": "15ca12b8-56ac-4053-8fe7-3b00a97b9a07",
    "claim": "For Task B, the baseline model outperformed all models trained on the stacked learner when using only plain averaged word embeddings.",
    "label": "refutes",
    "table_id": "2c898525-6aaa-4bee-b355-9151026dcc35"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
    "table_column_names": [
      "[EMPTY]",
      "Difference Function",
      "Seanad Abolition",
      "Video Games",
      "Pornography"
    ],
    "table_content_values": [
      [
        "OD-parse",
        "Absolute",
        "0.01",
        "-0.01",
        "0.07"
      ],
      [
        "OD-parse",
        "JS div.",
        "0.01",
        "-0.01",
        "-0.01"
      ],
      [
        "OD-parse",
        "EMD",
        "0.07",
        "0.01",
        "-0.01"
      ],
      [
        "OD",
        "Absolute",
        "[BOLD] 0.54",
        "[BOLD] 0.56",
        "[BOLD] 0.41"
      ],
      [
        "OD",
        "JS div.",
        "0.07",
        "-0.01",
        "-0.02"
      ],
      [
        "OD",
        "EMD",
        "0.26",
        "-0.01",
        "0.01"
      ],
      [
        "OD (no polarity shifters)",
        "Absolute",
        "0.23",
        "0.08",
        "0.04"
      ],
      [
        "OD (no polarity shifters)",
        "JS div.",
        "0.09",
        "-0.01",
        "-0.02"
      ],
      [
        "OD (no polarity shifters)",
        "EMD",
        "0.10",
        "0.01",
        "-0.01"
      ]
    ],
    "id": "05f252e0-8409-4a35-8596-dc70f2f2b281",
    "claim": "This is evident from the significant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters.",
    "label": "supports",
    "table_id": "fba0fe3a-58cc-47a0-a3ab-5d091d2b7fe7"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "472b4643-6646-495a-825f-bf3d650a90a0",
    "claim": "It is observed that the former outperforms the latter, indicating the key role of dialogue state estimation.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation",
    "paper_id": "1906.12068v1",
    "table_caption": "Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.",
    "table_column_names": [
      "System reference",
      "BLEU↑",
      "TER↓"
    ],
    "table_content_values": [
      [
        "en-fr-rnn-rev",
        "33.3",
        "50.2"
      ],
      [
        "en-fr-smt-rev",
        "36.5",
        "47.1"
      ],
      [
        "en-fr-trans-rev",
        "[BOLD] 36.8",
        "[BOLD] 46.8"
      ],
      [
        "en-es-rnn-rev",
        "37.8",
        "45.0"
      ],
      [
        "en-es-smt-rev",
        "39.2",
        "44.0"
      ],
      [
        "en-es-trans-rev",
        "[BOLD] 40.4",
        "[BOLD] 42.7"
      ]
    ],
    "id": "73c20a83-ae30-4b98-b2fc-1ba2d74977b6",
    "claim": "we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While Transformer models are the best ones according to the evaluation metrics,",
    "label": "supports",
    "table_id": "61bdf875-07d5-42cb-8051-fa032a1eb7b0"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "bb466346-414c-4c69-b178-131999ea4381",
    "claim": "accuracy on average the proposed method outperform other approaches by 2.8% and 2.45% for B-CNN and R-CNN respectively.",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "[BOLD] Baselines",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen ( 2015a )",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. ( 2018 )",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "[BOLD] Model Variants",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "6e0cec48-2365-4b12-a062-d9a7a42c0fa8",
    "claim": "Our joint model does not outperform all the base lines, with a gap of only 10.5 CoNLL F1 points from the last published results (KCP), and only surpassing our strong lemma baseline by 3 points.",
    "label": "refutes",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "b3ea990d-54c0-4078-a9fc-75898eb8965c",
    "claim": "When redundancy removal was applied to LogReg, it produces only marginal improvement.",
    "label": "supports",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "c707131e-e361-4e4d-9fcd-a3bbceb7b3cd",
    "claim": "( 2018 ) who \\detrained\" their model for RL, our NeuralTD is simpler without any \\reinforcement\\’\\ training.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "c2ee507c-4839-41e0-890a-5a08087869eb",
    "claim": "there are slight but consistent decreases when comparing to the metric trained using all 2,011 content words",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "a0fd51e0-1ceb-4ccc-a1cd-681d25090c32",
    "claim": "on the other hand, we are still noticeably outperformed by Refresh when directly comparing the length of the two summaries.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "Logistic Regression",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Sentiment – MPQA",
        "64.2",
        "39.1",
        "0.499"
      ],
      [
        "Sentiment – NRC",
        "63.9",
        "42.2",
        "0.599"
      ],
      [
        "Sentiment – V&B",
        "68.9",
        "60.0",
        "0.696"
      ],
      [
        "Sentiment – VADER",
        "66.0",
        "54.2",
        "0.654"
      ],
      [
        "Sentiment – Stanford",
        "68.0",
        "55.6",
        "0.696"
      ],
      [
        "Complaint Specific (all)",
        "65.7",
        "55.2",
        "0.634"
      ],
      [
        "Request",
        "64.2",
        "39.1",
        "0.583"
      ],
      [
        "Intensifiers",
        "64.5",
        "47.3",
        "0.639"
      ],
      [
        "Downgraders",
        "65.4",
        "49.8",
        "0.615"
      ],
      [
        "Temporal References",
        "64.2",
        "43.7",
        "0.535"
      ],
      [
        "Pronoun Types",
        "64.1",
        "39.1",
        "0.545"
      ],
      [
        "POS Bigrams",
        "72.2",
        "66.8",
        "0.756"
      ],
      [
        "LIWC",
        "71.6",
        "65.8",
        "0.784"
      ],
      [
        "Word2Vec Clusters",
        "67.7",
        "58.3",
        "0.738"
      ],
      [
        "Bag-of-Words",
        "79.8",
        "77.5",
        "0.866"
      ],
      [
        "All Features",
        "[BOLD] 80.5",
        "[BOLD] 78.0",
        "[BOLD] 0.873"
      ],
      [
        "Neural Networks",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "MLP",
        "78.3",
        "76.2",
        "0.845"
      ],
      [
        "LSTM",
        "80.2",
        "77.0",
        "0.864"
      ]
    ],
    "id": "a4aa23f5-33df-4432-aa1a-31d016705823",
    "claim": "Syntactic part-ofspeech features alone obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section hold high predictive accuracy for the task.",
    "label": "supports",
    "table_id": "caac7a4b-bc0e-4b38-bc24-9d0ad39f2fa7"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "83f16698-7b03-47a0-8716-3ab3a76eb741",
    "claim": "The proposed CNN-LSTMOur-neg-Ant improves upon the simple CNNLSTM-w/o neg.",
    "label": "supports",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "[BOLD] 71.2"
      ]
    ],
    "id": "ede7cf00-7c18-4d7a-86d8-b8d28c9d9ac9",
    "claim": "for example, for the system B in Table 2, the input systems are made available to the evaluation system, and this gives [BOLD] MUC-B1 (“E2”, default) a 4.6 precision point advantage over standard MUC-B1 (“E1”, see Table 2).",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "d27c412f-e946-4c35-88fd-6ce5ec25bca4",
    "claim": "However, while we notice a definite improvement over Peyrard and Gurevych (2018), our results still lack behind the golden-set correlation, suggesting that future work could further improve the capacity of learning summary-level correlation.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "ef59728f-b28b-4481-b3b7-3aac7d96ac9d",
    "claim": "In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is lower than Guo et al. (2019), a state-of-the-art model that does not employ external information.",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "4c4e4ede-1cec-4827-b374-11f06872dac4",
    "claim": "[CONTINUE] As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees.",
    "label": "supports",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "0206cb33-81d5-41f7-8a9b-65f54e67f75e",
    "claim": "However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is actually 4.8 percent (71.9% vs. 69.0%).",
    "label": "refutes",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "e75ba52a-52ff-4969-ae13-fb4cc981a78a",
    "claim": "covering the rare words can boost the performances across different out-of-domain (OOD) datasets significantly",
    "label": "not enough info",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "A context sensitive real-time Spell Checker with language adaptability",
    "paper_id": "1910.11242v1",
    "table_caption": "TABLE II: Synthetic Data Performance results",
    "table_column_names": [
      "[BOLD] Language",
      "[BOLD] # Test",
      "[BOLD] P@1",
      "[BOLD] P@3",
      "[BOLD] P@5",
      "[BOLD] P@10",
      "[BOLD] MRR"
    ],
    "table_content_values": [
      [
        "[BOLD] Language",
        "[BOLD] Samples",
        "[BOLD] P@1",
        "[BOLD] P@3",
        "[BOLD] P@5",
        "[BOLD] P@10",
        "[BOLD] MRR"
      ],
      [
        "Bengali",
        "140000",
        "91.30",
        "97.83",
        "98.94",
        "99.65",
        "94.68"
      ],
      [
        "Czech",
        "94205",
        "95.84",
        "98.72",
        "99.26",
        "99.62",
        "97.37"
      ],
      [
        "Danish",
        "140000",
        "85.84",
        "95.19",
        "97.28",
        "98.83",
        "90.85"
      ],
      [
        "Dutch",
        "140000",
        "86.83",
        "95.01",
        "97.04",
        "98.68",
        "91.32"
      ],
      [
        "English",
        "140000",
        "97.08",
        "99.39",
        "99.67",
        "99.86",
        "98.27"
      ],
      [
        "Finnish",
        "140000",
        "97.77",
        "99.58",
        "99.79",
        "99.90",
        "98.69"
      ],
      [
        "French",
        "140000",
        "86.52",
        "95.66",
        "97.52",
        "98.83",
        "91.38"
      ],
      [
        "German",
        "140000",
        "87.58",
        "96.16",
        "97.86",
        "99.05",
        "92.10"
      ],
      [
        "Greek",
        "30022",
        "84.95",
        "94.99",
        "96.88",
        "98.44",
        "90.27"
      ],
      [
        "Hebrew",
        "132596",
        "94.00",
        "98.26",
        "99.05",
        "99.62",
        "96.24"
      ],
      [
        "Hindi",
        "140000",
        "82.19",
        "93.71",
        "96.28",
        "98.30",
        "88.40"
      ],
      [
        "Indonesian",
        "140000",
        "95.01",
        "98.98",
        "99.50",
        "99.84",
        "97.04"
      ],
      [
        "Italian",
        "140000",
        "89.93",
        "97.31",
        "98.54",
        "99.38",
        "93.76"
      ],
      [
        "Marathi",
        "140000",
        "93.01",
        "98.16",
        "99.06",
        "99.66",
        "95.69"
      ],
      [
        "Polish",
        "140000",
        "95.65",
        "99.17",
        "99.62",
        "99.86",
        "97.44"
      ],
      [
        "Portuguese",
        "140000",
        "86.73",
        "96.29",
        "97.94",
        "99.10",
        "91.74"
      ],
      [
        "Romanian",
        "140000",
        "95.52",
        "98.79",
        "99.32",
        "99.68",
        "97.22"
      ],
      [
        "Russian",
        "140000",
        "94.85",
        "98.74",
        "99.33",
        "99.71",
        "96.86"
      ],
      [
        "Spanish",
        "140000",
        "85.91",
        "95.35",
        "97.18",
        "98.57",
        "90.92"
      ],
      [
        "Swedish",
        "140000",
        "88.86",
        "96.40",
        "98.00",
        "99.14",
        "92.87"
      ],
      [
        "Tamil",
        "140000",
        "98.05",
        "99.70",
        "99.88",
        "99.98",
        "98.88"
      ],
      [
        "Telugu",
        "140000",
        "97.11",
        "99.68",
        "99.92",
        "99.99",
        "98.38"
      ],
      [
        "Thai",
        "12403",
        "98.73",
        "99.71",
        "99.78",
        "99.85",
        "99.22"
      ],
      [
        "Turkish",
        "140000",
        "97.13",
        "99.51",
        "99.78",
        "99.92",
        "98.33"
      ]
    ],
    "id": "a24c8c0a-c398-4600-8503-17bec62989ed",
    "claim": "The system performs well on synthetic dataset with a minimum of 80% P@1 and 98% P@10.",
    "label": "supports",
    "table_id": "4a86596e-7c02-40e6-9cc4-de0764dd6350"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>External</bold>",
      "<bold>BLEU</bold>"
    ],
    "table_content_values": [
      [
        "Konstas et al. (2017)",
        "200K",
        "27.40"
      ],
      [
        "Song et al. (2018)",
        "200K",
        "28.20"
      ],
      [
        "Guo et al. (2019)",
        "200K",
        "31.60"
      ],
      [
        "G2S-GGNN",
        "200K",
        "<bold>32.23</bold>"
      ]
    ],
    "id": "aded4603-ff26-4698-8185-8b0236c4f926",
    "claim": "G2S-GGNN does not outperform others with the same amount of Gigaword sentences (200K), as shown in Table 3, with a BLEU score of 32.23.",
    "label": "refutes",
    "table_id": "103c9a5f-ec19-41cf-bc63-cbd81b53f4b1"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "2f40f1d6-68bd-422d-8d22-55e0bef9a42c",
    "claim": "TF and DF achieved different values of precision, recall and f-measure using the English corpora, with TF achieving a higher precision (P=0.0150) and f-measure (F=0.0293) than DF when using the Europarl corpus in English.",
    "label": "refutes",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.694",
        "0.728",
        "[BOLD] 22.3",
        "8.81"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.702",
        "0.747",
        "23.6",
        "11.7"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.692",
        "0.781",
        "49.9",
        "[BOLD] 12.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.698",
        "0.754",
        "39.2",
        "12.0"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.702",
        "0.757",
        "33.9",
        "[BOLD] 12.8"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.688",
        "0.753",
        "28.6",
        "11.8"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.704",
        "[BOLD] 0.794",
        "63.2",
        "[BOLD] 12.8"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.706",
        "0.768",
        "49.0",
        "[BOLD] 12.8"
      ]
    ],
    "id": "e0ea0f70-dd7a-44e6-af9c-6da48ffb8b11",
    "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.",
    "label": "supports",
    "table_id": "db289154-f4e4-4abd-b1f8-f8df8da1b2de"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "81166961-1a5a-4aec-8f8b-554e7059779c",
    "claim": "[CONTINUE] ACER and PPO obtain high performance in inform F1 and match rate as well.",
    "label": "supports",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "3480e832-0450-4214-b055-4167d9e2ac36",
    "claim": "these results show that the questionnaire takers had an average accuracy of 98.2% in answering word intrusion questions for words associated with meanings imparted by standard word embeddings,",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 2: Ablation study results.",
    "table_column_names": [
      "[BOLD] Variation",
      "[BOLD] Accuracy (%)",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "Submitted",
        "[BOLD] 69.23",
        "-"
      ],
      [
        "No emoji",
        "68.36",
        "- 0.87"
      ],
      [
        "No ELMo",
        "65.52",
        "- 3.71"
      ],
      [
        "Concat Pooling",
        "68.47",
        "- 0.76"
      ],
      [
        "LSTM hidden=4096",
        "69.10",
        "- 0.13"
      ],
      [
        "LSTM hidden=1024",
        "68.93",
        "- 0.30"
      ],
      [
        "LSTM hidden=512",
        "68.43",
        "- 0.80"
      ],
      [
        "POS emb dim=100",
        "68.99",
        "- 0.24"
      ],
      [
        "POS emb dim=75",
        "68.61",
        "- 0.62"
      ],
      [
        "POS emb dim=50",
        "69.33",
        "+ 0.10"
      ],
      [
        "POS emb dim=25",
        "69.21",
        "- 0.02"
      ],
      [
        "SGD optim lr=1",
        "64.33",
        "- 4.90"
      ],
      [
        "SGD optim lr=0.1",
        "66.11",
        "- 3.12"
      ],
      [
        "SGD optim lr=0.01",
        "60.72",
        "- 8.51"
      ],
      [
        "SGD optim lr=0.001",
        "30.49",
        "- 38.74"
      ]
    ],
    "id": "b7aee9bb-1d0f-4611-9d2c-3e1f6f0b870f",
    "claim": "[CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 25-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al.",
    "label": "refutes",
    "table_id": "aca64ad8-2778-467a-93ff-0acc12c969e6"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.",
    "table_column_names": [
      "AMR Anno.",
      "BLEU"
    ],
    "table_content_values": [
      [
        "Automatic",
        "16.8"
      ],
      [
        "Gold",
        "[BOLD] *17.5*"
      ]
    ],
    "id": "154f0d35-a7d3-4bc4-94c3-9f8a040f2fe9",
    "claim": "[CONTINUE] The improvement from automatic AMR to gold AMR (+0.7 BLEU) is significant, which shows that the translation quality of our model can be further improved with an increase of AMR parsing accuracy.",
    "label": "supports",
    "table_id": "d3d1985b-cf33-4981-9332-d03be820d004"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "71651470-7e38-4147-9c5f-c80b5110cfb8",
    "claim": "This is expected because SVM is a linear classifier that relies solely on the 1-gram, while neural classifiers like CNNs (row3) and LSTMs (row4) cannot learn non-linear function of n-grams when the n is larger than the number of word vector dimensions.",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "22dea611-7444-4493-8117-cc4ac8e97dad",
    "claim": "for example, if the evaluation begins by calculating all analogies using the relation ‘capital-common-countries’ then in analogy 1, there will be [15_1 + 15_2 + 6_2]/50 = 6.3 out of 50 answers found by 1—1, i.e., 12.6% of 50 is counted as the score.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "bd51e05d-94ea-4aaf-8572-7fff74309537",
    "claim": "This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs.",
    "label": "supports",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "306fb06d-a734-46af-9d03-b1fd6bdb10a2",
    "claim": "The performance of each approach that interacts with the agenda-based user simulator is shown in [CONTINUE] Table 3.",
    "label": "supports",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "d48f4021-6614-4e16-96c9-f4fad3f39106",
    "claim": "[CONTINUE] Opinion distance methods generally outperform the competition on both ARI and Silhouette coefficient.",
    "label": "supports",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 4: Results of the ablation study on the LDC2017T10 development set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>",
      "<bold>Size</bold>"
    ],
    "table_content_values": [
      [
        "biLSTM",
        "22.50",
        "30.42",
        "57.6M"
      ],
      [
        "<italic>GEt</italic> + biLSTM",
        "26.33",
        "32.62",
        "59.6M"
      ],
      [
        "<italic>GEb</italic> + biLSTM",
        "26.12",
        "32.49",
        "59.6M"
      ],
      [
        "<italic>GEt</italic> + <italic>GEb</italic> + biLSTM",
        "27.37",
        "33.30",
        "61.7M"
      ]
    ],
    "id": "59840d7f-df7c-4eb3-8f66-0f78d22aac52",
    "claim": "The complete model has significantly more parameters than the model without graph encoders (57.6M vs 61.7M).",
    "label": "refutes",
    "table_id": "5dbdd29a-b64a-47ff-824a-6b8486b9ad4a"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "9cb4abcc-8929-4b33-9ff7-358228567dd5",
    "claim": "Unlike the above three models, Word2Sense does not use pretrained vectors.",
    "label": "not enough info",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "46bed304-5dec-4a0f-8327-05dcd43ca54c",
    "claim": "However, coverage can compensate for much of the lost performance in each case.",
    "label": "not enough info",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection",
    "paper_id": "1904.04388v1",
    "table_caption": "Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.",
    "table_column_names": [
      "[BOLD] Type",
      "[BOLD] Reparandum Length  [BOLD] 1-2",
      "[BOLD] Reparandum Length  [BOLD] 3-5"
    ],
    "table_content_values": [
      [
        "content-content",
        "0.61 (30%)",
        "0.58 (52%)"
      ],
      [
        "content-function",
        "0.77 (20%)",
        "0.66 (17%)"
      ],
      [
        "function-function",
        "0.83 (50%)",
        "0.80 (32%)"
      ]
    ],
    "id": "e8525ca4-669e-4b02-829f-c5aeedc8e85f",
    "claim": "We found that rephrase disfluencies that contain content words are easier for the model to detect, compared to rephrases with function words only, and error decreases for longer disfluencies.",
    "label": "refutes",
    "table_id": "d5f567cd-d943-4265-b564-1829f3c072c3"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc∗: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.",
    "table_column_names": [
      "Model",
      "BLEU",
      "Acc∗"
    ],
    "table_content_values": [
      [
        "fu-1",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Multi-decoder",
        "7.6",
        "0.792"
      ],
      [
        "Style embed.",
        "15.4",
        "0.095"
      ],
      [
        "simple-transfer",
        "simple-transfer",
        "simple-transfer"
      ],
      [
        "Template",
        "18.0",
        "0.867"
      ],
      [
        "Delete/Retrieve",
        "12.6",
        "0.909"
      ],
      [
        "yang2018unsupervised",
        "yang2018unsupervised",
        "yang2018unsupervised"
      ],
      [
        "LM",
        "13.4",
        "0.854"
      ],
      [
        "LM + classifier",
        "[BOLD] 22.3",
        "0.900"
      ],
      [
        "Untransferred",
        "[BOLD] 31.4",
        "0.024"
      ]
    ],
    "id": "3010d663-a981-41d4-8f6c-555026bb0257",
    "claim": "We additionally find that supervised BLEU does not show a trade-off with Acc: for a single model type, higher Acc does not necessarily correspond to lower BLEU.",
    "label": "refutes",
    "table_id": "a78a417b-c642-4ade-8df2-6998a00348c2"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "e7d70231-bd8d-4422-be22-05085a4427f0",
    "claim": "When humans are asked to choose an option which they believe is more likely to be a correct causal conclusion, 80% select the correct label.",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "8bc5a9aa-47c8-40c0-917e-6659a847af46",
    "claim": "In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions.",
    "label": "supports",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "c95f0f73-ac64-4722-b8af-676db2efd814",
    "claim": "We believe that this is because ‘to’ is either a correct or an incorrect cue: to achieve high consistency, an MLM will more likely answer when the cue is “to”, and answer the other alternative, otherwise.",
    "label": "not enough info",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "b6bc24e5-272f-4803-863d-d4d8ace3af9d",
    "claim": "We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform improvement in the correlation values for the proposed algorithm, outperforming all the alternatives except Word2Vec baseline on average.",
    "label": "supports",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] R-1",
      "[BOLD] R-2",
      "[BOLD] R-SU"
    ],
    "table_content_values": [
      [
        "First-1",
        "26.83",
        "7.25",
        "6.46"
      ],
      [
        "First-2",
        "35.99",
        "10.17",
        "12.06"
      ],
      [
        "First-3",
        "39.41",
        "11.77",
        "14.51"
      ],
      [
        "LexRank Erkan and Radev ( 2004 )",
        "38.27",
        "12.70",
        "13.20"
      ],
      [
        "TextRank Mihalcea and Tarau ( 2004 )",
        "38.44",
        "13.10",
        "13.50"
      ],
      [
        "MMR Carbonell and Goldstein ( 1998 )",
        "38.77",
        "11.98",
        "12.91"
      ],
      [
        "PG-Original Lebanoff et al. ( 2018 )",
        "41.85",
        "12.91",
        "16.46"
      ],
      [
        "PG-MMR Lebanoff et al. ( 2018 )",
        "40.55",
        "12.36",
        "15.87"
      ],
      [
        "PG-BRNN Gehrmann et al. ( 2018 )",
        "42.80",
        "14.19",
        "16.75"
      ],
      [
        "CopyTransformer Gehrmann et al. ( 2018 )",
        "[BOLD] 43.57",
        "14.03",
        "17.37"
      ],
      [
        "Hi-MAP (Our Model)",
        "43.47",
        "[BOLD] 14.89",
        "[BOLD] 17.41"
      ]
    ],
    "id": "e581bff7-189f-47c9-bc6e-ad38451ed188",
    "claim": "We observe an improvement in performance between PG-original and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model).",
    "label": "refutes",
    "table_id": "6f86cef3-2ee0-4782-b5b4-f37400c22dfb"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 2: Experiment 1",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.001",
        "0.003",
        "-20.818",
        "***",
        "0.505"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.083",
        "0.048",
        "101.636",
        "***",
        "1.724"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.001",
        "0.001",
        "0.035",
        "[EMPTY]",
        "1.001"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.023",
        "0.012",
        "64.418",
        "***",
        "1.993"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.002",
        "0.001",
        "4.047",
        "***",
        "1.120"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.049",
        "0.019",
        "120.986",
        "***",
        "2.573"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.173",
        "0.065",
        "243.285",
        "***",
        "2.653"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.032",
        "0.023",
        "39.483",
        "***",
        "1.396"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.111",
        "0.061",
        "122.707",
        "***",
        "1.812"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.178",
        "0.080",
        "211.319",
        "***",
        "2.239"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.028",
        "0.015",
        "63.131",
        "***",
        "1.854"
      ]
    ],
    "id": "1a799142-09bf-4cb2-a6c1-1c85df1089a8",
    "claim": "Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus.",
    "label": "supports",
    "table_id": "661ddd6e-51d9-446c-add1-3e80f3b78109"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "9df6dedc-23bf-468f-a0c5-7df7dd75c9e3",
    "claim": "In comparison, GDPL is still comparable with ACER and PPO, but does not obtain a better match rate, and even achieves lower task success.",
    "label": "refutes",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "a5addb02-2aad-498e-99b8-e2e4dd7cf0b0",
    "claim": "For example, GCN+RC+LA (10) achieves a BLEU score of 52.9, which is better than GCN+RC+LA (9).",
    "label": "refutes",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "352ed084-1079-4116-b4ec-37b4d6ebe790",
    "claim": "All G2S models have lower entailment compared to S2S.",
    "label": "refutes",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "20f5e50c-4725-4274-bdff-c605884d6206",
    "claim": "When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.4% F1 score decrease (A2−A1).",
    "label": "refutes",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "4a18f8e5-71c1-4cf1-8638-bed82a865841",
    "claim": "The difference between accuracy on Easy and Hard is more pronounced for RoBERTa, suggesting a reliance on superficial cues.",
    "label": "refutes",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
    "table_column_names": [
      "[BOLD] Model",
      "D",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN(1)",
        "300",
        "10.9M",
        "20.9",
        "52.0"
      ],
      [
        "DCGCN(2)",
        "180",
        "10.9M",
        "[BOLD] 22.2",
        "[BOLD] 52.3"
      ],
      [
        "DCGCN(2)",
        "240",
        "11.3M",
        "22.8",
        "52.8"
      ],
      [
        "DCGCN(4)",
        "180",
        "11.4M",
        "[BOLD] 23.4",
        "[BOLD] 53.4"
      ],
      [
        "DCGCN(1)",
        "420",
        "12.6M",
        "22.2",
        "52.4"
      ],
      [
        "DCGCN(2)",
        "300",
        "12.5M",
        "23.8",
        "53.8"
      ],
      [
        "DCGCN(3)",
        "240",
        "12.3M",
        "[BOLD] 23.9",
        "[BOLD] 54.1"
      ],
      [
        "DCGCN(2)",
        "360",
        "14.0M",
        "24.2",
        "[BOLD] 54.4"
      ],
      [
        "DCGCN(3)",
        "300",
        "14.0M",
        "[BOLD] 24.4",
        "54.2"
      ],
      [
        "DCGCN(2)",
        "420",
        "15.6M",
        "24.1",
        "53.7"
      ],
      [
        "DCGCN(4)",
        "300",
        "15.6M",
        "[BOLD] 24.6",
        "[BOLD] 54.8"
      ],
      [
        "DCGCN(3)",
        "420",
        "18.6M",
        "24.5",
        "54.6"
      ],
      [
        "DCGCN(4)",
        "360",
        "18.4M",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "f245ef3f-7ca3-4a2e-9b70-5f42ca63476c",
    "claim": "In general, we found when the parameter budget is the same, shallower DCGCN models can obtain better results than the deeper ones.",
    "label": "refutes",
    "table_id": "135bc50f-f12e-493b-854d-58858c4c5c86"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "a5a4fd4a-41ee-417e-a619-d1398be5d04a",
    "claim": "Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other.",
    "label": "supports",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "f721af34-eeee-4c8b-80f4-3e8ce9ac71a6",
    "claim": "However, we define that the “119.99” operator as: if RSI <= 119.99 then RSI meets the requirement.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "ea7b772f-808f-4add-97cf-6a2b10dfe9a9",
    "claim": "The hybrid model does not yield scores close to or even above the better model of the two on all tasks.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "74e89463-6301-450d-97e1-7cdefad17983",
    "claim": "[CONTINUE] The performances of all models decrease as the diameters of the graphs increase.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "c864e92d-ec01-45c1-aa22-91071f6e2241",
    "claim": "For BERT models, after fine-tuning on COPA, RoBERTa-large achieves the best performance on both Easy and Hard.",
    "label": "not enough info",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.818",
        "0.719",
        "37.3",
        "10.0"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.819",
        "0.734",
        "26.3",
        "14.2"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.813",
        "0.770",
        "36.4",
        "18.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.807",
        "0.796",
        "28.4",
        "21.5"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.798",
        "0.783",
        "39.7",
        "19.2"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.804",
        "0.785",
        "27.1",
        "20.3"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.805",
        "[BOLD] 0.817",
        "43.3",
        "21.6"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.818",
        "0.805",
        "[BOLD] 29.0",
        "[BOLD] 22.8"
      ]
    ],
    "id": "c815f80d-4626-4775-bfff-d8b3630c274d",
    "claim": "[CONTINUE] Table 2 shows that the model with paraphrase loss (M1) slightly improves Sim over M0 on both datasets under similar Acc.",
    "label": "supports",
    "table_id": "ce976181-466e-4d2c-a980-797bf966424e"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "5081e11b-aed5-4a7e-9c0a-00cf1b73e630",
    "claim": "Interestingly, the error analysis on this dataset revealed that the BiLSTM model was unable to correctly classify one-word scopes.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "8bea0857-41f9-4d7c-82b3-1bd70e74a1b4",
    "claim": "The models have worse results when handling sentences with 20 or fewer tokens.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "023aa496-aafd-4f83-aa81-5c76da94e3e0",
    "claim": "[CONTINUE] Due to joint training, our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1038",
        "0.0170",
        "0.0490",
        "0.0641",
        "0.0641",
        "0.0613",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1282",
        "0.0291",
        "0.0410",
        "0.0270",
        "0.0270",
        "0.1154",
        "0.0661"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.6185",
        "0.3744",
        "0.4144",
        "0.4394",
        "0.4394",
        "[BOLD] 0.7553",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.6308",
        "0.4124",
        "0.4404",
        "0.4515",
        "0.4945",
        "[BOLD] 0.8609",
        "0.5295"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "[BOLD] 0.0021",
        "0.0004",
        "0.0011",
        "0.0014",
        "0.0014",
        "0.0013",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0011",
        "0.0008",
        "0.0011",
        "0.0008",
        "0.0008",
        "[BOLD] 0.0030",
        "0.0018"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0012",
        "0.0008",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0016",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0003",
        "0.0009",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0017",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "[BOLD] 0.0041",
        "0.0007",
        "0.0021",
        "0.0027",
        "0.0027",
        "0.0026",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0022",
        "0.0016",
        "0.0022",
        "0.0015",
        "0.0015",
        "[BOLD] 0.0058",
        "0.0036"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0024",
        "0.0016",
        "0.0018",
        "0.0019",
        "0.0019",
        "[BOLD] 0.0031",
        "0.0023"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0005",
        "0.0018",
        "0.0018",
        "0.0020",
        "0.0021",
        "[BOLD] 0.0034",
        "0.0022"
      ]
    ],
    "id": "52d13569-4080-411e-a922-9afd23f2b1b1",
    "claim": "Despite filtering out multiple hypernyms, the recall values for the Portuguese corpora are still relatively high.",
    "label": "refutes",
    "table_id": "7ff90dc3-0887-4d7b-b7bf-bb5149801b4e"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "bc61ec91-f1d8-4416-9fdb-87ade3bbd1e8",
    "claim": "LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9.",
    "label": "supports",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "b3f85fb2-384a-484e-9e52-d0bc2a0efac5",
    "claim": "[CONTINUE] However, words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are not significantly associated with tweets that are not complaints.",
    "label": "refutes",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns",
    "paper_id": "1810.05201v1",
    "table_caption": "Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
    "table_column_names": [
      "[EMPTY]",
      "M",
      "F",
      "B",
      "O"
    ],
    "table_content_values": [
      [
        "Random",
        "47.5",
        "50.5",
        "[ITALIC] 1.06",
        "49.0"
      ],
      [
        "Token Distance",
        "50.6",
        "47.5",
        "[ITALIC] 0.94",
        "49.1"
      ],
      [
        "Topical Entity",
        "50.2",
        "47.3",
        "[ITALIC] 0.94",
        "48.8"
      ],
      [
        "Syntactic Distance",
        "66.7",
        "66.7",
        "[ITALIC]  [BOLD] 1.00",
        "66.7"
      ],
      [
        "Parallelism",
        "[BOLD] 69.3",
        "[BOLD] 69.2",
        "[ITALIC]  [BOLD] 1.00",
        "[BOLD] 69.2"
      ],
      [
        "Parallelism+URL",
        "[BOLD] 74.2",
        "[BOLD] 71.6",
        "[ITALIC]  [BOLD] 0.96",
        "[BOLD] 72.9"
      ],
      [
        "Transformer-Single",
        "59.6",
        "56.6",
        "[ITALIC] 0.95",
        "58.1"
      ],
      [
        "Transformer-Multi",
        "62.9",
        "61.7",
        "[ITALIC] 0.98",
        "62.3"
      ]
    ],
    "id": "1e1d2baf-44da-4531-9601-26027fdd859a",
    "claim": "RANDOM is indeed closer here to the expected 50% and other baselines are closer to gender-parity.",
    "label": "supports",
    "table_id": "dc3c5fd8-0e21-4c46-a1db-b82788e58324"
  },
  {
    "paper": "Assessing Gender Bias in Machine Translation – A Case Study with Google Translate",
    "paper_id": "1809.02208v4",
    "table_caption": "Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
    "table_column_names": [
      "Category",
      "Female (%)",
      "Male (%)",
      "Neutral (%)"
    ],
    "table_content_values": [
      [
        "Office and administrative support",
        "11.015",
        "58.812",
        "16.954"
      ],
      [
        "Architecture and engineering",
        "2.299",
        "72.701",
        "10.92"
      ],
      [
        "Farming, fishing, and forestry",
        "12.179",
        "62.179",
        "14.744"
      ],
      [
        "Management",
        "11.232",
        "66.667",
        "12.681"
      ],
      [
        "Community and social service",
        "20.238",
        "62.5",
        "10.119"
      ],
      [
        "Healthcare support",
        "25.0",
        "43.75",
        "17.188"
      ],
      [
        "Sales and related",
        "8.929",
        "62.202",
        "16.964"
      ],
      [
        "Installation, maintenance, and repair",
        "5.22",
        "58.333",
        "17.125"
      ],
      [
        "Transportation and material moving",
        "8.81",
        "62.976",
        "17.5"
      ],
      [
        "Legal",
        "11.905",
        "72.619",
        "10.714"
      ],
      [
        "Business and financial operations",
        "7.065",
        "67.935",
        "15.58"
      ],
      [
        "Life, physical, and social science",
        "5.882",
        "73.284",
        "10.049"
      ],
      [
        "Arts, design, entertainment, sports, and media",
        "10.36",
        "67.342",
        "11.486"
      ],
      [
        "Education, training, and library",
        "23.485",
        "53.03",
        "9.091"
      ],
      [
        "Building and grounds cleaning and maintenance",
        "12.5",
        "68.333",
        "11.667"
      ],
      [
        "Personal care and service",
        "18.939",
        "49.747",
        "18.434"
      ],
      [
        "Healthcare practitioners and technical",
        "22.674",
        "51.744",
        "15.116"
      ],
      [
        "Production",
        "14.331",
        "51.199",
        "18.245"
      ],
      [
        "Computer and mathematical",
        "4.167",
        "66.146",
        "14.062"
      ],
      [
        "Construction and extraction",
        "8.578",
        "61.887",
        "17.525"
      ],
      [
        "Protective service",
        "8.631",
        "65.179",
        "12.5"
      ],
      [
        "Food preparation and serving related",
        "21.078",
        "58.333",
        "17.647"
      ],
      [
        "Total",
        "11.76",
        "58.93",
        "15.939"
      ]
    ],
    "id": "18c4bae1-104d-4e43-bcbc-da4b289a9d62",
    "claim": "What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general.",
    "label": "supports",
    "table_id": "2e483d7f-201a-4e26-b179-5216c375183e"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "d2a8a2b1-1542-4216-b507-9eb0813d114b",
    "claim": "PCS can detect 4,113 new scope relations, 833 fewer than with gold cues.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer aroma+palate",
        "Beer look",
        "74.41",
        "74.83",
        "74.94",
        "72.75",
        "76.41",
        "[BOLD] 79.53",
        "80.29"
      ],
      [
        "Beer look+palate",
        "Beer aroma",
        "68.57",
        "69.23",
        "67.55",
        "69.92",
        "76.45",
        "[BOLD] 77.94",
        "78.11"
      ],
      [
        "Beer look+aroma",
        "Beer palate",
        "63.88",
        "67.82",
        "65.72",
        "74.66",
        "73.40",
        "[BOLD] 75.24",
        "75.50"
      ]
    ],
    "id": "edeea560-96b7-4321-b28d-aa8e670d56ca",
    "claim": "We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) underperforms ORACLE by a large margin.",
    "label": "supports",
    "table_id": "461a04b0-4d80-4f54-a0ed-42b74709e562"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "aa63576f-129c-4fdd-a6d2-2f7410459284",
    "claim": "GDPL outperforms three baselines significantly in all aspects (sign test, p-value < 0.01) except for the quality compared with ACER.",
    "label": "supports",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
    "table_column_names": [
      "[EMPTY]",
      "WN-N P",
      "WN-N R",
      "WN-N F",
      "WN-V P",
      "WN-V R",
      "WN-V F",
      "VN P",
      "VN R",
      "VN F"
    ],
    "table_content_values": [
      [
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2"
      ],
      [
        "type",
        ".700",
        ".654",
        ".676",
        ".535",
        ".474",
        ".503",
        ".327",
        ".309",
        ".318"
      ],
      [
        "x+POS",
        ".699",
        ".651",
        ".674",
        ".544",
        ".472",
        ".505",
        ".339",
        ".312",
        ".325"
      ],
      [
        "lemma",
        ".706",
        ".660",
        ".682",
        ".576",
        ".520",
        ".547",
        ".384",
        ".360",
        ".371"
      ],
      [
        "x+POS",
        "<bold>.710</bold>",
        "<bold>.662</bold>",
        "<bold>.685</bold>",
        "<bold>.589</bold>",
        "<bold>.529</bold>",
        "<bold>.557</bold>",
        "<bold>.410</bold>",
        "<bold>.389</bold>",
        "<bold>.399</bold>"
      ],
      [
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep"
      ],
      [
        "type",
        ".712",
        ".661",
        ".686",
        ".545",
        ".457",
        ".497",
        ".324",
        ".296",
        ".310"
      ],
      [
        "x+POS",
        ".715",
        ".659",
        ".686",
        ".560",
        ".464",
        ".508",
        ".349",
        ".320",
        ".334"
      ],
      [
        "lemma",
        "<bold>.725</bold>",
        "<bold>.668</bold>",
        "<bold>.696</bold>",
        ".591",
        ".512",
        ".548",
        ".408",
        ".371",
        ".388"
      ],
      [
        "x+POS",
        ".722",
        ".666",
        ".693",
        "<bold>.609</bold>",
        "<bold>.527</bold>",
        "<bold>.565</bold>",
        "<bold>.412</bold>",
        "<bold>.381</bold>",
        "<bold>.396</bold>"
      ]
    ],
    "id": "c6f524cd-84be-41ec-9b1e-61695d024d5c",
    "claim": "[CONTINUE] Lemma-based targets without POS disambiguation perform best on WN-N when dependency-based contexts are used; however, the difference to lemmatized and disambiguated targets is not statistically significant (p > .1).",
    "label": "supports",
    "table_id": "051fe422-03d4-44d5-836c-7f982b328555"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "4301bd18-8256-4555-b14e-6fa07d64c262",
    "claim": "Another interesting fact in Table 1 is that the training throughput on the linear dataset does not scale better than the throughput on the balanced dataset, as the batch size increases.",
    "label": "refutes",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "58ed9fb7-c389-41e3-a456-a2d5f2d7dc0e",
    "claim": "Negation can be either clearly express or be subtly used.",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "e2bf9a66-bb8d-4c47-b276-a33bcb67117c",
    "claim": "Table 4 shows that GDPL has the smallest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves more like the human.",
    "label": "supports",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "298f453e-23a7-4de3-a6b1-be249400fc27",
    "claim": "This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations.",
    "label": "supports",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
    "table_column_names": [
      "System",
      "All P",
      "All R",
      "All F1",
      "In  [ITALIC] E+ P",
      "In  [ITALIC] E+ R",
      "In  [ITALIC] E+ F1"
    ],
    "table_content_values": [
      [
        "Name matching",
        "15.03",
        "15.03",
        "15.03",
        "29.13",
        "29.13",
        "29.13"
      ],
      [
        "MIL (model 1)",
        "35.87",
        "35.87",
        "35.87 ±0.72",
        "69.38",
        "69.38",
        "69.38 ±1.29"
      ],
      [
        "MIL-ND (model 2)",
        "37.42",
        "[BOLD] 37.42",
        "37.42 ±0.35",
        "72.50",
        "[BOLD] 72.50",
        "[BOLD] 72.50 ±0.68"
      ],
      [
        "[ITALIC] τMIL-ND (model 2)",
        "[BOLD] 38.91",
        "36.73",
        "[BOLD] 37.78 ±0.26",
        "[BOLD] 73.19",
        "71.15",
        "72.16 ±0.48"
      ],
      [
        "Supervised learning",
        "42.90",
        "42.90",
        "42.90 ±0.59",
        "83.12",
        "83.12",
        "83.12 ±1.15"
      ]
    ],
    "id": "bbea621d-e96a-41a8-ad30-309fb0ef0a51",
    "claim": "MIL-ND does not achieve higher precision, recall, and F1 than MIL, and using its confidence at test time (τ MIL-ND, 'All' setting) was not beneficial in terms of precision and F1.",
    "label": "refutes",
    "table_id": "ce21ef70-1b61-4a48-9c97-69426861922c"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "216f48dd-a7c0-4e32-a01f-007f70a4db01",
    "claim": "Table 9: Performance of different models on the neural user simulator.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection",
    "paper_id": "3",
    "table_caption": "Table 5: Task B results",
    "table_column_names": [
      "[EMPTY]",
      "Micro F1"
    ],
    "table_content_values": [
      [
        "Baseline",
        "0.709"
      ],
      [
        "W2V (<italic>d</italic>=50)",
        "0.736"
      ],
      [
        "W2V (<italic>d</italic>=500)",
        "0.753"
      ],
      [
        "S2V",
        "0.748"
      ],
      [
        "S2V + W2V (<italic>d</italic>=50)",
        "0.744"
      ],
      [
        "S2V + K + W2V(<italic>d</italic>=50)",
        "0.749"
      ],
      [
        "SIF (DE)",
        "0.759"
      ],
      [
        "SIF (DE-EN)",
        "<bold>0.765</bold>"
      ]
    ],
    "id": "086ef478-1afa-472e-b71f-ca7b784c40fb",
    "claim": "For Task B, all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings.",
    "label": "supports",
    "table_id": "2c898525-6aaa-4bee-b355-9151026dcc35"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "34bd9559-e3ae-43a5-9c37-ca40fbcdd85a",
    "claim": "LSTM does significantly better than Word2Vec, especially for MEN-TR-3k (0.766 vs. 0.552, p-value<0.00001), RG65 (0.790 vs. 0.744, p-value<0.00001) and MTurk771 (0.682 vs. 0.650, p-value<0.00001).",
    "label": "not enough info",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "cbc15a85-cab6-4dbe-acc7-941079c444f5",
    "claim": "A potential reason is that the RL agent has only learned limited useful signals in some small, low-quality data; therefore, other summarisation signals including aspect modelling, coverage modelling and salience modelling are missing.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "091c653e-d166-4f19-a914-0f0a73b1b51e",
    "claim": "Our proposed method outperforms Pretrained Word2Sense embeddings, despite the latter having the advantage of training on a larger corpus.",
    "label": "refutes",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "036aebad-b965-470d-beea-378f90f6e122",
    "claim": "The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "label": "supports",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "a48fe54a-935d-4004-97f5-8dbf683c3567",
    "claim": "[CONTINUE] Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005).",
    "label": "supports",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "1503579e-421f-4f43-8df4-b7ad246a398e",
    "claim": "based on the analysis results, we conclude that the dialog states were successfully retained in the model policy and user simulator, but the source of error lies in the action selection with respect to informability and matching.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 1: Effect of using the shortest dependency path on each relation type.",
    "table_column_names": [
      "[BOLD] Relation",
      "[BOLD] best F1 (in 5-fold) without sdp",
      "[BOLD] best F1 (in 5-fold) with sdp",
      "[BOLD] Diff."
    ],
    "table_content_values": [
      [
        "USAGE",
        "60.34",
        "80.24",
        "+ 19.90"
      ],
      [
        "MODEL-FEATURE",
        "48.89",
        "70.00",
        "+ 21.11"
      ],
      [
        "PART_WHOLE",
        "29.51",
        "70.27",
        "+40.76"
      ],
      [
        "TOPIC",
        "45.80",
        "91.26",
        "+45.46"
      ],
      [
        "RESULT",
        "54.35",
        "81.58",
        "+27.23"
      ],
      [
        "COMPARE",
        "20.00",
        "61.82",
        "+ 41.82"
      ],
      [
        "macro-averaged",
        "50.10",
        "76.10",
        "+26.00"
      ]
    ],
    "id": "3197a3a2-dcb8-4bbb-8be5-a2e8eee69365",
    "claim": "However, the sdp information has a clear positive impact on all the relation types (Table 1).",
    "label": "supports",
    "table_id": "1eca16bf-a63a-45a1-a4a3-84f12cf74c95"
  },
  {
    "paper": "Neural End-to-End Learning for Computational Argumentation Mining",
    "paper_id": "1704.06104v2",
    "table_caption": "Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.",
    "table_column_names": [
      "[EMPTY]",
      "C-F1 100%",
      "C-F1 50%",
      "R-F1 100%",
      "R-F1 50%",
      "F1 100%",
      "F1 50%"
    ],
    "table_content_values": [
      [
        "Y-3",
        "49.59",
        "65.37",
        "26.28",
        "37.00",
        "34.35",
        "47.25"
      ],
      [
        "Y-3:Y<italic>C</italic>-1",
        "54.71",
        "66.84",
        "28.44",
        "37.35",
        "37.40",
        "47.92"
      ],
      [
        "Y-3:Y<italic>R</italic>-1",
        "51.32",
        "66.49",
        "26.92",
        "37.18",
        "35.31",
        "47.69"
      ],
      [
        "Y-3:Y<italic>C</italic>-3",
        "<bold>54.58</bold>",
        "67.66",
        "<bold>30.22</bold>",
        "<bold>40.30</bold>",
        "<bold>38.90</bold>",
        "<bold>50.51</bold>"
      ],
      [
        "Y-3:Y<italic>R</italic>-3",
        "53.31",
        "66.71",
        "26.65",
        "35.86",
        "35.53",
        "46.64"
      ],
      [
        "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
        "52.95",
        "<bold>67.84</bold>",
        "27.90",
        "39.71",
        "36.54",
        "50.09"
      ],
      [
        "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
        "54.55",
        "67.60",
        "28.30",
        "38.26",
        "37.26",
        "48.86"
      ]
    ],
    "id": "a8be9400-0253-4cda-ad4a-06b707c381b5",
    "claim": "Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally stronger: [CONTINUE] as in Eq.",
    "label": "refutes",
    "table_id": "d0039005-f056-4745-9652-8796a2c2b307"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "6a91430a-d1d2-4fc6-80ff-a18af52d33a8",
    "claim": "In contrast, our proposed classifier can almost precisely identify the one-word scope without any syntactic information.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>ADDED</bold>",
      "<bold>MISS</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "47.34",
        "37.14"
      ],
      [
        "G2S-GIN",
        "48.67",
        "33.64"
      ],
      [
        "G2S-GAT",
        "48.24",
        "33.73"
      ],
      [
        "G2S-GGNN",
        "48.66",
        "34.06"
      ],
      [
        "GOLD",
        "50.77",
        "28.35"
      ],
      [
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ]
    ],
    "id": "9e0ce9f6-6eb5-4eda-97f7-ec6948db9bf5",
    "claim": "As shown in Table 8, the S2S baseline outperforms the G2S approaches.",
    "label": "refutes",
    "table_id": "623385a5-d0a0-41c3-90d3-29ada42c8827"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "bfb26805-c8f6-4854-9e1c-14b7534b396e",
    "claim": "We suspect that there are not enough data to pretrain the models and that the thread classification task used to pretrain the HAN models may not be sophisticated enough to learn effective thread vectors.",
    "label": "supports",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "883e2064-9987-4f7e-8c74-d971736b8f14",
    "claim": "This strongly indicates that there is a superficial cue that affects model performance, but this cue is not captured by Word frequency",
    "label": "not enough info",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "87943fea-34a2-4928-b643-cc2d1a2e32fd",
    "claim": "We also have competitive results to Guo et al.",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "64086c42-77ce-41fd-8af0-b264671ca83c",
    "claim": "[CONTINUE] When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DocSub achieved better values of precision, but lower values of recall.",
    "label": "supports",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 2: Human evaluation results on MSCOCO.",
    "table_column_names": [
      "[EMPTY]",
      "caption",
      "attention relevance"
    ],
    "table_content_values": [
      [
        "softmax",
        "3.50",
        "3.38"
      ],
      [
        "sparsemax",
        "3.71",
        "3.89"
      ],
      [
        "TVmax",
        "[BOLD] 3.87",
        "[BOLD] 4.10"
      ]
    ],
    "id": "e7aa3eec-8b70-4c8f-820e-a6c49ccaf595",
    "claim": "Despite performing slightly worse than sparsemax under automatic metrics, TVMAX outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2.",
    "label": "supports",
    "table_id": "77097db2-3bbc-4070-a3a0-2eaad08e47de"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "edf3572a-6423-4a55-ad0e-a09662bdec7c",
    "claim": "We have 116,674 tweets, with an average length of 22.3 tokens.",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "56156f9c-7163-4650-9091-2240dfc00b55",
    "claim": "The results in Table 3 show that translation quality of LRN is slightly worse than that of GRU (-0.02 BLEU).",
    "label": "supports",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "1af207f8-66bc-4bfc-b6a3-c651e12783f0",
    "claim": "Without the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.",
    "label": "supports",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "Logistic Regression",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Sentiment – MPQA",
        "64.2",
        "39.1",
        "0.499"
      ],
      [
        "Sentiment – NRC",
        "63.9",
        "42.2",
        "0.599"
      ],
      [
        "Sentiment – V&B",
        "68.9",
        "60.0",
        "0.696"
      ],
      [
        "Sentiment – VADER",
        "66.0",
        "54.2",
        "0.654"
      ],
      [
        "Sentiment – Stanford",
        "68.0",
        "55.6",
        "0.696"
      ],
      [
        "Complaint Specific (all)",
        "65.7",
        "55.2",
        "0.634"
      ],
      [
        "Request",
        "64.2",
        "39.1",
        "0.583"
      ],
      [
        "Intensifiers",
        "64.5",
        "47.3",
        "0.639"
      ],
      [
        "Downgraders",
        "65.4",
        "49.8",
        "0.615"
      ],
      [
        "Temporal References",
        "64.2",
        "43.7",
        "0.535"
      ],
      [
        "Pronoun Types",
        "64.1",
        "39.1",
        "0.545"
      ],
      [
        "POS Bigrams",
        "72.2",
        "66.8",
        "0.756"
      ],
      [
        "LIWC",
        "71.6",
        "65.8",
        "0.784"
      ],
      [
        "Word2Vec Clusters",
        "67.7",
        "58.3",
        "0.738"
      ],
      [
        "Bag-of-Words",
        "79.8",
        "77.5",
        "0.866"
      ],
      [
        "All Features",
        "[BOLD] 80.5",
        "[BOLD] 78.0",
        "[BOLD] 0.873"
      ],
      [
        "Neural Networks",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "MLP",
        "78.3",
        "76.2",
        "0.845"
      ],
      [
        "LSTM",
        "80.2",
        "77.0",
        "0.864"
      ]
    ],
    "id": "d1da902e-3754-4ed7-afcf-cc1642e09b62",
    "claim": "However, models trained using linguistic features on the training data obtain significantly higher predictive accuracy.",
    "label": "supports",
    "table_id": "caac7a4b-bc0e-4b38-bc24-9d0ad39f2fa7"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 2: F1 score results per relation type of the best performing models.",
    "table_column_names": [
      "Relation type",
      "Count",
      "Intra-sentential co-occ.  [ITALIC] ρ=0",
      "Intra-sentential co-occ.  [ITALIC] ρ=5",
      "Intra-sentential co-occ.  [ITALIC] ρ=10",
      "BoC(Wiki-PubMed-PMC) LR",
      "BoC(Wiki-PubMed-PMC) SVM",
      "BoC(Wiki-PubMed-PMC) ANN"
    ],
    "table_content_values": [
      [
        "TherapyTiming(TP,TD)",
        "428",
        "[BOLD] 0.84",
        "0.59",
        "0.47",
        "0.78",
        "0.81",
        "0.78"
      ],
      [
        "NextReview(Followup,TP)",
        "164",
        "[BOLD] 0.90",
        "0.83",
        "0.63",
        "0.86",
        "0.88",
        "0.84"
      ],
      [
        "Toxicity(TP,CF/TR)",
        "163",
        "[BOLD] 0.91",
        "0.77",
        "0.55",
        "0.85",
        "0.86",
        "0.86"
      ],
      [
        "TestTiming(TN,TD/TP)",
        "184",
        "0.90",
        "0.81",
        "0.42",
        "0.96",
        "[BOLD] 0.97",
        "0.95"
      ],
      [
        "TestFinding(TN,TR)",
        "136",
        "0.76",
        "0.60",
        "0.44",
        "[BOLD] 0.82",
        "0.79",
        "0.78"
      ],
      [
        "Threat(O,CF/TR)",
        "32",
        "0.85",
        "0.69",
        "0.54",
        "[BOLD] 0.95",
        "[BOLD] 0.95",
        "0.92"
      ],
      [
        "Intervention(TP,YR)",
        "5",
        "[BOLD] 0.88",
        "0.65",
        "0.47",
        "-",
        "-",
        "-"
      ],
      [
        "EffectOf(Com,CF)",
        "3",
        "[BOLD] 0.92",
        "0.62",
        "0.23",
        "-",
        "-",
        "-"
      ],
      [
        "Severity(CF,CS)",
        "75",
        "[BOLD] 0.61",
        "0.53",
        "0.47",
        "0.52",
        "0.55",
        "0.51"
      ],
      [
        "RecurLink(YR,YR/CF)",
        "7",
        "[BOLD] 1.0",
        "[BOLD] 1.0",
        "0.64",
        "-",
        "-",
        "-"
      ],
      [
        "RecurInfer(NR/YR,TR)",
        "51",
        "0.97",
        "0.69",
        "0.43",
        "[BOLD] 0.99",
        "[BOLD] 0.99",
        "0.98"
      ],
      [
        "GetOpinion(Referral,CF/other)",
        "4",
        "[BOLD] 0.75",
        "[BOLD] 0.75",
        "0.5",
        "-",
        "-",
        "-"
      ],
      [
        "Context(Dis,DisCont)",
        "40",
        "[BOLD] 0.70",
        "0.63",
        "0.53",
        "0.60",
        "0.41",
        "0.57"
      ],
      [
        "TestToAssess(TN,CF/TR)",
        "36",
        "0.76",
        "0.66",
        "0.36",
        "[BOLD] 0.92",
        "[BOLD] 0.92",
        "0.91"
      ],
      [
        "TimeStamp(TD,TP)",
        "221",
        "[BOLD] 0.88",
        "0.83",
        "0.50",
        "0.86",
        "0.85",
        "0.83"
      ],
      [
        "TimeLink(TP,TP)",
        "20",
        "[BOLD] 0.92",
        "0.85",
        "0.45",
        "0.91",
        "[BOLD] 0.92",
        "0.90"
      ],
      [
        "Overall",
        "1569",
        "0.90",
        "0.73",
        "0.45",
        "0.92",
        "[BOLD] 0.93",
        "0.91"
      ]
    ],
    "id": "4c5f3d6e-0a82-4654-8825-545bffd70dd2",
    "claim": "[CONTINUE] As the results of applying the co-occurrence baseline (ρ = 0) shows (Table 2), the semantic relations in this data are strongly concentrated within a sentence boundary, especially for the relation of RecurLink, with an F1 of 1.0.",
    "label": "supports",
    "table_id": "a49b4909-de4c-4505-8b9c-c37d9a0137c3"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "9faf0fb8-7f04-487b-8c21-c849d0edd997",
    "claim": "Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input.",
    "label": "supports",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "fe88348b-5c34-4558-8ca9-26f3e4deb724",
    "claim": "between all three systems, GloVe ranks last, followed by the original implementation of this model, and finally the optimized implementation.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.818",
        "0.719",
        "37.3",
        "10.0"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.819",
        "0.734",
        "26.3",
        "14.2"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.813",
        "0.770",
        "36.4",
        "18.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.807",
        "0.796",
        "28.4",
        "21.5"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.798",
        "0.783",
        "39.7",
        "19.2"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.804",
        "0.785",
        "27.1",
        "20.3"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.805",
        "[BOLD] 0.817",
        "43.3",
        "21.6"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.818",
        "0.805",
        "[BOLD] 29.0",
        "[BOLD] 22.8"
      ]
    ],
    "id": "54edbc96-7e26-4732-9e3c-08ce9c75397e",
    "claim": "For Yelp, M0 has better Acc and PP than M1 at comparable semantic similarity.",
    "label": "refutes",
    "table_id": "ce976181-466e-4d2c-a980-797bf966424e"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "a6372ae5-cf0e-4ad3-b2e6-864fcecf61d1",
    "claim": "on the other hand, neither the distance nor syntactic feature plays an important role in entity coreference performance, which indicates that the relation types of entities provide valuable information for cross-document entity coreference resolution.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability",
    "paper_id": "1912.12628v1",
    "table_caption": "Table 1: Accuracy obtained by training an standalone classifier, applying the API and the proposed wrapper for each domain",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] BB source acc.",
      "[BOLD] BB target acc.",
      "[BOLD] Non-reject. acc. (10/20/30%)",
      "[BOLD] Class. quality (10/20/30%)",
      "[BOLD] Reject. quality (10/20/30%)"
    ],
    "table_content_values": [
      [
        "[BOLD] Apply Yelp BB to SST-2",
        "89.18±0.08%",
        "77.13±0.52%",
        "82.43±0.22% 88.19±0.50% 93.60±0.16%",
        "80.40±0.39% 83.11±0.80% 83.05±0.23%",
        "6.03±0.45 6.04±0.51 4.97±0.07"
      ],
      [
        "[BOLD] Apply SST-2 BB to Yelp",
        "83.306±0.18%",
        "82.106±0.88%",
        "87,98±0.18% 92.13±0.38% 94.19±0.33%",
        "85.49±0.88% 84.53±0.38% 78.99±0.46%",
        "8.30±1.63 5.72±0.27 3.73±0.10"
      ],
      [
        "[BOLD] Apply Electronics BB to Music",
        "86.39±0.22%",
        "90.38±0.13%",
        "95.04±0.43% 96.45±0.35% 97.26±0.31%",
        "90.67±0.88% 83.93±0.67% 75.77±0.54%",
        "10.7±1.65 4.82±0.35 3.25±0.14"
      ],
      [
        "[BOLD] Apply Music BB to Electronics",
        "93.10±0.02%",
        "79.85±0.0%",
        "83.26±0.41% 87.06±0.55% 90.50±0.29%",
        "79.97±0.74% 79.93±0.87% 76.81±0.41%",
        "4.1±0.55 3.80±0.35 3.32±0.09"
      ]
    ],
    "id": "be06cdae-15fd-48da-8cf2-f7558b2216da",
    "claim": "[CONTINUE] In general terms, the results displayed in table 1 show that the rejection method can reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain.",
    "label": "supports",
    "table_id": "306969fe-d4f7-4a0d-a211-e63a8af0368a"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.",
    "table_column_names": [
      "[ITALIC] k",
      "Ar",
      "Es",
      "Fr",
      "Ru",
      "Zh",
      "En"
    ],
    "table_content_values": [
      [
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy"
      ],
      [
        "0",
        "88.0",
        "87.9",
        "87.9",
        "87.8",
        "87.7",
        "87.4"
      ],
      [
        "1",
        "92.4",
        "91.9",
        "92.1",
        "92.1",
        "91.5",
        "89.4"
      ],
      [
        "2",
        "91.9",
        "91.8",
        "91.8",
        "91.8",
        "91.3",
        "88.3"
      ],
      [
        "3",
        "92.0",
        "92.3",
        "92.1",
        "91.6",
        "91.2",
        "87.9"
      ],
      [
        "4",
        "92.1",
        "92.4",
        "92.5",
        "92.0",
        "90.5",
        "86.9"
      ],
      [
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy"
      ],
      [
        "0",
        "81.9",
        "81.9",
        "81.8",
        "81.8",
        "81.8",
        "81.2"
      ],
      [
        "1",
        "87.9",
        "87.7",
        "87.8",
        "87.9",
        "87.7",
        "84.5"
      ],
      [
        "2",
        "87.4",
        "87.5",
        "87.4",
        "87.3",
        "87.2",
        "83.2"
      ],
      [
        "3",
        "87.8",
        "87.9",
        "87.9",
        "87.3",
        "87.3",
        "82.9"
      ],
      [
        "4",
        "88.3",
        "88.6",
        "88.4",
        "88.1",
        "87.7",
        "82.1"
      ],
      [
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU"
      ],
      [
        "[EMPTY]",
        "32.7",
        "49.1",
        "38.5",
        "34.2",
        "32.1",
        "96.6"
      ]
    ],
    "id": "438c0d33-4a57-47d3-9bbc-d767c13a8119",
    "claim": "[CONTINUE] Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 1 and does not improve at higher layers, with some drops at layers 2 and 3.",
    "label": "supports",
    "table_id": "430d822d-f0b9-4d4b-b9a6-b995a9e11686"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "f7b8c7ad-fdaf-48cc-9171-2c26b57c1fb0",
    "claim": "the distribution of dialogue sessions can be seen in Fig.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 4: Accuracy of transferring between domains. Models with † use labeled data from source domains and unlabeled data from the target domain. Models with ‡ use human rationales on the target task.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer look + Beer aroma + Beer palate",
        "Hotel location",
        "78.65",
        "79.09",
        "79.28",
        "80.42",
        "82.10",
        "[BOLD] 84.52",
        "85.43"
      ],
      [
        "Beer look + Beer aroma + Beer palate",
        "Hotel cleanliness",
        "86.44",
        "86.68",
        "89.01",
        "86.95",
        "87.15",
        "[BOLD] 90.66",
        "92.09"
      ],
      [
        "Beer look + Beer aroma + Beer palate",
        "Hotel service",
        "85.34",
        "86.61",
        "87.91",
        "87.37",
        "86.40",
        "[BOLD] 89.93",
        "92.42"
      ]
    ],
    "id": "f0eebd6c-646f-409a-8ea7-e68af50007a0",
    "claim": "The error reduction over the best baseline is 15.08% on average.",
    "label": "supports",
    "table_id": "6258d803-d62d-43e0-9e7c-29fb98cf0ad8"
  },
  {
    "paper": "A context sensitive real-time Spell Checker with language adaptability",
    "paper_id": "1910.11242v1",
    "table_caption": "TABLE II: Synthetic Data Performance results",
    "table_column_names": [
      "[BOLD] Language",
      "[BOLD] # Test",
      "[BOLD] P@1",
      "[BOLD] P@3",
      "[BOLD] P@5",
      "[BOLD] P@10",
      "[BOLD] MRR"
    ],
    "table_content_values": [
      [
        "[BOLD] Language",
        "[BOLD] Samples",
        "[BOLD] P@1",
        "[BOLD] P@3",
        "[BOLD] P@5",
        "[BOLD] P@10",
        "[BOLD] MRR"
      ],
      [
        "Bengali",
        "140000",
        "91.30",
        "97.83",
        "98.94",
        "99.65",
        "94.68"
      ],
      [
        "Czech",
        "94205",
        "95.84",
        "98.72",
        "99.26",
        "99.62",
        "97.37"
      ],
      [
        "Danish",
        "140000",
        "85.84",
        "95.19",
        "97.28",
        "98.83",
        "90.85"
      ],
      [
        "Dutch",
        "140000",
        "86.83",
        "95.01",
        "97.04",
        "98.68",
        "91.32"
      ],
      [
        "English",
        "140000",
        "97.08",
        "99.39",
        "99.67",
        "99.86",
        "98.27"
      ],
      [
        "Finnish",
        "140000",
        "97.77",
        "99.58",
        "99.79",
        "99.90",
        "98.69"
      ],
      [
        "French",
        "140000",
        "86.52",
        "95.66",
        "97.52",
        "98.83",
        "91.38"
      ],
      [
        "German",
        "140000",
        "87.58",
        "96.16",
        "97.86",
        "99.05",
        "92.10"
      ],
      [
        "Greek",
        "30022",
        "84.95",
        "94.99",
        "96.88",
        "98.44",
        "90.27"
      ],
      [
        "Hebrew",
        "132596",
        "94.00",
        "98.26",
        "99.05",
        "99.62",
        "96.24"
      ],
      [
        "Hindi",
        "140000",
        "82.19",
        "93.71",
        "96.28",
        "98.30",
        "88.40"
      ],
      [
        "Indonesian",
        "140000",
        "95.01",
        "98.98",
        "99.50",
        "99.84",
        "97.04"
      ],
      [
        "Italian",
        "140000",
        "89.93",
        "97.31",
        "98.54",
        "99.38",
        "93.76"
      ],
      [
        "Marathi",
        "140000",
        "93.01",
        "98.16",
        "99.06",
        "99.66",
        "95.69"
      ],
      [
        "Polish",
        "140000",
        "95.65",
        "99.17",
        "99.62",
        "99.86",
        "97.44"
      ],
      [
        "Portuguese",
        "140000",
        "86.73",
        "96.29",
        "97.94",
        "99.10",
        "91.74"
      ],
      [
        "Romanian",
        "140000",
        "95.52",
        "98.79",
        "99.32",
        "99.68",
        "97.22"
      ],
      [
        "Russian",
        "140000",
        "94.85",
        "98.74",
        "99.33",
        "99.71",
        "96.86"
      ],
      [
        "Spanish",
        "140000",
        "85.91",
        "95.35",
        "97.18",
        "98.57",
        "90.92"
      ],
      [
        "Swedish",
        "140000",
        "88.86",
        "96.40",
        "98.00",
        "99.14",
        "92.87"
      ],
      [
        "Tamil",
        "140000",
        "98.05",
        "99.70",
        "99.88",
        "99.98",
        "98.88"
      ],
      [
        "Telugu",
        "140000",
        "97.11",
        "99.68",
        "99.92",
        "99.99",
        "98.38"
      ],
      [
        "Thai",
        "12403",
        "98.73",
        "99.71",
        "99.78",
        "99.85",
        "99.22"
      ],
      [
        "Turkish",
        "140000",
        "97.13",
        "99.51",
        "99.78",
        "99.92",
        "98.33"
      ]
    ],
    "id": "31b8b6fe-df87-467d-9695-321d94ad69f9",
    "claim": "The system does not perform well on synthetic dataset with a minimum of 80% P@1 and 98% P@10.",
    "label": "refutes",
    "table_id": "4a86596e-7c02-40e6-9cc4-de0764dd6350"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 3: Performance comparison of our model with different values of m on the two datasets.",
    "table_column_names": [
      "[ITALIC] m",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "1",
        "0.541",
        "0.595",
        "[BOLD] 0.566",
        "0.495",
        "0.621",
        "0.551"
      ],
      [
        "2",
        "0.521",
        "0.597",
        "0.556",
        "0.482",
        "0.656",
        "0.555"
      ],
      [
        "3",
        "0.490",
        "0.617",
        "0.547",
        "0.509",
        "0.633",
        "0.564"
      ],
      [
        "4",
        "0.449",
        "0.623",
        "0.522",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "5",
        "0.467",
        "0.609",
        "0.529",
        "0.488",
        "0.677",
        "0.567"
      ]
    ],
    "id": "6369cebf-b9ca-4fd0-9d72-621681d2ebdf",
    "claim": "These experiments show that the number of factors giving the best performance does not vary depending on the underlying data distribution.",
    "label": "refutes",
    "table_id": "fbd16514-f1a6-4567-8557-22b8d673c5b6"
  },
  {
    "paper": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation",
    "paper_id": "1909.00754v2",
    "table_caption": "Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model Mrksic et al. (2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ Budzianowski et al. (2018).",
    "table_column_names": [
      "[BOLD] DST Models",
      "[BOLD] Joint Acc. WoZ 2.0",
      "[BOLD] Joint Acc. MultiWoZ",
      "[BOLD] ITC"
    ],
    "table_content_values": [
      [
        "Baselines Mrksic et al. ( 2017 )",
        "70.8%",
        "25.83%",
        "[ITALIC] O( [ITALIC] mn)"
      ],
      [
        "NBT-CNN Mrksic et al. ( 2017 )",
        "84.2%",
        "-",
        "[ITALIC] O( [ITALIC] mn)"
      ],
      [
        "StateNet_PSI Ren et al. ( 2018 )",
        "[BOLD] 88.9%",
        "-",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "GLAD Nouri and Hosseini-Asl ( 2018 )",
        "88.5%",
        "35.58%",
        "[ITALIC] O( [ITALIC] mn)"
      ],
      [
        "HyST (ensemble) Goel et al. ( 2019 )",
        "-",
        "44.22%",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "DSTRead (ensemble) Gao et al. ( 2019 )",
        "-",
        "42.12%",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "TRADE Wu et al. ( 2019 )",
        "-",
        "48.62%",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "COMER",
        "88.6%",
        "[BOLD] 48.79%",
        "[ITALIC] O(1)"
      ]
    ],
    "id": "8c45d451-b086-4e3f-a4ec-3f686a647811",
    "claim": "On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which is lower than the previous state-of-the-art.",
    "label": "refutes",
    "table_id": "d5d1590e-42a1-4452-9cc7-1bce08b243b7"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
    "table_column_names": [
      "[BOLD] Language pair",
      "[BOLD] Model type",
      "[BOLD] Oracle model",
      "[BOLD] Decoder configuration  [BOLD] Uniform",
      "[BOLD] Decoder configuration  [BOLD] BI + IS"
    ],
    "table_content_values": [
      [
        "es-en",
        "Unadapted",
        "36.4",
        "34.7",
        "36.6"
      ],
      [
        "es-en",
        "No-reg",
        "36.6",
        "34.8",
        "-"
      ],
      [
        "es-en",
        "EWC",
        "37.0",
        "36.3",
        "[BOLD] 37.2"
      ],
      [
        "en-de",
        "Unadapted",
        "36.4",
        "26.8",
        "38.8"
      ],
      [
        "en-de",
        "No-reg",
        "41.7",
        "31.8",
        "-"
      ],
      [
        "en-de",
        "EWC",
        "42.1",
        "38.6",
        "[BOLD] 42.0"
      ]
    ],
    "id": "690d5d5d-b552-4cdf-b858-ccefaf6ddfbc",
    "claim": "BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU gain over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU gain over the approach described in Freitag and Al-Onaizan (2016).",
    "label": "supports",
    "table_id": "a02493f5-adad-4462-a582-8a2cfe6f431d"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.818",
        "0.719",
        "37.3",
        "10.0"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.819",
        "0.734",
        "26.3",
        "14.2"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.813",
        "0.770",
        "36.4",
        "18.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.807",
        "0.796",
        "28.4",
        "21.5"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.798",
        "0.783",
        "39.7",
        "19.2"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.804",
        "0.785",
        "27.1",
        "20.3"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.805",
        "[BOLD] 0.817",
        "43.3",
        "21.6"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.818",
        "0.805",
        "[BOLD] 29.0",
        "[BOLD] 22.8"
      ]
    ],
    "id": "ce07043c-e05f-4ac4-bf96-7de4531fb8f8",
    "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation.",
    "label": "refutes",
    "table_id": "ce976181-466e-4d2c-a980-797bf966424e"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "17171f81-9831-4b32-bfa9-35bd2481a747",
    "claim": "When the model focuses on “nele” and “type”, it learns the semantic meaning of them, thus enabling the prediction of triples that have “nele” and “type”",
    "label": "not enough info",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "8667413d-d331-4b2e-bae7-01f3a106b373",
    "claim": "As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods, except for the Portuguese Europarl corpus, where DocSub had the highest value.",
    "label": "refutes",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "<bold>Baselines</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>)",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "<bold>Model Variants</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "<bold>79.5</bold>"
      ]
    ],
    "id": "447eeb5c-f007-4096-b630-ce70255d1c14",
    "claim": "[CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model.",
    "label": "supports",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "b920ceff-a2e2-4cc1-877c-73c2b3404336",
    "claim": "Also, the average human rating for Refresh is not significantly higher (p (cid:28) 0.01) than ExtAbsRL.",
    "label": "refutes",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "705d2c1b-ce84-4754-b506-76987159316d",
    "claim": "This is because word representation learning approaches, like OIWE-IPG and SOV, do not consider the semantic distance learning issue, which has a significant impact on word similarity task.",
    "label": "not enough info",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.818",
        "0.719",
        "37.3",
        "10.0"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.819",
        "0.734",
        "26.3",
        "14.2"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.813",
        "0.770",
        "36.4",
        "18.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.807",
        "0.796",
        "28.4",
        "21.5"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.798",
        "0.783",
        "39.7",
        "19.2"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.804",
        "0.785",
        "27.1",
        "20.3"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.805",
        "[BOLD] 0.817",
        "43.3",
        "21.6"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.818",
        "0.805",
        "[BOLD] 29.0",
        "[BOLD] 22.8"
      ]
    ],
    "id": "80fcba83-634a-4705-a314-22a36d228ec5",
    "claim": "For Yelp, M1 has better Acc and PP than M0 at comparable semantic similarity.",
    "label": "supports",
    "table_id": "ce976181-466e-4d2c-a980-797bf966424e"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
    "table_column_names": [
      "ID LSTM-800",
      "5-fold CV 70.56",
      "Δ 0.66",
      "Single model 67.54",
      "Δ 0.78",
      "Ensemble 67.65",
      "Δ 0.30"
    ],
    "table_content_values": [
      [
        "LSTM-400",
        "70.50",
        "0.60",
        "[BOLD] 67.59",
        "0.83",
        "[BOLD] 68.00",
        "0.65"
      ],
      [
        "IN-TITLE",
        "70.11",
        "0.21",
        "[EMPTY]",
        "[EMPTY]",
        "67.52",
        "0.17"
      ],
      [
        "[BOLD] SUBMISSION",
        "69.90",
        "–",
        "66.76",
        "–",
        "67.35",
        "–"
      ],
      [
        "NO-HIGHWAY",
        "69.72",
        "−0.18",
        "66.42",
        "−0.34",
        "66.64",
        "−0.71"
      ],
      [
        "NO-OVERLAPS",
        "69.46",
        "−0.44",
        "65.07",
        "−1.69",
        "66.47",
        "−0.88"
      ],
      [
        "LSTM-400-DROPOUT",
        "69.45",
        "−0.45",
        "65.53",
        "−1.23",
        "67.28",
        "−0.07"
      ],
      [
        "NO-TRANSLATIONS",
        "69.42",
        "−0.48",
        "65.92",
        "−0.84",
        "67.23",
        "−0.12"
      ],
      [
        "NO-ELMO-FINETUNING",
        "67.71",
        "−2.19",
        "65.16",
        "−1.60",
        "65.42",
        "−1.93"
      ]
    ],
    "id": "5ccbb007-877f-4e17-8513-9731641554db",
    "claim": "[CONTINUE] However, our data augmentation technique (NO-TRANSLATIONS) had a significant impact on the final score, reducing it by 0.84 points.",
    "label": "refutes",
    "table_id": "0980bab3-642c-413e-8b15-6abd868956a8"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "83c12d32-c4f9-43d4-bfeb-ca7dd1f991a0",
    "claim": "both (Nguyen et al., 2016) and ours pre-compute a vocabulary of top-K possible responses, which are used as the only acceptable responses.",
    "label": "not enough info",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "7a281d71-ec5a-4086-bb9e-b1a04976b77b",
    "claim": "The full model gives 25.5 BLEU points on the AMR15 dev set.",
    "label": "supports",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "b87e736b-b577-4883-9cd3-271efb940ee7",
    "claim": "The semantic threshold for OD-d2v is set at 0.3 while for OD-w2v is set at 0.6.",
    "label": "supports",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "5a893e75-277d-48d6-abe4-2d1757a04b02",
    "claim": "This suggests that graph encoders based on gating mechanisms are not as effective as other models in text generation models.",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] Full UAS",
      "[BOLD] PPA Acc."
    ],
    "table_content_values": [
      [
        "RBG",
        "94.17",
        "88.51"
      ],
      [
        "RBG + HPCD (full)",
        "94.19",
        "89.59"
      ],
      [
        "RBG + LSTM-PP",
        "94.14",
        "86.35"
      ],
      [
        "RBG + OntoLSTM-PP",
        "94.30",
        "90.11"
      ],
      [
        "RBG + Oracle PP",
        "94.60",
        "98.97"
      ]
    ],
    "id": "bb0c415e-fa38-4ace-aa92-8f8480f1b2ab",
    "claim": "However, when gold PP attachment are used, we note a large potential improve [CONTINUE] ment of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach.",
    "label": "supports",
    "table_id": "e727bc8f-c0e1-4e2b-a7cf-cc55c64f45d4"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.",
    "table_column_names": [
      "Dataset",
      "Accuracy",
      "Fleiss’ kappa  [ITALIC] k"
    ],
    "table_content_values": [
      [
        "Original COPA",
        "100.0",
        "0.973"
      ],
      [
        "Balanced COPA",
        "97.0",
        "0.798"
      ]
    ],
    "id": "05fde2b1-5561-41b9-b324-49eb1967cf32",
    "claim": "The human evaluation shows that our mirrored instances are not as difficult as the original ones (see Table 3).",
    "label": "refutes",
    "table_id": "58160fa7-ce6a-4ff8-805b-43776f982ae5"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "4bc3daec-347b-482d-9818-20213bdeba76",
    "claim": "However, the drop in performance on the QA-SRL task, from which the model's weights are initialized, is much smaller with BIDAF (ELMO) than MQAN, and this corroborates the idea that contextualized ELMo representations, which benefit from general pre-training and are transferred to the task using the fine-tuning paradigm, are more amenable for achieving cross-dataset generalization.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "0fdec0b4-dd5c-40da-b23e-43bf04c01a24",
    "claim": "The PRKGC model gives considerably good results, which indicates the non-triviality of RC-QEDE.",
    "label": "refutes",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "cb23a665-93d6-4923-befd-01b39b00e674",
    "claim": "We hypothesize that the gating mechanism cannot better capture long-distance dependencies between nodes far apart in the graph.",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "6241fc50-d27d-46d7-9be1-a96d6b384e0e",
    "claim": "When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under BLEU (around 3 points) is greater than that under Meteor (around 5 points).",
    "label": "refutes",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "[BOLD] 71.2"
      ]
    ],
    "id": "f5755c32-6e1b-4528-93b2-e775520cd8c0",
    "claim": "Without knowledge of the input systems, the score of MUC-B1, which most closely follows the MUC scoring methodology (Vilain et al., 1995), was higher than MUC-B1.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "d170dc40-b5d5-403a-8e9e-c3be389759ad",
    "claim": "At the same time, the distributional information embedded into the network appears to have acted as a stabilizing force.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "b3fc9434-7e2f-467b-823c-6e91b5ce5db4",
    "claim": "We suspect that two reasons for the performance drop on booking hotels are 1) the vocabularies of booking hotels are more similar to that of others than of booking flights or restaurants, making it easier to say keywords unintentionally and 2) users tend to vary more in asking details on booking a hotel,",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "4ac59499-db43-4949-aa71-72b2deaffc8a",
    "claim": "The gap has become larger when the threshold becomes smaller, since there is much more noises when the score becomes smaller, our capsule net and word-level attention models are more robust to these noises.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).",
    "table_column_names": [
      "[EMPTY]",
      "dev CS",
      "dev mono",
      "test CS",
      "test mono"
    ],
    "table_content_values": [
      [
        "CS-only-LM",
        "45.20",
        "65.87",
        "43.20",
        "62.80"
      ],
      [
        "Fine-Tuned-LM",
        "49.60",
        "72.67",
        "47.60",
        "71.33"
      ],
      [
        "CS-only-disc",
        "[BOLD] 75.60",
        "70.40",
        "70.80",
        "70.53"
      ],
      [
        "Fine-Tuned-disc",
        "70.80",
        "[BOLD] 74.40",
        "[BOLD] 75.33",
        "[BOLD] 75.87"
      ]
    ],
    "id": "e042f4df-12c4-467c-95bd-5043fd5e178e",
    "claim": "The CS-ONLY-DISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions.",
    "label": "refutes",
    "table_id": "1e599458-4bd1-4761-9fc4-8743eb9e544b"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "4180aa48-324e-4fc1-89e9-f95ee5717491",
    "claim": "For example, we take the triple (nele, type, nele).",
    "label": "not enough info",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "6532fb08-f821-4080-912e-391b0e279557",
    "claim": "Increasing the window size to 10 increases the F1 score marginally (A3−A4).",
    "label": "refutes",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "Aligning Vector-spaces with Noisy Supervised Lexicons",
    "paper_id": "1903.10238v1",
    "table_caption": "Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
    "table_column_names": [
      "Method",
      "En→It best",
      "En→It avg",
      "En→It iters",
      "En→De best",
      "En→De avg",
      "En→De iters",
      "En→Fi best",
      "En→Fi avg",
      "En→Fi iters",
      "En→Es best",
      "En→Es avg",
      "En→Es iters"
    ],
    "table_content_values": [
      [
        "Artetxe et al., 2018b",
        "[BOLD] 48.53",
        "48.13",
        "573",
        "48.47",
        "48.19",
        "773",
        "33.50",
        "32.63",
        "988",
        "37.60",
        "37.33",
        "808"
      ],
      [
        "Noise-aware Alignment",
        "[BOLD] 48.53",
        "[BOLD] 48.20",
        "471",
        "[BOLD] 49.67",
        "[BOLD] 48.89",
        "568",
        "[BOLD] 33.98",
        "[BOLD] 33.68",
        "502",
        "[BOLD] 38.40",
        "[BOLD] 37.79",
        "551"
      ]
    ],
    "id": "ef19ef2f-f972-4231-b2ce-9604c2f0de42",
    "claim": "In contrast, the noise-aware model requires more iterations to converge.",
    "label": "refutes",
    "table_id": "279e3d12-df99-48f8-83ea-d3e42b8bbfcc"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
    "table_column_names": [
      "Corpus",
      "Metric",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "Europarl",
        "TotalTerms:",
        "957",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "836",
        "1,000"
      ],
      [
        "Europarl",
        "TotalRoots:",
        "44",
        "1",
        "1",
        "1",
        "1",
        "43",
        "1"
      ],
      [
        "Europarl",
        "NumberRels:",
        "1,588",
        "1,025",
        "1,028",
        "1,185",
        "1,103",
        "1,184",
        "999"
      ],
      [
        "Europarl",
        "MaxDepth:",
        "21",
        "921",
        "901",
        "788",
        "835",
        "8",
        "15"
      ],
      [
        "Europarl",
        "MinDepth:",
        "1",
        "921",
        "901",
        "788",
        "835",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgDepth:",
        "11.82",
        "921",
        "901",
        "788",
        "835",
        "3.05",
        "8.46"
      ],
      [
        "Europarl",
        "DepthCohesion:",
        "1.78",
        "1",
        "1",
        "1",
        "1",
        "2.62",
        "1.77"
      ],
      [
        "Europarl",
        "MaxWidth:",
        "20",
        "2",
        "3",
        "4",
        "3",
        "88",
        "41"
      ],
      [
        "Europarl",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgWidth:",
        "1.99",
        "1.03",
        "1.03",
        "1.19",
        "1.10",
        "4.20",
        "2.38"
      ],
      [
        "TED Talks",
        "TotalTerms:",
        "476",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000"
      ],
      [
        "TED Talks",
        "TotalRoots:",
        "164",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "NumberRels:",
        "521",
        "1,029",
        "1,331",
        "3,025",
        "3,438",
        "3,802",
        "1,009"
      ],
      [
        "TED Talks",
        "MaxDepth:",
        "16",
        "915",
        "658",
        "454",
        "395",
        "118",
        "12"
      ],
      [
        "TED Talks",
        "MinDepth:",
        "1",
        "913",
        "658",
        "454",
        "395",
        "110",
        "1"
      ],
      [
        "TED Talks",
        "AvgDepth:",
        "5.82",
        "914",
        "658",
        "454",
        "395",
        "112.24",
        "5.95"
      ],
      [
        "TED Talks",
        "DepthCohesion:",
        "2.75",
        "1",
        "1",
        "1",
        "1",
        "1.05",
        "2.02"
      ],
      [
        "TED Talks",
        "MaxWidth:",
        "25",
        "2",
        "77",
        "13",
        "12",
        "66",
        "98"
      ],
      [
        "TED Talks",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "AvgWidth:",
        "1.83",
        "1.03",
        "1.36",
        "3.03",
        "3.44",
        "6.64",
        "2.35"
      ]
    ],
    "id": "689f8a9c-3097-4448-b010-e9413fcaeabc",
    "claim": "[CONTINUE] For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 789 terms with different values of term frequency, while having 211 that share the same value of term frequency with other terms.",
    "label": "supports",
    "table_id": "7fd32b5d-95c3-40d7-8e3d-16391cad7f14"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "f8dc8a50-467a-4ac0-8ec0-796d2151a87d",
    "claim": "[CONTINUE] For both datasets, our approach substantially outperforms the baselines.",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc∗: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.",
    "table_column_names": [
      "Model",
      "BLEU",
      "Acc∗"
    ],
    "table_content_values": [
      [
        "fu-1",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Multi-decoder",
        "7.6",
        "0.792"
      ],
      [
        "Style embed.",
        "15.4",
        "0.095"
      ],
      [
        "simple-transfer",
        "simple-transfer",
        "simple-transfer"
      ],
      [
        "Template",
        "18.0",
        "0.867"
      ],
      [
        "Delete/Retrieve",
        "12.6",
        "0.909"
      ],
      [
        "yang2018unsupervised",
        "yang2018unsupervised",
        "yang2018unsupervised"
      ],
      [
        "LM",
        "13.4",
        "0.854"
      ],
      [
        "LM + classifier",
        "[BOLD] 22.3",
        "0.900"
      ],
      [
        "Untransferred",
        "[BOLD] 31.4",
        "0.024"
      ]
    ],
    "id": "ae9d1040-1c3b-4d4b-8743-4b74fea43a96",
    "claim": "We additionally find that supervised BLEU shows a trade-off with Acc: for a single model type, higher Acc generally corresponds to lower BLEU.",
    "label": "supports",
    "table_id": "a78a417b-c642-4ade-8df2-6998a00348c2"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "dd0fd46f-faae-4f40-bace-d38e01bbf8de",
    "claim": "RoBERTa, due to its optimizations and higher training data, outperforms the other models by a significant margin, indicating the large potential for models trained on much larger data",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "6c15ac43-fcb9-4598-a50f-607a89c8074f",
    "claim": "Overall results show that ATR achieves the best performance and consumes the least training time.",
    "label": "refutes",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "4726d56c-a183-45f6-ada9-df0b103a0e3e",
    "claim": "Therefore, our method covers most contexts where “to” is an",
    "label": "not enough info",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "4534055e-da0f-4973-ac32-54f5b588cf48",
    "claim": "GDPL achieves extremely high performance in the task success on account of the substantial improvement in inform F1 and match rate over the baselines.",
    "label": "supports",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "a7929669-2be1-4529-b9c0-91a29db43a73",
    "claim": "2018b; Dong et\\xa0al.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "17.3",
        "0.828"
      ],
      [
        "Wiener filter",
        "19.5",
        "0.722"
      ],
      [
        "Minimizing DCE",
        "15.8",
        "[BOLD] 0.269"
      ],
      [
        "FSEGAN",
        "14.9",
        "0.291"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "15.6",
        "0.330"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 14.4",
        "0.303"
      ],
      [
        "Clean speech",
        "5.7",
        "0.0"
      ]
    ],
    "id": "32381fb3-5662-4d51-bf38-51a4d3b1023b",
    "claim": "[CONTINUE] In Librispeech + DEMAND, minimizing DCE (15.8%) and FSEGAN (14.9%) achieves a lower WER than acoustic supervision (15.6%) and multi-task learning (14.4%).",
    "label": "refutes",
    "table_id": "3c6c1f56-ed0b-418a-ac6c-6449488a89e9"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
    "table_column_names": [
      "[EMPTY]",
      "WN-N P",
      "WN-N R",
      "WN-N F",
      "WN-V P",
      "WN-V R",
      "WN-V F",
      "VN P",
      "VN R",
      "VN F"
    ],
    "table_content_values": [
      [
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2"
      ],
      [
        "type",
        ".700",
        ".654",
        ".676",
        ".535",
        ".474",
        ".503",
        ".327",
        ".309",
        ".318"
      ],
      [
        "x+POS",
        ".699",
        ".651",
        ".674",
        ".544",
        ".472",
        ".505",
        ".339",
        ".312",
        ".325"
      ],
      [
        "lemma",
        ".706",
        ".660",
        ".682",
        ".576",
        ".520",
        ".547",
        ".384",
        ".360",
        ".371"
      ],
      [
        "x+POS",
        "<bold>.710</bold>",
        "<bold>.662</bold>",
        "<bold>.685</bold>",
        "<bold>.589</bold>",
        "<bold>.529</bold>",
        "<bold>.557</bold>",
        "<bold>.410</bold>",
        "<bold>.389</bold>",
        "<bold>.399</bold>"
      ],
      [
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep"
      ],
      [
        "type",
        ".712",
        ".661",
        ".686",
        ".545",
        ".457",
        ".497",
        ".324",
        ".296",
        ".310"
      ],
      [
        "x+POS",
        ".715",
        ".659",
        ".686",
        ".560",
        ".464",
        ".508",
        ".349",
        ".320",
        ".334"
      ],
      [
        "lemma",
        "<bold>.725</bold>",
        "<bold>.668</bold>",
        "<bold>.696</bold>",
        ".591",
        ".512",
        ".548",
        ".408",
        ".371",
        ".388"
      ],
      [
        "x+POS",
        ".722",
        ".666",
        ".693",
        "<bold>.609</bold>",
        "<bold>.527</bold>",
        "<bold>.565</bold>",
        "<bold>.412</bold>",
        "<bold>.381</bold>",
        "<bold>.396</bold>"
      ]
    ],
    "id": "bfa7b13c-5cb0-4467-ae28-9a92c6efd558",
    "claim": "For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p ≤ .005) with borderline significance for WN-N and WN-V (p ≈ .05).",
    "label": "supports",
    "table_id": "051fe422-03d4-44d5-836c-7f982b328555"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Original",
      "Italian Debiased",
      "Italian English",
      "Italian Reduction",
      "German Original",
      "German Debiased",
      "German English",
      "German Reduction"
    ],
    "table_content_values": [
      [
        "Same Gender",
        "0.442",
        "0.434",
        "0.424",
        "–",
        "0.491",
        "0.478",
        "0.446",
        "–"
      ],
      [
        "Different Gender",
        "0.385",
        "0.421",
        "0.415",
        "–",
        "0.415",
        "0.435",
        "0.403",
        "–"
      ],
      [
        "difference",
        "0.057",
        "0.013",
        "0.009",
        "[BOLD] 91.67%",
        "0.076",
        "0.043",
        "0.043",
        "[BOLD] 100%"
      ]
    ],
    "id": "c8ffde58-4d95-40e3-aa21-fbdbe463b9dd",
    "claim": "In Italian, we get an increase of 91.67% of the gap with respect to English.",
    "label": "refutes",
    "table_id": "a3f7a8f4-e69f-4e32-beb4-281137d9a4a5"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
    "table_column_names": [
      "Metric",
      "Method of validation",
      "Yelp",
      "Lit."
    ],
    "table_content_values": [
      [
        "Acc",
        "% of machine and human judgments that match",
        "94",
        "84"
      ],
      [
        "Sim",
        "Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation",
        "0.79",
        "0.75"
      ],
      [
        "PP",
        "Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency",
        "0.81",
        "0.67"
      ]
    ],
    "id": "9846b931-84f9-407f-9a32-b37c96b7c9f1",
    "claim": "[CONTINUE] To validate Acc, human annotators were asked to judge the style of 100 transferred sentences [CONTINUE] We then compute the percentage of machine and human judgments that match.",
    "label": "supports",
    "table_id": "11046f41-73be-47e9-9f30-8fe50765c22d"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "e8f0310f-c946-4b95-88b3-d257d8ea56f7",
    "claim": "Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively.",
    "label": "supports",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "b9c0aac4-ab15-4aa4-b7a2-2782a675158e",
    "claim": "HDSA shows the effectiveness of explicitly capturing intent and dialog history.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "a84bc7ed-7512-4465-8dc1-256e680d2065",
    "claim": "However, the greatest performance increase is seen for the last scenario, which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived.",
    "label": "supports",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "401dfb4f-8750-4c42-bc8d-b0d5942f798f",
    "claim": "Overall results show that LRN achieves competitive performance but consumes the least training time.",
    "label": "supports",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "f7a0d205-c70c-45a4-ad95-ee004bb48b14",
    "claim": "Interestingly, G2S-GGNN has better performance among our models.",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "64535f27-4dd7-4691-827f-a7633eb8c941",
    "claim": "with respect to  the efficiency criteria, in which task the dialog systems take shorter time to reach the successful termination in an average and the total dialog time is shorter when averaged across all dialog sessions, the trend shows that all dialog methods have a strong tendency to increase dialog latency with time.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "4c929b50-4a25-4aa9-a703-75aa75de0325",
    "claim": "The mechanism successfully alleviates the over-fitting issue caused by the imbalanced two tasks’ sizes.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "c0e96242-c3ea-48c3-a932-693d83be5c5c",
    "claim": "[CONTINUE] Surprisingly, GDPL even outperforms human in completing the task, and its average dialog turns are close to those of humans, though GDPL is inferior in terms of match rate.",
    "label": "supports",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "922444a0-578f-4b72-a5f4-e457f6e26693",
    "claim": "HAN models outperform both LogReg and SVM using the current set of features.",
    "label": "supports",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
    "paper_id": "1909.02622v2",
    "table_caption": "Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.",
    "table_column_names": [
      "Setting",
      "Metrics",
      "<bold>Direct Assessment</bold> cs-en",
      "<bold>Direct Assessment</bold> de-en",
      "<bold>Direct Assessment</bold> fi-en",
      "<bold>Direct Assessment</bold> lv-en",
      "<bold>Direct Assessment</bold> ru-en",
      "<bold>Direct Assessment</bold> tr-en",
      "<bold>Direct Assessment</bold> zh-en",
      "<bold>Direct Assessment</bold> Average"
    ],
    "table_content_values": [
      [
        "Baselines",
        "METEOR++",
        "0.552",
        "0.538",
        "0.720",
        "0.563",
        "0.627",
        "0.626",
        "0.646",
        "0.610"
      ],
      [
        "Baselines",
        "RUSE(*)",
        "0.624",
        "0.644",
        "0.750",
        "0.697",
        "0.673",
        "0.716",
        "0.691",
        "0.685"
      ],
      [
        "Baselines",
        "BERTScore-F1",
        "0.670",
        "0.686",
        "0.820",
        "0.710",
        "0.729",
        "0.714",
        "0.704",
        "0.719"
      ],
      [
        "Sent-Mover",
        "Smd + W2V",
        "0.438",
        "0.505",
        "0.540",
        "0.442",
        "0.514",
        "0.456",
        "0.494",
        "0.484"
      ],
      [
        "Sent-Mover",
        "Smd + ELMO + PMeans",
        "0.569",
        "0.558",
        "0.732",
        "0.525",
        "0.581",
        "0.620",
        "0.584",
        "0.595"
      ],
      [
        "Sent-Mover",
        "Smd + BERT + PMeans",
        "0.607",
        "0.623",
        "0.770",
        "0.639",
        "0.667",
        "0.641",
        "0.619",
        "0.652"
      ],
      [
        "Sent-Mover",
        "Smd + BERT + MNLI + PMeans",
        "0.616",
        "0.643",
        "0.785",
        "0.660",
        "0.664",
        "0.668",
        "0.633",
        "0.667"
      ],
      [
        "Word-Mover",
        "Wmd-1 + W2V",
        "0.392",
        "0.463",
        "0.558",
        "0.463",
        "0.456",
        "0.485",
        "0.481",
        "0.471"
      ],
      [
        "Word-Mover",
        "Wmd-1 + ELMO + PMeans",
        "0.579",
        "0.588",
        "0.753",
        "0.559",
        "0.617",
        "0.679",
        "0.645",
        "0.631"
      ],
      [
        "Word-Mover",
        "Wmd-1 + BERT + PMeans",
        "0.662",
        "0.687",
        "0.823",
        "0.714",
        "0.735",
        "0.734",
        "0.719",
        "0.725"
      ],
      [
        "Word-Mover",
        "Wmd-1 + BERT + MNLI + PMeans",
        "0.670",
        "0.708",
        "<bold>0.835</bold>",
        "<bold>0.746</bold>",
        "<bold>0.738</bold>",
        "0.762",
        "<bold>0.744</bold>",
        "<bold>0.743</bold>"
      ],
      [
        "Word-Mover",
        "Wmd-2 + BERT + MNLI + PMeans",
        "<bold>0.679</bold>",
        "<bold>0.710</bold>",
        "0.832",
        "0.745",
        "0.736",
        "<bold>0.763</bold>",
        "0.740",
        "<bold>0.743</bold>"
      ]
    ],
    "id": "11b3b856-eb9c-4249-a21b-9c084802ad70",
    "claim": "Table 1: In all language pairs, the best correlation is not achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans.",
    "label": "refutes",
    "table_id": "5d8fa2ea-9a19-48f7-b577-698a1cb0cbc6"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "b8a62c5c-3087-4351-a5fd-9c2882dcdd08",
    "claim": "Our proposed method does not outperform GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set.",
    "label": "refutes",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "70c37968-9955-4a49-83f6-a44b58ca002c",
    "claim": "Also in cross-document coreference, it achieves the best joint results, except for the CEAF metric.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
    "table_column_names": [
      "[BOLD] Decoder configuration",
      "[BOLD] es-en  [BOLD] Health",
      "[BOLD] es-en  [BOLD] Bio",
      "[BOLD] en-de  [BOLD] News",
      "[BOLD] en-de  [BOLD] TED",
      "[BOLD] en-de  [BOLD] IT"
    ],
    "table_content_values": [
      [
        "Oracle model",
        "35.9",
        "36.1",
        "37.8",
        "24.1",
        "39.6"
      ],
      [
        "Uniform",
        "33.1",
        "36.4",
        "21.9",
        "18.4",
        "38.9"
      ],
      [
        "Identity-BI",
        "35.0",
        "36.6",
        "32.7",
        "25.3",
        "42.6"
      ],
      [
        "BI",
        "35.9",
        "36.5",
        "38.0",
        "26.1",
        "[BOLD] 44.7"
      ],
      [
        "IS",
        "[BOLD] 36.0",
        "36.8",
        "37.5",
        "25.6",
        "43.3"
      ],
      [
        "BI + IS",
        "[BOLD] 36.0",
        "[BOLD] 36.9",
        "[BOLD] 38.4",
        "[BOLD] 26.4",
        "[BOLD] 44.7"
      ]
    ],
    "id": "6d7e29b8-084e-4ad5-8c41-e8975a7ddf6b",
    "claim": "BI and IS both individually outperform the oracle for all domains, [CONTINUE] With adaptive decoding, we can assume that a uniform ensemble will always perform better than a single model for any potentially unknown domain.",
    "label": "refutes",
    "table_id": "f27c1026-3ef4-450e-97a8-368ddf0d0848"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "3d29f63f-a0e4-422c-8299-a1e9ebd041b4",
    "claim": "The results show that coverage information does not improve the generalization of both examined models across various NLI datasets.",
    "label": "refutes",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
    "table_column_names": [
      "[BOLD] Emoji alias",
      "[BOLD] N",
      "[BOLD] emoji #",
      "[BOLD] emoji %",
      "[BOLD] no-emoji #",
      "[BOLD] no-emoji %",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "mask",
        "163",
        "154",
        "94.48",
        "134",
        "82.21",
        "- 12.27"
      ],
      [
        "two_hearts",
        "87",
        "81",
        "93.10",
        "77",
        "88.51",
        "- 4.59"
      ],
      [
        "heart_eyes",
        "122",
        "109",
        "89.34",
        "103",
        "84.43",
        "- 4.91"
      ],
      [
        "heart",
        "267",
        "237",
        "88.76",
        "235",
        "88.01",
        "- 0.75"
      ],
      [
        "rage",
        "92",
        "78",
        "84.78",
        "66",
        "71.74",
        "- 13.04"
      ],
      [
        "cry",
        "116",
        "97",
        "83.62",
        "83",
        "71.55",
        "- 12.07"
      ],
      [
        "sob",
        "490",
        "363",
        "74.08",
        "345",
        "70.41",
        "- 3.67"
      ],
      [
        "unamused",
        "167",
        "121",
        "72.46",
        "116",
        "69.46",
        "- 3.00"
      ],
      [
        "weary",
        "204",
        "140",
        "68.63",
        "139",
        "68.14",
        "- 0.49"
      ],
      [
        "joy",
        "978",
        "649",
        "66.36",
        "629",
        "64.31",
        "- 2.05"
      ],
      [
        "sweat_smile",
        "111",
        "73",
        "65.77",
        "75",
        "67.57",
        "1.80"
      ],
      [
        "confused",
        "77",
        "46",
        "59.74",
        "48",
        "62.34",
        "2.60"
      ]
    ],
    "id": "bcd7aad2-e4b4-4978-ad9e-8aee69123d1c",
    "claim": "[CONTINUE] Further, contrary to intuition, the sob emoji contributes less than cry, despite representing a stronger emotion.",
    "label": "supports",
    "table_id": "2410b1c3-4d48-49e9-9279-dc23daa879d1"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "eab2c281-14b3-43ac-b1db-b83a227a2122",
    "claim": "Our text classifiers for identifying negation cues and finding the negation scope are built on top of a BERT classifier",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "63d505e9-bb71-4a79-a21f-837a2d8402d8",
    "claim": "we see that in most cases fine-tuning on B-COPA does not help the models’ performance, only their robustness.",
    "label": "not enough info",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "b8dd43d6-532b-40e2-8758-5c8e8fd69a02",
    "claim": "However, CMOW generally outperforms CBOW embeddings.",
    "label": "refutes",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "89a48433-b8ad-489b-a96c-267e8f760ad5",
    "claim": "The results in the table suggest that cleaning the missing slots did not provide more complex training examples.",
    "label": "refutes",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 1: Benchmark performance, Spearman’s ρ. SGNS results with * taken from [morphfit]. Best results per column (benchmark) annotated for our setup only.",
    "table_column_names": [
      "Context: w2",
      "Context: w2 SimLex",
      "Context: w2 SimLex",
      "Context: w2 SimLex",
      "Context: w2 SimLex",
      "Context: w2 SimVerb"
    ],
    "table_content_values": [
      [
        "target",
        "N",
        "V",
        "A",
        "all",
        "V"
      ],
      [
        "type",
        ".334",
        "<bold>.336</bold>",
        "<bold>.518</bold>",
        ".348",
        ".307"
      ],
      [
        "x + POS",
        ".342",
        ".323",
        ".513",
        ".350",
        ".279"
      ],
      [
        "lemma",
        "<bold>.362</bold>",
        ".333",
        ".497",
        "<bold>.351</bold>",
        ".400"
      ],
      [
        "x + POS",
        ".354",
        "<bold>.336</bold>",
        ".504",
        ".345",
        "<bold>.406</bold>"
      ],
      [
        "* type",
        "-",
        "-",
        "-",
        ".339",
        ".277"
      ],
      [
        "* type MFit-A",
        "-",
        "-",
        "-",
        ".385",
        "-"
      ],
      [
        "* type MFit-AR",
        "-",
        "-",
        "-",
        ".439",
        ".381"
      ],
      [
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W"
      ],
      [
        "type",
        ".366",
        ".365",
        ".489",
        ".362",
        ".314"
      ],
      [
        "x + POS",
        ".364",
        ".351",
        ".482",
        ".359",
        ".287"
      ],
      [
        "lemma",
        "<bold>.391</bold>",
        ".380",
        "<bold>.522</bold>",
        "<bold>.379</bold>",
        ".401"
      ],
      [
        "x + POS",
        ".384",
        "<bold>.388</bold>",
        ".480",
        ".366",
        "<bold>.431</bold>"
      ],
      [
        "* type",
        "-",
        "-",
        "-",
        ".376",
        ".313"
      ],
      [
        "* type MFit-AR",
        "-",
        "-",
        "-",
        ".434",
        ".418"
      ]
    ],
    "id": "e2df8ba1-ef82-4c6d-9407-a8cd9c22a003",
    "claim": "[CONTINUE] Lemmatized targets generally perform better, with the boost being more pronounced on SimVerb.",
    "label": "supports",
    "table_id": "c39ac67a-58a6-454e-be26-4c7a37808036"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "ed79ce87-cdaa-4117-94e3-e9003fdbdb66",
    "claim": "When redundancy removal was applied to LogReg, it produces significant improvement.",
    "label": "refutes",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection",
    "paper_id": "1904.04388v1",
    "table_caption": "Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Model",
      "[BOLD] dev mean",
      "[BOLD] dev best",
      "[BOLD] test mean",
      "[BOLD] test best",
      "[ITALIC] α"
    ],
    "table_content_values": [
      [
        "single",
        "text",
        "86.54",
        "86.80",
        "86.47",
        "86.96",
        "–"
      ],
      [
        "single",
        "raw",
        "35.00",
        "37.33",
        "35.78",
        "37.70",
        "–"
      ],
      [
        "single",
        "innovations",
        "80.86",
        "81.51",
        "80.28",
        "82.15",
        "–"
      ],
      [
        "early",
        "text + raw",
        "86.46",
        "86.65",
        "86.24",
        "86.53",
        "–"
      ],
      [
        "early",
        "text + innovations",
        "86.53",
        "86.77",
        "86.54",
        "87.00",
        "–"
      ],
      [
        "early",
        "text + raw + innovations",
        "86.35",
        "86.69",
        "86.55",
        "86.44",
        "–"
      ],
      [
        "late",
        "text + raw",
        "86.71",
        "87.05",
        "86.35",
        "86.71",
        "0.2"
      ],
      [
        "late",
        "text + innovations",
        "[BOLD] 86.98",
        "[BOLD] 87.48",
        "[BOLD] 86.68",
        "[BOLD] 87.02",
        "0.5"
      ],
      [
        "late",
        "text + raw + innovations",
        "86.95",
        "87.30",
        "86.60",
        "86.87",
        "0.5"
      ]
    ],
    "id": "4e7fe7a0-0c7a-4e48-b4e9-0aaa1d9a2c28",
    "claim": "We found that innovations are not helpful in both early and late fusion frameworks, and late fusion does not perform better on average.",
    "label": "refutes",
    "table_id": "f0441bd9-731a-41df-8a83-1d084b332e89"
  },
  {
    "paper": "Evaluation of Greek Word Embeddings",
    "paper_id": "1904.04032v3",
    "table_caption": "Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
    "table_column_names": [
      "Category Semantic",
      "Category no oov words",
      "gr_def 58.42%",
      "gr_neg10 59.33%",
      "cc.el.300  [BOLD] 68.80%",
      "wiki.el 27.20%",
      "gr_cbow_def 31.76%",
      "gr_d300_nosub 60.79%",
      "gr_w2v_sg_n5 52.70%"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "with oov words",
        "52.97%",
        "55.33%",
        "[BOLD] 64.34%",
        "25.73%",
        "28.80%",
        "55.11%",
        "47.82%"
      ],
      [
        "Syntactic",
        "no oov words",
        "65.73%",
        "61.02%",
        "[BOLD] 69.35%",
        "40.90%",
        "64.02%",
        "53.69%",
        "52.60%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "[BOLD] 53.95%",
        "48.69%",
        "49.43%",
        "28.42%",
        "52.54%",
        "44.06%",
        "43.13%"
      ],
      [
        "Overall",
        "no oov words",
        "63.02%",
        "59.96%",
        "[BOLD] 68.97%",
        "36.45%",
        "52.04%",
        "56.30%",
        "52.66%"
      ],
      [
        "[EMPTY]",
        "with oov words",
        "53.60%",
        "51.00%",
        "[BOLD] 54.60%",
        "27.50%",
        "44.30%",
        "47.90%",
        "44.80%"
      ]
    ],
    "id": "8871d76b-c023-4dac-9ec0-866526d5cbed",
    "claim": "Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model cc.el.300 has outperformed all the other models apart from the case of the Syntactic category when we included the out-of-vocabulary (oov) terms [CONTINUE] where the model gr def had the best performance.",
    "label": "supports",
    "table_id": "5b468728-2bb8-41a6-8b44-30b94d52dd3b"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "cdad25c1-2680-41e3-b279-9383fc241c09",
    "claim": "Replacing the attention normalizing function with softmax operation increases the F1 score marginally (A3−A5).",
    "label": "refutes",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.",
    "table_column_names": [
      "[EMPTY]",
      "dev perp ↓",
      "dev acc ↑",
      "dev wer ↓",
      "test perp ↓",
      "test acc ↑",
      "test wer ↓"
    ],
    "table_content_values": [
      [
        "Spanish-only-LM",
        "329.68",
        "26.6",
        "30.47",
        "322.26",
        "25.1",
        "29.62"
      ],
      [
        "English-only-LM",
        "320.92",
        "29.3",
        "32.02",
        "314.04",
        "30.3",
        "32.51"
      ],
      [
        "All:CS-last-LM",
        "76.64",
        "47.8",
        "14.56",
        "76.97",
        "49.2",
        "14.13"
      ],
      [
        "All:Shuffled-LM",
        "68.00",
        "51.8",
        "13.64",
        "68.72",
        "51.4",
        "13.89"
      ],
      [
        "CS-only-LM",
        "43.20",
        "60.7",
        "12.60",
        "43.42",
        "57.9",
        "12.18"
      ],
      [
        "CS-only+vocab-LM",
        "45.61",
        "61.0",
        "12.56",
        "45.79",
        "58.8",
        "12.49"
      ],
      [
        "Fine-Tuned-LM",
        "39.76",
        "66.9",
        "10.71",
        "40.11",
        "65.4",
        "10.17"
      ],
      [
        "CS-only-disc",
        "–",
        "72.0",
        "6.35",
        "–",
        "70.5",
        "6.70"
      ],
      [
        "Fine-Tuned-disc",
        "–",
        "[BOLD] 74.2",
        "[BOLD] 5.85",
        "–",
        "[BOLD] 75.5",
        "[BOLD] 5.59"
      ]
    ],
    "id": "3bdf91f0-992d-410b-9583-2f70056ed281",
    "claim": "For each model we report both perplexity and accuracy (except for discriminative training, where perplexity is not valid), where each of them is reported according to the best performing model on that measure (on the dev set).",
    "label": "supports",
    "table_id": "d4c8c3ca-899e-4e8c-8efe-81b447163aa5"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 3: ARI and Silhouette coefficient scores.",
    "table_column_names": [
      "Methods",
      "Seanad Abolition ARI",
      "Seanad Abolition  [ITALIC] Sil",
      "Video Games ARI",
      "Video Games  [ITALIC] Sil",
      "Pornography ARI",
      "Pornography  [ITALIC] Sil"
    ],
    "table_content_values": [
      [
        "TF-IDF",
        "0.23",
        "0.02",
        "-0.01",
        "0.01",
        "-0.02",
        "0.01"
      ],
      [
        "WMD",
        "0.09",
        "0.01",
        "0.01",
        "0.01",
        "-0.02",
        "0.01"
      ],
      [
        "Sent2vec",
        "-0.01",
        "-0.01",
        "0.11",
        "0.06",
        "0.01",
        "0.02"
      ],
      [
        "Doc2vec",
        "-0.01",
        "-0.03",
        "-0.01",
        "0.01",
        "0.02",
        "-0.01"
      ],
      [
        "BERT",
        "0.03",
        "-0.04",
        "0.08",
        "0.05",
        "-0.01",
        "0.03"
      ],
      [
        "OD-parse",
        "0.01",
        "-0.04",
        "-0.01",
        "0.02",
        "0.07",
        "0.05"
      ],
      [
        "OD",
        "[BOLD] 0.54",
        "[BOLD] 0.31",
        "[BOLD] 0.56",
        "[BOLD] 0.42",
        "[BOLD] 0.41",
        "[BOLD] 0.41"
      ]
    ],
    "id": "aa179cf6-fac1-48d2-8bcf-e5ad45f72d25",
    "claim": "[CONTINUE] A notable exception is the \"Seanad Abolition\" dataset, where TF-IDF performs relatively better than WMD, Sent2vec and Doc2vec.",
    "label": "supports",
    "table_id": "41274426-d552-4e87-b3ba-efed0c4b05ab"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "LR-All Features – Original Data",
        "80.5",
        "78.0",
        "0.873"
      ],
      [
        "Dist. Supervision + Pooling",
        "77.2",
        "75.7",
        "0.853"
      ],
      [
        "Dist. Supervision + EasyAdapt",
        "[BOLD] 81.2",
        "[BOLD] 79.0",
        "[BOLD] 0.885"
      ]
    ],
    "id": "2b367697-ccca-45ce-a3ac-ef1ce5323dac",
    "claim": "Results presented in Table 7 show that the domain adaptation approach does not significantly boost F1 (t-test, p>0.5) and ROC AUC (0.012).",
    "label": "refutes",
    "table_id": "6dbcb51c-9427-4e68-be86-351890ad3d0a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "dd2ffda3-81a0-4aea-af41-fd85e010806f",
    "claim": "when the best model for each dataset is deployed in either setting, our program ablation does best and the non-ablated tree does slightly worse but still significantly outperforms the baseline sentence model (t = 12.7, p = 5.0×10-36).",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Original",
      "Italian Debiased",
      "Italian English",
      "Italian Reduction",
      "German Original",
      "German Debiased",
      "German English",
      "German Reduction"
    ],
    "table_content_values": [
      [
        "Same Gender",
        "0.442",
        "0.434",
        "0.424",
        "–",
        "0.491",
        "0.478",
        "0.446",
        "–"
      ],
      [
        "Different Gender",
        "0.385",
        "0.421",
        "0.415",
        "–",
        "0.415",
        "0.435",
        "0.403",
        "–"
      ],
      [
        "difference",
        "0.057",
        "0.013",
        "0.009",
        "[BOLD] 91.67%",
        "0.076",
        "0.043",
        "0.043",
        "[BOLD] 100%"
      ]
    ],
    "id": "058d2a40-7061-4dfa-bb41-bf00efd55f2c",
    "claim": "In German, we get a reduction of 100%.",
    "label": "supports",
    "table_id": "a3f7a8f4-e69f-4e32-beb4-281137d9a4a5"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "ca96fdb8-2600-46e2-b129-d6c81562945f",
    "claim": "Although these four models have the same number of layers, dense connections allow the model to achieve much better performance.",
    "label": "supports",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "87b2347d-2e19-4a25-bb66-a742f0e8ebfd",
    "claim": "The relatively low accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are not well-equipped to perform this task \"out-of-the-box\".",
    "label": "refutes",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "6943a94d-d91a-4fb2-965f-97aa1fa957ce",
    "claim": "Longer sentences pose additional challenges to the models.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training"
    ],
    "table_content_values": [
      [
        "Batch size",
        "Iter",
        "Recur",
        "Fold",
        "Iter",
        "Recur",
        "Fold"
      ],
      [
        "1",
        "19.2",
        "81.4",
        "16.5",
        "2.5",
        "4.8",
        "9.0"
      ],
      [
        "10",
        "49.3",
        "217.9",
        "52.2",
        "4.0",
        "4.2",
        "37.5"
      ],
      [
        "25",
        "72.1",
        "269.9",
        "61.6",
        "5.5",
        "3.6",
        "54.7"
      ]
    ],
    "id": "e8e28650-51c4-4fbe-b946-b7966b1625a2",
    "claim": "[CONTINUE] As a result, the folding technique performs better than the recursive approach for the training task.",
    "label": "supports",
    "table_id": "82989071-ae58-4870-9a3b-7e5f7a1ea4e7"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "377ecb73-e548-4831-8bbb-d0c2edda5227",
    "claim": "The largest gain is by 4% on the CoordInv task.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "755c48ed-ff95-42cf-9e30-670ae9546e4d",
    "claim": "This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 2: Human evaluation results on MSCOCO.",
    "table_column_names": [
      "[EMPTY]",
      "caption",
      "attention relevance"
    ],
    "table_content_values": [
      [
        "softmax",
        "3.50",
        "3.38"
      ],
      [
        "sparsemax",
        "3.71",
        "3.89"
      ],
      [
        "TVmax",
        "[BOLD] 3.87",
        "[BOLD] 4.10"
      ]
    ],
    "id": "1cd4c25a-774f-4e75-a511-1dead6a68155",
    "claim": "The inferior score on attention relevance shows that TVMAX is worse at selecting the relevant features and its output is less interpretable.",
    "label": "refutes",
    "table_id": "77097db2-3bbc-4070-a3a0-2eaad08e47de"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "2da369f5-44cc-4f6d-8c37-97bfb561547b",
    "claim": "Row (1)-(7) show each model with different representations on the original dataset.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.",
    "table_column_names": [
      "[ITALIC] Block",
      "[ITALIC] n",
      "[ITALIC] m",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "1",
        "1",
        "1",
        "17.6",
        "48.3"
      ],
      [
        "1",
        "1",
        "2",
        "19.2",
        "50.3"
      ],
      [
        "1",
        "2",
        "1",
        "18.4",
        "49.1"
      ],
      [
        "1",
        "1",
        "3",
        "19.6",
        "49.4"
      ],
      [
        "1",
        "3",
        "1",
        "20.0",
        "50.5"
      ],
      [
        "1",
        "3",
        "3",
        "21.4",
        "51.0"
      ],
      [
        "1",
        "3",
        "6",
        "21.8",
        "51.7"
      ],
      [
        "1",
        "6",
        "3",
        "21.7",
        "51.5"
      ],
      [
        "1",
        "6",
        "6",
        "22.0",
        "52.1"
      ],
      [
        "2",
        "3",
        "6",
        "[BOLD] 23.5",
        "53.3"
      ],
      [
        "2",
        "6",
        "3",
        "23.3",
        "[BOLD] 53.4"
      ],
      [
        "2",
        "6",
        "6",
        "22.0",
        "52.1"
      ]
    ],
    "id": "cd3bfb66-26db-4ea5-bbc7-33e9dd881d74",
    "claim": "We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give significantly different results for both 1 DCGCN block and 2 DCGCN blocks.",
    "label": "refutes",
    "table_id": "9ac477a4-1794-4d29-8be4-94cfe8c3b70a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "fd328d46-3d4e-4a19-9b01-87e794fed8b9",
    "claim": "(2017).8 Overall both BERT (76.5%) and RoBERTa (87.7%) do not outperform the best previous model (71.4%) on Hard instances without superficial cues.",
    "label": "refutes",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 1: Classifier performance",
    "table_column_names": [
      "Dataset",
      "Class",
      "Precision",
      "Recall",
      "F1"
    ],
    "table_content_values": [
      [
        "[ITALIC] W. & H.",
        "Racism",
        "0.73",
        "0.79",
        "0.76"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.69",
        "0.73",
        "0.71"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.88",
        "0.85",
        "0.86"
      ],
      [
        "[ITALIC] W.",
        "Racism",
        "0.56",
        "0.77",
        "0.65"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.62",
        "0.73",
        "0.67"
      ],
      [
        "[EMPTY]",
        "R. & S.",
        "0.56",
        "0.62",
        "0.59"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.95",
        "0.92",
        "0.94"
      ],
      [
        "[ITALIC] D. et al.",
        "Hate",
        "0.32",
        "0.53",
        "0.4"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.96",
        "0.88",
        "0.92"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.81",
        "0.95",
        "0.87"
      ],
      [
        "[ITALIC] G. et al.",
        "Harass.",
        "0.41",
        "0.19",
        "0.26"
      ],
      [
        "[EMPTY]",
        "Non.",
        "0.75",
        "0.9",
        "0.82"
      ],
      [
        "[ITALIC] F. et al.",
        "Hate",
        "0.33",
        "0.42",
        "0.37"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.87",
        "0.88",
        "0.88"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.5",
        "0.7",
        "0.58"
      ],
      [
        "[EMPTY]",
        "Neither",
        "0.88",
        "0.77",
        "0.82"
      ]
    ],
    "id": "1ea08ac9-44c5-4494-894b-728c4468c01b",
    "claim": "In particular, we see that hate speech and harassment are relatively easy to detect.",
    "label": "refutes",
    "table_id": "c989cbdc-ad17-41ce-836a-77ad71aefbc8"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 4: Manual evaluation results (%) using models from Table 2 (i.e., with roughly fixed Acc). > means “better than”. ΔSim=Sim(A)−Sim(B), and ΔPP=PP(A)−PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.",
    "table_column_names": [
      "Dataset",
      "Models A",
      "Models B",
      "Transfer quality A>B",
      "Transfer quality B>A",
      "Transfer quality Tie",
      "Semantic preservation A>B",
      "Semantic preservation B>A",
      "Semantic preservation Tie",
      "Semantic preservation ΔSim",
      "Fluency A>B",
      "Fluency B>A",
      "Fluency Tie",
      "Fluency ΔPP"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "M0",
        "M2",
        "9.0",
        "6.0",
        "85.1",
        "1.5",
        "[BOLD] 25.4",
        "73.1",
        "-0.05",
        "10.4",
        "[BOLD] 23.9",
        "65.7",
        "0.9"
      ],
      [
        "Yelp",
        "M0",
        "M7",
        "9.6",
        "14.7",
        "75.8",
        "2.5",
        "[BOLD] 54.5",
        "42.9",
        "-0.09",
        "4.6",
        "[BOLD] 39.4",
        "56.1",
        "8.3"
      ],
      [
        "Yelp",
        "M6",
        "M7",
        "13.7",
        "11.6",
        "74.7",
        "16.0",
        "16.7",
        "67.4",
        "0.01",
        "10.3",
        "20.0",
        "69.7",
        "14.3"
      ],
      [
        "[EMPTY]",
        "M2",
        "M7",
        "5.8",
        "9.3",
        "84.9",
        "8.1",
        "[BOLD] 25.6",
        "66.3",
        "-0.04",
        "14.0",
        "[BOLD] 26.7",
        "59.3",
        "7.4"
      ],
      [
        "Literature",
        "M2",
        "M6",
        "4.2",
        "6.7",
        "89.2",
        "16.7",
        "20.8",
        "62.5",
        "0.01",
        "[BOLD] 40.8",
        "13.3",
        "45.8",
        "-13.3"
      ],
      [
        "Literature",
        "M6",
        "M7",
        "15.8",
        "13.3",
        "70.8",
        "[BOLD] 25.0",
        "9.2",
        "65.8",
        "0.03",
        "14.2",
        "20.8",
        "65.0",
        "14.2"
      ]
    ],
    "id": "207bf674-dc1a-4979-8432-1342a80d538f",
    "claim": "For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments but significantly different Sim scores.",
    "label": "refutes",
    "table_id": "d3f0a504-9761-4763-bbb8-2d7bfe87ec88"
  },
  {
    "paper": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection",
    "paper_id": "1904.04388v1",
    "table_caption": "Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Model",
      "[BOLD] dev mean",
      "[BOLD] dev best",
      "[BOLD] test mean",
      "[BOLD] test best",
      "[ITALIC] α"
    ],
    "table_content_values": [
      [
        "single",
        "text",
        "86.54",
        "86.80",
        "86.47",
        "86.96",
        "–"
      ],
      [
        "single",
        "raw",
        "35.00",
        "37.33",
        "35.78",
        "37.70",
        "–"
      ],
      [
        "single",
        "innovations",
        "80.86",
        "81.51",
        "80.28",
        "82.15",
        "–"
      ],
      [
        "early",
        "text + raw",
        "86.46",
        "86.65",
        "86.24",
        "86.53",
        "–"
      ],
      [
        "early",
        "text + innovations",
        "86.53",
        "86.77",
        "86.54",
        "87.00",
        "–"
      ],
      [
        "early",
        "text + raw + innovations",
        "86.35",
        "86.69",
        "86.55",
        "86.44",
        "–"
      ],
      [
        "late",
        "text + raw",
        "86.71",
        "87.05",
        "86.35",
        "86.71",
        "0.2"
      ],
      [
        "late",
        "text + innovations",
        "[BOLD] 86.98",
        "[BOLD] 87.48",
        "[BOLD] 86.68",
        "[BOLD] 87.02",
        "0.5"
      ],
      [
        "late",
        "text + raw + innovations",
        "86.95",
        "87.30",
        "86.60",
        "86.87",
        "0.5"
      ]
    ],
    "id": "4d0e6a67-dd03-4a72-9b81-f8e50e9faae3",
    "claim": "The interpolation weight α for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction.",
    "label": "supports",
    "table_id": "f0441bd9-731a-41df-8a83-1d084b332e89"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "82060521-bd71-4f0d-90fd-6b0e9de930b3",
    "claim": "The performances of all models increase as the diameters of the graphs increase.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "7d1503a6-0b59-4b98-885a-a90ffb1da624",
    "claim": "[CONTINUE] however, GRU yields the best BLEU score of 26.28, outperforming oLRN (+0.45 BLEU).",
    "label": "refutes",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "On the difficulty of a distributional semantics of spoken language",
    "paper_id": "1803.08869v2",
    "table_caption": "Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
    "table_column_names": [
      "[EMPTY]",
      "Recall@10 (%)",
      "Median rank",
      "RSAimage"
    ],
    "table_content_values": [
      [
        "VGS",
        "27",
        "6",
        "0.4"
      ],
      [
        "SegMatch",
        "[BOLD] 10",
        "[BOLD] 37",
        "[BOLD] 0.5"
      ],
      [
        "Audio2vec-U",
        "5",
        "105",
        "0.0"
      ],
      [
        "Audio2vec-C",
        "2",
        "647",
        "0.0"
      ],
      [
        "Mean MFCC",
        "1",
        "1,414",
        "0.0"
      ],
      [
        "Chance",
        "0",
        "3,955",
        "0.0"
      ]
    ],
    "id": "92d4db6a-df9b-45a3-bb55-a309229fec18",
    "claim": "It does not come close to VGS on paraphrase retrieval, but it does correlate with the visual modality even better.",
    "label": "supports",
    "table_id": "087b26ab-9679-4d8d-b96c-57140c1a8b7b"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "dfce52f0-6525-4195-a26e-a69faf5d99eb",
    "claim": "[CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no main effect on SER from cleaning the missed slots, with only slight reductions in insertions and deletions.",
    "label": "refutes",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts",
    "paper_id": "1907.12674v1",
    "table_caption": "Table 3: Average diachronic performance",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Algorithm",
      "[BOLD] Precision",
      "[BOLD] Recall",
      "[BOLD] F1"
    ],
    "table_content_values": [
      [
        "Giga",
        "Baseline",
        "0.19",
        "0.51",
        "0.28"
      ],
      [
        "Giga",
        "Threshold",
        "0.46",
        "0.41",
        "[BOLD] 0.41"
      ],
      [
        "NOW",
        "Baseline",
        "0.26",
        "0.53",
        "0.34"
      ],
      [
        "NOW",
        "Threshold",
        "0.42",
        "0.41",
        "[BOLD] 0.41"
      ]
    ],
    "id": "ac0a6d46-3c6c-4e6f-8342-d436e43c9c8f",
    "claim": "For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold decreases recall and increases precision (differences are statistically significant with t-test, p < 0.05).",
    "label": "supports",
    "table_id": "99068c93-65d4-4183-8b1c-e0c3ee0d6005"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 5: Textual similarity scores (asymmetric, Multi30k).",
    "table_column_names": [
      "[EMPTY]",
      "EN → DE R@1",
      "EN → DE R@5",
      "EN → DE R@10",
      "DE → EN R@1",
      "DE → EN R@5",
      "DE → EN R@10"
    ],
    "table_content_values": [
      [
        "FME",
        "51.4",
        "76.4",
        "84.5",
        "46.9",
        "71.2",
        "79.1"
      ],
      [
        "AME",
        "[BOLD] 51.7",
        "[BOLD] 76.7",
        "[BOLD] 85.1",
        "[BOLD] 49.1",
        "[BOLD] 72.6",
        "[BOLD] 80.5"
      ]
    ],
    "id": "ad0e53cd-7e67-4eb5-bb8d-33949d1f8e6a",
    "claim": "AME outperforms the FME model, confirming the importance of word embeddings adaptation.",
    "label": "supports",
    "table_id": "7a510b9a-eb7e-4be5-8a97-0694604a8f80"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "8346b898-68f9-44f3-84de-cf4cfeeb808c",
    "claim": "On the other hand, ACER is still subject to trainability limitation due to the lacking of expressivity power in DSTC models.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Parameters",
      "[BOLD] Validation AUC@0.05",
      "[BOLD] Test AUC@0.05"
    ],
    "table_content_values": [
      [
        "Base",
        "8.0M",
        "[BOLD] 0.871",
        "0.816"
      ],
      [
        "4L SRU → 2L LSTM",
        "7.3M",
        "0.864",
        "[BOLD] 0.829"
      ],
      [
        "4L SRU → 2L SRU",
        "7.8M",
        "0.856",
        "[BOLD] 0.829"
      ],
      [
        "Flat → hierarchical",
        "12.4M",
        "0.825",
        "0.559"
      ],
      [
        "Cross entropy → hinge loss",
        "8.0M",
        "0.765",
        "0.693"
      ],
      [
        "6.6M → 1M examples",
        "8.0M",
        "0.835",
        "0.694"
      ],
      [
        "6.6M → 100K examples",
        "8.0M",
        "0.565",
        "0.417"
      ],
      [
        "200 → 100 negatives",
        "8.0M",
        "0.864",
        "0.647"
      ],
      [
        "200 → 10 negatives",
        "8.0M",
        "0.720",
        "0.412"
      ]
    ],
    "id": "5323b8d2-37fa-4b1b-a9d9-05db8157d527",
    "claim": "[CONTINUE] We observed no advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs similarly to either a 4 layer or a 2 layer SRU with a comparable number of parameters.",
    "label": "supports",
    "table_id": "5efab962-70b6-488d-9601-cbd5b4852a40"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "67184fb9-20ca-445e-8366-7d03160cce3a",
    "claim": "The single DCGCN model achieves a BLEU score of 30.4 and a CHRF++ score of 59.6, outperforming the ensemble approach based on combining five DCGCN models initialized with different random seeds.",
    "label": "refutes",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "2a87f7a2-dfb7-48c8-944d-aab7eb874e0e",
    "claim": "The proposed method does not outperform the original embeddings and performs worse than the SOV.",
    "label": "refutes",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 4: Results of Self-Play Evaluation.",
    "table_column_names": [
      "System",
      "TGPC Succ. (%)",
      "TGPC #Turns",
      "CWC Succ. (%)",
      "CWC #Turns"
    ],
    "table_content_values": [
      [
        "Retrieval ",
        "7.16",
        "4.17",
        "0",
        "-"
      ],
      [
        "Retrieval-Stgy ",
        "47.80",
        "6.7",
        "44.6",
        "7.42"
      ],
      [
        "PMI ",
        "35.36",
        "6.38",
        "47.4",
        "5.29"
      ],
      [
        "Neural ",
        "54.76",
        "4.73",
        "47.6",
        "5.16"
      ],
      [
        "Kernel ",
        "62.56",
        "4.65",
        "53.2",
        "4.08"
      ],
      [
        "DKRN (ours)",
        "[BOLD] 89.0",
        "5.02",
        "[BOLD] 84.4",
        "4.20"
      ]
    ],
    "id": "292204b8-c7d0-4b68-8a35-f392930d4194",
    "claim": "This superior confirms the effectiveness of our approach.",
    "label": "supports",
    "table_id": "5a8c9b40-6ff6-44b7-af2e-4bf75b46492f"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "026892e8-1d2c-412c-8881-af5a2b4400b2",
    "claim": "Among all the baselines, GDPL does not obtain the most preference against PPO.",
    "label": "refutes",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "69b2e0e5-215d-4de9-840b-6752ca98311a",
    "claim": "The DCGCN models do not achieve the highest BLEU points on the En-De and En-Cs tasks, respectively.",
    "label": "refutes",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "95306f5e-c196-47cd-99a0-f54bb70d41e7",
    "claim": "Longer sentences do not pose additional challenges to the models.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "bdc9f7b9-8ef2-48a3-b7a9-7e6927c6ca87",
    "claim": "our model achieves a slightly better performance in AUC than the baseline models and the proposed model works better.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "47460ee9-1544-4470-82f5-665181faad47",
    "claim": "This suggests that our models are not capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.",
    "label": "refutes",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "d93f4cc1-346e-4d0f-a0c7-c1cd87778fb1",
    "claim": "Adding either the global node or the linear combination improves the baseline models with only dense connections.",
    "label": "supports",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "Baseline (No SA)Anderson et al. ( 2018 )",
        "55.00",
        "0M"
      ],
      [
        "SA (S: 1,2,3 - B: 1)",
        "55.11",
        "} 0.107M"
      ],
      [
        "SA (S: 1,2,3 - B: 2)",
        "55.17",
        "} 0.107M"
      ],
      [
        "[BOLD] SA (S: 1,2,3 - B: 3)",
        "[BOLD] 55.27",
        "} 0.107M"
      ]
    ],
    "id": "5cd2b818-6cbd-43ff-a379-23d4544992da",
    "claim": "We showed that it is not possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks.",
    "label": "refutes",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training"
    ],
    "table_content_values": [
      [
        "Batch size",
        "Iter",
        "Recur",
        "Fold",
        "Iter",
        "Recur",
        "Fold"
      ],
      [
        "1",
        "19.2",
        "81.4",
        "16.5",
        "2.5",
        "4.8",
        "9.0"
      ],
      [
        "10",
        "49.3",
        "217.9",
        "52.2",
        "4.0",
        "4.2",
        "37.5"
      ],
      [
        "25",
        "72.1",
        "269.9",
        "61.6",
        "5.5",
        "3.6",
        "54.7"
      ]
    ],
    "id": "7bb70ac8-2e77-430a-9c0c-c47235dad046",
    "claim": "The amount of resources is insufficient for executing forward computations, and therefore our framework does not outperform the folding technique for the inference task with up to 4.93x faster throughput.",
    "label": "refutes",
    "table_id": "82989071-ae58-4870-9a3b-7e5f7a1ea4e7"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Parameters",
      "[BOLD] Validation AUC@0.05",
      "[BOLD] Test AUC@0.05"
    ],
    "table_content_values": [
      [
        "Base",
        "8.0M",
        "[BOLD] 0.871",
        "0.816"
      ],
      [
        "4L SRU → 2L LSTM",
        "7.3M",
        "0.864",
        "[BOLD] 0.829"
      ],
      [
        "4L SRU → 2L SRU",
        "7.8M",
        "0.856",
        "[BOLD] 0.829"
      ],
      [
        "Flat → hierarchical",
        "12.4M",
        "0.825",
        "0.559"
      ],
      [
        "Cross entropy → hinge loss",
        "8.0M",
        "0.765",
        "0.693"
      ],
      [
        "6.6M → 1M examples",
        "8.0M",
        "0.835",
        "0.694"
      ],
      [
        "6.6M → 100K examples",
        "8.0M",
        "0.565",
        "0.417"
      ],
      [
        "200 → 100 negatives",
        "8.0M",
        "0.864",
        "0.647"
      ],
      [
        "200 → 10 negatives",
        "8.0M",
        "0.720",
        "0.412"
      ]
    ],
    "id": "5a52f554-0dbd-442d-af13-23015337b5f0",
    "claim": "We observed an advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs worse than either a 4 layer or a 2 layer SRU with a comparable number of parameters.",
    "label": "refutes",
    "table_id": "5efab962-70b6-488d-9601-cbd5b4852a40"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] PPA Acc."
    ],
    "table_content_values": [
      [
        "full",
        "89.7"
      ],
      [
        "- sense priors",
        "88.4"
      ],
      [
        "- attention",
        "87.5"
      ]
    ],
    "id": "ef8adc9d-e855-47d4-b3d1-f5099f9892f7",
    "claim": "The second row in Table 3 shows the test accuracy of a system trained without sense priors [CONTINUE] and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations, but still grounded in WordNet.",
    "label": "supports",
    "table_id": "6f5e49e4-970d-41d9-99e5-fe58653ea248"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Present",
      "[BOLD] Not Present"
    ],
    "table_content_values": [
      [
        "Emoji",
        "4805 (76.6%)",
        "23952 (68.0%)"
      ],
      [
        "Hashtags",
        "2122 (70.5%)",
        "26635 (69.4%)"
      ]
    ],
    "id": "58fa3a44-8ffc-4329-bddb-79ec2c2b9b21",
    "claim": "[CONTINUE] Hashtags also have a [CONTINUE] positive effect on classification performance, however it is less significant.",
    "label": "supports",
    "table_id": "76b1831c-65dd-45d3-bdc5-b490b3a9cee2"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "c048f310-3949-4f77-a0fd-f053459012d4",
    "claim": "Each participant evaluates 3 dialog sessions of each model.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer aroma+palate",
        "Beer look",
        "74.41",
        "74.83",
        "74.94",
        "72.75",
        "76.41",
        "[BOLD] 79.53",
        "80.29"
      ],
      [
        "Beer look+palate",
        "Beer aroma",
        "68.57",
        "69.23",
        "67.55",
        "69.92",
        "76.45",
        "[BOLD] 77.94",
        "78.11"
      ],
      [
        "Beer look+aroma",
        "Beer palate",
        "63.88",
        "67.82",
        "65.72",
        "74.66",
        "73.40",
        "[BOLD] 75.24",
        "75.50"
      ]
    ],
    "id": "284e66c0-e63c-4e04-99a7-e91ee70cdd14",
    "claim": "Our model (OURS) obtains substantial gains in accuracy over the baselines across all three target aspects.",
    "label": "supports",
    "table_id": "461a04b0-4d80-4f54-a0ed-42b74709e562"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "9c92823f-0db0-4455-b09c-2636c5e90d5d",
    "claim": "With the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.",
    "label": "refutes",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Two Causal Principles for Improving Visual Dialog",
    "paper_id": "1911.10496v2",
    "table_caption": "Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.",
    "table_column_names": [
      "Model",
      "LF ",
      "HCIAE ",
      "CoAtt ",
      "RvA "
    ],
    "table_content_values": [
      [
        "baseline",
        "57.21",
        "56.98",
        "56.46",
        "56.74"
      ],
      [
        "+P1",
        "61.88",
        "60.12",
        "60.27",
        "61.02"
      ],
      [
        "+P2",
        "72.65",
        "71.50",
        "71.41",
        "71.44"
      ],
      [
        "+P1+P2",
        "[BOLD] 73.63",
        "71.99",
        "71.87",
        "72.88"
      ]
    ],
    "id": "a6442b25-639a-4e9f-acc1-2af93942e266",
    "claim": "Note that the effectiveness of P1 and P2 are not necessarily additive, as combining P1 and P2 does not always perform the best.",
    "label": "refutes",
    "table_id": "df3ac154-71e4-4dc9-845e-7976d7dce3ae"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 3: Performance comparison of our model with different values of m on the two datasets.",
    "table_column_names": [
      "[ITALIC] m",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "1",
        "0.541",
        "0.595",
        "[BOLD] 0.566",
        "0.495",
        "0.621",
        "0.551"
      ],
      [
        "2",
        "0.521",
        "0.597",
        "0.556",
        "0.482",
        "0.656",
        "0.555"
      ],
      [
        "3",
        "0.490",
        "0.617",
        "0.547",
        "0.509",
        "0.633",
        "0.564"
      ],
      [
        "4",
        "0.449",
        "0.623",
        "0.522",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "5",
        "0.467",
        "0.609",
        "0.529",
        "0.488",
        "0.677",
        "0.567"
      ]
    ],
    "id": "5b31abdf-f132-46a7-8da5-490adbe8d469",
    "claim": "We observe that for the NYT10 dataset, m = 4 gives the highest F1 score.",
    "label": "refutes",
    "table_id": "fbd16514-f1a6-4567-8557-22b8d673c5b6"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
    "table_column_names": [
      "[BOLD] Training data",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] Disfl"
    ],
    "table_content_values": [
      [
        "Original",
        "0",
        "22",
        "0",
        "14"
      ],
      [
        "Cleaned added",
        "0",
        "23",
        "0",
        "14"
      ],
      [
        "Cleaned missing",
        "0",
        "1",
        "0",
        "2"
      ],
      [
        "Cleaned",
        "0",
        "0",
        "0",
        "5"
      ]
    ],
    "id": "2096086d-1f21-4a72-992f-724d69319e5d",
    "claim": "The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency.",
    "label": "supports",
    "table_id": "82ce68c2-64df-452b-ac0b-064c7fdaefa8"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "64d3506d-5cb3-472b-8186-a7e06b507407",
    "claim": "The Word2Vec embeddings appear to perform better than our method on the random test, although we suspect that the difference is marginal.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "5025d368-b507-4e9e-850b-ac7661dbc30b",
    "claim": "Although SFN requires a large portion of training data to achieve superior performance, we find that combining large amounts of multi-action parallel data can significantly improve the model’s performance.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts",
    "paper_id": "1907.12674v1",
    "table_caption": "Table 3: Average diachronic performance",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Algorithm",
      "[BOLD] Precision",
      "[BOLD] Recall",
      "[BOLD] F1"
    ],
    "table_content_values": [
      [
        "Giga",
        "Baseline",
        "0.19",
        "0.51",
        "0.28"
      ],
      [
        "Giga",
        "Threshold",
        "0.46",
        "0.41",
        "[BOLD] 0.41"
      ],
      [
        "NOW",
        "Baseline",
        "0.26",
        "0.53",
        "0.34"
      ],
      [
        "NOW",
        "Threshold",
        "0.42",
        "0.41",
        "[BOLD] 0.41"
      ]
    ],
    "id": "750beeba-88c2-479e-a030-0576133256a4",
    "claim": "For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold increases recall and decreases precision (differences are statistically significant with t-test, p < 0.05).",
    "label": "refutes",
    "table_id": "99068c93-65d4-4183-8b1c-e0c3ee0d6005"
  },
  {
    "paper": "Two Causal Principles for Improving Visual Dialog",
    "paper_id": "1911.10496v2",
    "table_caption": "Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.",
    "table_column_names": [
      "Model",
      "baseline",
      "QT",
      "S  [ITALIC] R0",
      "S  [ITALIC] R1",
      "S  [ITALIC] R2",
      "S  [ITALIC] R3",
      "D"
    ],
    "table_content_values": [
      [
        "LF ",
        "57.21",
        "58.97",
        "67.82",
        "71.27",
        "72.04",
        "72.36",
        "72.65"
      ],
      [
        "LF +P1",
        "61.88",
        "62.87",
        "69.47",
        "72.16",
        "72.85",
        "73.42",
        "[BOLD] 73.63"
      ]
    ],
    "id": "e02742aa-05d4-4e91-ab12-6f4aaa5409ee",
    "claim": "Overall, all of the implementations can improve the performances of base models.",
    "label": "supports",
    "table_id": "c4098014-d3c7-4ee8-b56d-3553dcc3d89f"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "7dad5701-2235-4fc8-b66b-306812347530",
    "claim": "In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks except WC.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Two Causal Principles for Improving Visual Dialog",
    "paper_id": "1911.10496v2",
    "table_caption": "Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.",
    "table_column_names": [
      "Model",
      "LF ",
      "HCIAE ",
      "CoAtt ",
      "RvA "
    ],
    "table_content_values": [
      [
        "baseline",
        "57.21",
        "56.98",
        "56.46",
        "56.74"
      ],
      [
        "+P1",
        "61.88",
        "60.12",
        "60.27",
        "61.02"
      ],
      [
        "+P2",
        "72.65",
        "71.50",
        "71.41",
        "71.44"
      ],
      [
        "+P1+P2",
        "[BOLD] 73.63",
        "71.99",
        "71.87",
        "72.88"
      ]
    ],
    "id": "38bfa8f1-7948-49d5-bc60-08c75e385df9",
    "claim": "Note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 performs the best.",
    "label": "supports",
    "table_id": "df3ac154-71e4-4dc9-845e-7976d7dce3ae"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "47e27211-3006-49e1-a5d6-34a1993698c1",
    "claim": "Despite achieving high performance in the task success, GDPL does not show substantial improvement in inform F1 and match rate over the baselines.",
    "label": "refutes",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Aligning Vector-spaces with Noisy Supervised Lexicons",
    "paper_id": "1903.10238v1",
    "table_caption": "Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
    "table_column_names": [
      "Method",
      "En→It best",
      "En→It avg",
      "En→It iters",
      "En→De best",
      "En→De avg",
      "En→De iters",
      "En→Fi best",
      "En→Fi avg",
      "En→Fi iters",
      "En→Es best",
      "En→Es avg",
      "En→Es iters"
    ],
    "table_content_values": [
      [
        "Artetxe et al., 2018b",
        "[BOLD] 48.53",
        "48.13",
        "573",
        "48.47",
        "48.19",
        "773",
        "33.50",
        "32.63",
        "988",
        "37.60",
        "37.33",
        "808"
      ],
      [
        "Noise-aware Alignment",
        "[BOLD] 48.53",
        "[BOLD] 48.20",
        "471",
        "[BOLD] 49.67",
        "[BOLD] 48.89",
        "568",
        "[BOLD] 33.98",
        "[BOLD] 33.68",
        "502",
        "[BOLD] 38.40",
        "[BOLD] 37.79",
        "551"
      ]
    ],
    "id": "6077db2c-6ffe-4629-9f6c-42197e0ad297",
    "claim": "In most setups our best case is not better than the former best case.",
    "label": "refutes",
    "table_id": "279e3d12-df99-48f8-83ea-d3e42b8bbfcc"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "024b4b9e-0169-4451-a529-066860ae0555",
    "claim": "[CONTINUE] As we can observe, it seems that clustering semantically related terms will increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected.",
    "label": "supports",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
    "table_column_names": [
      "[EMPTY]",
      "MSCOCO spice",
      "MSCOCO cider",
      "MSCOCO rouge [ITALIC] L",
      "MSCOCO bleu4",
      "MSCOCO meteor",
      "MSCOCO rep↓",
      "Flickr30k spice",
      "Flickr30k cider",
      "Flickr30k rouge [ITALIC] L",
      "Flickr30k bleu4",
      "Flickr30k meteor",
      "Flickr30k rep↓"
    ],
    "table_content_values": [
      [
        "softmax",
        "18.4",
        "0.967",
        "52.9",
        "29.9",
        "24.9",
        "3.76",
        "13.5",
        "0.443",
        "44.2",
        "19.9",
        "19.1",
        "6.09"
      ],
      [
        "sparsemax",
        "[BOLD] 18.9",
        "[BOLD] 0.990",
        "[BOLD] 53.5",
        "[BOLD] 31.5",
        "[BOLD] 25.3",
        "3.69",
        "[BOLD] 13.7",
        "[BOLD] 0.444",
        "[BOLD] 44.3",
        "[BOLD] 20.7",
        "[BOLD] 19.3",
        "5.84"
      ],
      [
        "TVmax",
        "18.5",
        "0.974",
        "53.1",
        "29.9",
        "25.1",
        "[BOLD] 3.17",
        "13.3",
        "0.438",
        "44.2",
        "20.5",
        "19.0",
        "[BOLD] 3.97"
      ]
    ],
    "id": "0abaf60d-6117-4b20-8493-f3678aadd259",
    "claim": "Selective attention mechanisms like sparsemax and especially TVMAX do not reduce repetition, as measured by the REP metric reported in Table 1.",
    "label": "refutes",
    "table_id": "55b8ec67-1f65-4852-943b-d1530519e837"
  },
  {
    "paper": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
    "paper_id": "1809.09078v2",
    "table_caption": "Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] Trigger  [BOLD] Identification (%)",
      "[BOLD] Trigger  [BOLD] Identification (%)",
      "[BOLD] Trigger  [BOLD] Identification (%)",
      "[BOLD] Trigger  [BOLD] Classification (%)",
      "[BOLD] Trigger  [BOLD] Classification (%)",
      "[BOLD] Trigger  [BOLD] Classification (%)",
      "[BOLD] Argument  [BOLD] Identification (%)",
      "[BOLD] Argument  [BOLD] Identification (%)",
      "[BOLD] Argument  [BOLD] Identification (%)",
      "[BOLD] Argument  [BOLD] Role (%)",
      "[BOLD] Argument  [BOLD] Role (%)",
      "[BOLD] Argument  [BOLD] Role (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] Method",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1"
      ],
      [
        "Cross-Event",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "68.7",
        "68.9",
        "68.8",
        "50.9",
        "49.7",
        "50.3",
        "45.1",
        "44.1",
        "44.6"
      ],
      [
        "JointBeam",
        "76.9",
        "65.0",
        "70.4",
        "73.7",
        "62.3",
        "67.5",
        "69.8",
        "47.9",
        "56.8",
        "64.7",
        "44.4",
        "52.7"
      ],
      [
        "DMCNN",
        "[BOLD] 80.4",
        "67.7",
        "73.5",
        "75.6",
        "63.6",
        "69.1",
        "68.8",
        "51.9",
        "59.1",
        "62.2",
        "46.9",
        "53.5"
      ],
      [
        "PSL",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "75.3",
        "64.4",
        "69.4",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "JRNN",
        "68.5",
        "[BOLD] 75.7",
        "71.9",
        "66.0",
        "[BOLD] 73.0",
        "69.3",
        "61.4",
        "64.2",
        "62.8",
        "54.2",
        "56.7",
        "55.4"
      ],
      [
        "dbRNN",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "74.1",
        "69.8",
        "71.9",
        "71.3",
        "64.5",
        "67.7",
        "66.2",
        "52.8",
        "58.7"
      ],
      [
        "[BOLD] JMEE",
        "80.2",
        "72.1",
        "[BOLD] 75.9",
        "[BOLD] 76.3",
        "71.3",
        "[BOLD] 73.7",
        "[BOLD] 71.4",
        "[BOLD] 65.6",
        "[BOLD] 68.4",
        "[BOLD] 66.8",
        "[BOLD] 54.9",
        "[BOLD] 60.3"
      ]
    ],
    "id": "df4b4d38-b005-4456-b84e-1780b5fcc389",
    "claim": "From the table, we can see that our JMEE framework does not achieve the best F1 scores for both trigger classification and argument-related subtasks among all the compared methods.",
    "label": "refutes",
    "table_id": "cb128f63-9ce7-4d32-ba3b-4924dc7b8b7d"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "c38e08e0-2ef9-4fdc-bf65-6218e5f62a85",
    "claim": "The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset.",
    "label": "supports",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "a4701b9a-2f9b-4864-8085-d1ec451d1455",
    "claim": "our system also receives the highest rating in 70% of test cases.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.",
    "table_column_names": [
      "[BOLD] Decoder configuration",
      "[BOLD] es-en  [BOLD] Health",
      "[BOLD] es-en  [BOLD] Bio",
      "[BOLD] en-de  [BOLD] News",
      "[BOLD] en-de  [BOLD] TED",
      "[BOLD] en-de  [BOLD] IT"
    ],
    "table_content_values": [
      [
        "Oracle model",
        "35.9",
        "37.8",
        "37.8",
        "27.0",
        "57.0"
      ],
      [
        "Uniform",
        "36.0",
        "36.4",
        "[BOLD] 38.9",
        "26.0",
        "43.5"
      ],
      [
        "BI + IS",
        "[BOLD] 36.2",
        "[BOLD] 38.0",
        "38.7",
        "[BOLD] 26.1",
        "[BOLD] 56.4"
      ]
    ],
    "id": "db48fd7d-eac7-402b-986a-1295738e6236",
    "claim": "[CONTINUE] EWC models do not perform as well as uniform ensembling, as evidenced by the fact that in some cases, uniform ensembling outperforms the oracle.",
    "label": "refutes",
    "table_id": "eda25ffd-d637-46ae-9e22-e89836a2ffed"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "e096ffd1-7885-4c82-b7df-e0b27418c2b0",
    "claim": "GDPL is better at booking flights and restaurants than finding hotels, even though its SLU precision is comparable to other agents.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] R-1",
      "[BOLD] R-2",
      "[BOLD] R-SU"
    ],
    "table_content_values": [
      [
        "First-1",
        "26.83",
        "7.25",
        "6.46"
      ],
      [
        "First-2",
        "35.99",
        "10.17",
        "12.06"
      ],
      [
        "First-3",
        "39.41",
        "11.77",
        "14.51"
      ],
      [
        "LexRank Erkan and Radev ( 2004 )",
        "38.27",
        "12.70",
        "13.20"
      ],
      [
        "TextRank Mihalcea and Tarau ( 2004 )",
        "38.44",
        "13.10",
        "13.50"
      ],
      [
        "MMR Carbonell and Goldstein ( 1998 )",
        "38.77",
        "11.98",
        "12.91"
      ],
      [
        "PG-Original Lebanoff et al. ( 2018 )",
        "41.85",
        "12.91",
        "16.46"
      ],
      [
        "PG-MMR Lebanoff et al. ( 2018 )",
        "40.55",
        "12.36",
        "15.87"
      ],
      [
        "PG-BRNN Gehrmann et al. ( 2018 )",
        "42.80",
        "14.19",
        "16.75"
      ],
      [
        "CopyTransformer Gehrmann et al. ( 2018 )",
        "[BOLD] 43.57",
        "14.03",
        "17.37"
      ],
      [
        "Hi-MAP (Our Model)",
        "43.47",
        "[BOLD] 14.89",
        "[BOLD] 17.41"
      ]
    ],
    "id": "ac84d56e-4583-4f9c-ae2a-0731f62551ba",
    "claim": "Our model outperforms PG-MMR when trained and tested on the Multi-News dataset.",
    "label": "supports",
    "table_id": "6f86cef3-2ee0-4782-b5b4-f37400c22dfb"
  },
  {
    "paper": "Zero-Shot Grounding of Objects from Natural Language Queries",
    "paper_id": "1908.07129v1",
    "table_caption": "Table 3: Category-wise performance with the default split of Flickr30k Entities.",
    "table_column_names": [
      "Method",
      "Overall",
      "people",
      "clothing",
      "bodyparts",
      "animals",
      "vehicles",
      "instruments",
      "scene",
      "other"
    ],
    "table_content_values": [
      [
        "QRC - VGG(det)",
        "60.21",
        "75.08",
        "55.9",
        "20.27",
        "73.36",
        "68.95",
        "45.68",
        "65.27",
        "38.8"
      ],
      [
        "CITE - VGG(det)",
        "61.89",
        "[BOLD] 75.95",
        "58.50",
        "30.78",
        "[BOLD] 77.03",
        "[BOLD] 79.25",
        "48.15",
        "58.78",
        "43.24"
      ],
      [
        "ZSGNet - VGG (cls)",
        "60.12",
        "72.52",
        "60.57",
        "38.51",
        "63.61",
        "64.47",
        "49.59",
        "64.66",
        "41.09"
      ],
      [
        "ZSGNet - Res50 (cls)",
        "[BOLD] 63.39",
        "73.87",
        "[BOLD] 66.18",
        "[BOLD] 45.27",
        "73.79",
        "71.38",
        "[BOLD] 58.54",
        "[BOLD] 66.49",
        "[BOLD] 45.53"
      ]
    ],
    "id": "bb3c6b73-b0df-4c55-a45f-44744409c0cf",
    "claim": "However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet shows much better performance.",
    "label": "supports",
    "table_id": "347fd2b9-f84e-4288-a2e3-b793a47f6266"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "6eaf2d50-5277-4860-8a5f-c43beb58c9e3",
    "claim": "The coverage mechanism is not effective in our models.",
    "label": "refutes",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Low-supervision urgency detection and transfer in short crisis messages",
    "paper_id": "1907.06745v1",
    "table_caption": "TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
    "table_column_names": [
      "System",
      "Accuracy",
      "Precision",
      "Recall",
      "F-Measure"
    ],
    "table_content_values": [
      [
        "Local",
        "63.97%",
        "64.27%",
        "64.50%",
        "63.93%"
      ],
      [
        "Manual",
        "64.25%",
        "[BOLD] 70.84%∗∗",
        "48.50%",
        "57.11%"
      ],
      [
        "Wiki",
        "67.25%",
        "66.51%",
        "69.50%",
        "67.76%"
      ],
      [
        "Local-Manual",
        "65.75%",
        "67.96%",
        "59.50%",
        "62.96%"
      ],
      [
        "Wiki-Local",
        "67.40%",
        "65.54%",
        "68.50%",
        "66.80%"
      ],
      [
        "Wiki-Manual",
        "67.75%",
        "70.38%",
        "63.00%",
        "65.79%"
      ],
      [
        "[ITALIC] Our Approach",
        "[BOLD] 69.25%∗∗∗",
        "68.76%",
        "[BOLD] 70.50%∗∗",
        "[BOLD] 69.44%∗∗∗"
      ]
    ],
    "id": "54e7aefb-6033-47d9-9435-ec4661f93470",
    "claim": "Similarly, manual features reduce recall, but help the system to improve accuracy and precision (sometimes considerably).",
    "label": "supports",
    "table_id": "678b9f33-65b1-4b0a-b4f6-96de47883169"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "549bac5e-c1c3-4601-9908-b900f7c78abd",
    "claim": "[CONTINUE] However, the slightly increased invalid response percentage [CONTINUE] We also observe our DAMD model outperforms HDSA in both diversity and appropriateness scores.",
    "label": "supports",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
    "table_column_names": [
      "[EMPTY]",
      "Difference Function",
      "Seanad Abolition",
      "Video Games",
      "Pornography"
    ],
    "table_content_values": [
      [
        "OD-parse",
        "Absolute",
        "0.01",
        "-0.01",
        "0.07"
      ],
      [
        "OD-parse",
        "JS div.",
        "0.01",
        "-0.01",
        "-0.01"
      ],
      [
        "OD-parse",
        "EMD",
        "0.07",
        "0.01",
        "-0.01"
      ],
      [
        "OD",
        "Absolute",
        "[BOLD] 0.54",
        "[BOLD] 0.56",
        "[BOLD] 0.41"
      ],
      [
        "OD",
        "JS div.",
        "0.07",
        "-0.01",
        "-0.02"
      ],
      [
        "OD",
        "EMD",
        "0.26",
        "-0.01",
        "0.01"
      ],
      [
        "OD (no polarity shifters)",
        "Absolute",
        "0.23",
        "0.08",
        "0.04"
      ],
      [
        "OD (no polarity shifters)",
        "JS div.",
        "0.09",
        "-0.01",
        "-0.02"
      ],
      [
        "OD (no polarity shifters)",
        "EMD",
        "0.10",
        "0.01",
        "-0.01"
      ]
    ],
    "id": "8f6c2db9-abdd-4e42-9ad8-3898a329957d",
    "claim": "On the three datasets, OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 respectively compared to the scores of 0.01, -0.01 and 0.07 by OD-parse.",
    "label": "supports",
    "table_id": "fba0fe3a-58cc-47a0-a3ab-5d091d2b7fe7"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "Baseline (No SA)Anderson et al. ( 2018 )",
        "55.00",
        "0M"
      ],
      [
        "SA (S: 1,2,3 - B: 1)",
        "55.11",
        "} 0.107M"
      ],
      [
        "SA (S: 1,2,3 - B: 2)",
        "55.17",
        "} 0.107M"
      ],
      [
        "[BOLD] SA (S: 1,2,3 - B: 3)",
        "[BOLD] 55.27",
        "} 0.107M"
      ]
    ],
    "id": "f248d065-435c-4785-bd05-398870db94b1",
    "claim": "[CONTINUE] We notice small improvements relative to the baseline showing that self-attention alone does improve the VQA task.",
    "label": "supports",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "ba9a2f4b-93b8-4d76-9216-ce8824a47208",
    "claim": "We don’t evaluate RoBERTa on the 100 instance subset of COPA due to its tendency to pick superficial cues.",
    "label": "not enough info",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
    "table_column_names": [
      "[BOLD] Representation",
      "[BOLD] Hyper parameters Filter size",
      "[BOLD] Hyper parameters Num. Feature maps",
      "[BOLD] Hyper parameters Activation func.",
      "[BOLD] Hyper parameters L2 Reg.",
      "[BOLD] Hyper parameters Learning rate",
      "[BOLD] Hyper parameters Dropout Prob.",
      "[BOLD] F1.(avg. in 5-fold) with default values",
      "[BOLD] F1.(avg. in 5-fold) with optimal values"
    ],
    "table_content_values": [
      [
        "CoNLL08",
        "4-5",
        "1000",
        "Softplus",
        "1.15e+01",
        "1.13e-03",
        "1",
        "73.34",
        "74.49"
      ],
      [
        "SB",
        "4-5",
        "806",
        "Sigmoid",
        "8.13e-02",
        "1.79e-03",
        "0.87",
        "72.83",
        "[BOLD] 75.05"
      ],
      [
        "UD v1.3",
        "5",
        "716",
        "Softplus",
        "1.66e+00",
        "9.63E-04",
        "1",
        "68.93",
        "69.57"
      ]
    ],
    "id": "e0cadaaf-cbe4-40d0-82a3-380e6977dab4",
    "claim": "We observe that the results for the UD representation are comparable to the two others.",
    "label": "refutes",
    "table_id": "602023a1-3ab5-4788-8101-59ae8f0c6ce1"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 1: Benchmark performance, Spearman’s ρ. SGNS results with * taken from [morphfit]. Best results per column (benchmark) annotated for our setup only.",
    "table_column_names": [
      "Context: w2",
      "Context: w2 SimLex",
      "Context: w2 SimLex",
      "Context: w2 SimLex",
      "Context: w2 SimLex",
      "Context: w2 SimVerb"
    ],
    "table_content_values": [
      [
        "target",
        "N",
        "V",
        "A",
        "all",
        "V"
      ],
      [
        "type",
        ".334",
        "<bold>.336</bold>",
        "<bold>.518</bold>",
        ".348",
        ".307"
      ],
      [
        "x + POS",
        ".342",
        ".323",
        ".513",
        ".350",
        ".279"
      ],
      [
        "lemma",
        "<bold>.362</bold>",
        ".333",
        ".497",
        "<bold>.351</bold>",
        ".400"
      ],
      [
        "x + POS",
        ".354",
        "<bold>.336</bold>",
        ".504",
        ".345",
        "<bold>.406</bold>"
      ],
      [
        "* type",
        "-",
        "-",
        "-",
        ".339",
        ".277"
      ],
      [
        "* type MFit-A",
        "-",
        "-",
        "-",
        ".385",
        "-"
      ],
      [
        "* type MFit-AR",
        "-",
        "-",
        "-",
        ".439",
        ".381"
      ],
      [
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W",
        "Context: dep-W"
      ],
      [
        "type",
        ".366",
        ".365",
        ".489",
        ".362",
        ".314"
      ],
      [
        "x + POS",
        ".364",
        ".351",
        ".482",
        ".359",
        ".287"
      ],
      [
        "lemma",
        "<bold>.391</bold>",
        ".380",
        "<bold>.522</bold>",
        "<bold>.379</bold>",
        ".401"
      ],
      [
        "x + POS",
        ".384",
        "<bold>.388</bold>",
        ".480",
        ".366",
        "<bold>.431</bold>"
      ],
      [
        "* type",
        "-",
        "-",
        "-",
        ".376",
        ".313"
      ],
      [
        "* type MFit-AR",
        "-",
        "-",
        "-",
        ".434",
        ".418"
      ]
    ],
    "id": "cc099e12-c222-4508-992e-c442f61982cf",
    "claim": "Lemmatized targets generally do not perform better, with the boost being more pronounced on SimVerb.",
    "label": "refutes",
    "table_id": "c39ac67a-58a6-454e-be26-4c7a37808036"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "STS12",
      "STS13",
      "STS14",
      "STS15",
      "STS16"
    ],
    "table_content_values": [
      [
        "CBOW",
        "43.5",
        "[BOLD] 50.0",
        "[BOLD] 57.7",
        "[BOLD] 63.2",
        "61.0"
      ],
      [
        "CMOW",
        "39.2",
        "31.9",
        "38.7",
        "49.7",
        "52.2"
      ],
      [
        "Hybrid",
        "[BOLD] 49.6",
        "46.0",
        "55.1",
        "62.4",
        "[BOLD] 62.1"
      ],
      [
        "cmp. CBOW",
        "+14.6%",
        "-8%",
        "-4.5%",
        "-1.5%",
        "+1.8%"
      ],
      [
        "cmp. CMOW",
        "+26.5%",
        "+44.2%",
        "+42.4",
        "+25.6%",
        "+19.0%"
      ]
    ],
    "id": "af376ecf-fa75-4847-bbcc-3b39da24aa06",
    "claim": "The hybrid model is not able to repair this deficit, increasing the difference to 8%.",
    "label": "refutes",
    "table_id": "f3ae0058-2624-4cd3-a19a-389a8be5d740"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 4: Cue classification on the test set.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] F-Score  [BOLD] Baseline",
      "[BOLD] F-Score  [BOLD] Proposed",
      "[BOLD] Support"
    ],
    "table_content_values": [
      [
        "False cues",
        "0.61",
        "0.68",
        "47"
      ],
      [
        "Actual cues",
        "0.97",
        "0.98",
        "557"
      ]
    ],
    "id": "5d367238-2cda-4729-80eb-d27d1c716d89",
    "claim": "They are 553 true positives, 48 false positives, and 5 false negatives.",
    "label": "not enough info",
    "table_id": "08a8fe8c-92a2-41dc-a6e0-f97b95380cae"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "f21cc3b7-2533-4f2f-a259-ac3cd13583d2",
    "claim": "However, the main improvement of SER comes from training on cleaned data with up to 94% error reduction without the ranker and 97% with.11 just cleaning the training data has a much less dramatic effect than using a semantic control mechanism, such as the reranker (4.27% vs. 0.97% SER).",
    "label": "refutes",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "SA (S: 3 - M: 1)",
        "55.25",
        "} 0.082M"
      ],
      [
        "[BOLD] SA (S: 3 - B: 3)",
        "[BOLD] 55.42",
        "} 0.082M"
      ],
      [
        "SA (S: 3 - B: 4)",
        "55.33",
        "} 0.082M"
      ],
      [
        "SA (S: 3 - B: 6)",
        "55.31",
        "} 0.082M"
      ],
      [
        "SA (S: 3 - B: 1,3,5)",
        "55.45",
        "} 0.245M"
      ],
      [
        "[BOLD] SA (S: 3 - B: 2,4,6)",
        "[BOLD] 55.56",
        "} 0.245M"
      ]
    ],
    "id": "3a75d020-da89-4447-ab2b-ae91ec897986",
    "claim": "Though the improvement is slim, it is encouraging to continue researching into visual modulation",
    "label": "supports",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "89bb4f7f-01cd-4bda-a4ad-cc257857127b",
    "claim": "PPO agent obtains the highest ratio of successful turns, but GDPL outperforms other agents on SLU precision.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "ad70b00f-de99-4e50-88ae-fb0d5320778d",
    "claim": "Consequently, with an 8% i is substantially more linguistically informed than CBOW.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "c3ba1cac-9233-4b8e-9df7-50eb7d95b160",
    "claim": "the Pearson correlation coefficients in Table VI present the above-mentioned results with the cosine similarity scores used to compare two word embeddings.",
    "label": "not enough info",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
    "table_column_names": [
      "[EMPTY]",
      "Difference Function",
      "Seanad Abolition",
      "Video Games",
      "Pornography"
    ],
    "table_content_values": [
      [
        "OD-parse",
        "Absolute",
        "0.01",
        "-0.01",
        "0.07"
      ],
      [
        "OD-parse",
        "JS div.",
        "0.01",
        "-0.01",
        "-0.01"
      ],
      [
        "OD-parse",
        "EMD",
        "0.07",
        "0.01",
        "-0.01"
      ],
      [
        "OD",
        "Absolute",
        "[BOLD] 0.54",
        "[BOLD] 0.56",
        "[BOLD] 0.41"
      ],
      [
        "OD",
        "JS div.",
        "0.07",
        "-0.01",
        "-0.02"
      ],
      [
        "OD",
        "EMD",
        "0.26",
        "-0.01",
        "0.01"
      ],
      [
        "OD (no polarity shifters)",
        "Absolute",
        "0.23",
        "0.08",
        "0.04"
      ],
      [
        "OD (no polarity shifters)",
        "JS div.",
        "0.09",
        "-0.01",
        "-0.02"
      ],
      [
        "OD (no polarity shifters)",
        "EMD",
        "0.10",
        "0.01",
        "-0.01"
      ]
    ],
    "id": "82ec1cdd-1c49-4aaa-9764-e45b072fb22d",
    "claim": "[CONTINUE] Sentiment polarity shifters have a low impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" does not significantly hurt the Opinion Representation phase, and thereby does not lead to incorrect computation of opinion distance.",
    "label": "refutes",
    "table_id": "fba0fe3a-58cc-47a0-a3ab-5d091d2b7fe7"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
    "table_column_names": [
      "# of Heads",
      "Accuracy",
      "Val. Loss",
      "Effect"
    ],
    "table_content_values": [
      [
        "1",
        "89.44%",
        "0.2811",
        "-6.84%"
      ],
      [
        "2",
        "91.20%",
        "0.2692",
        "-5.08%"
      ],
      [
        "4",
        "93.85%",
        "0.2481",
        "-2.43%"
      ],
      [
        "8",
        "96.02%",
        "0.2257",
        "-0.26%"
      ],
      [
        "10",
        "96.28%",
        "0.2197",
        "[EMPTY]"
      ],
      [
        "16",
        "96.32%",
        "0.2190",
        "+0.04"
      ]
    ],
    "id": "435103bb-73be-4283-91b1-b429a3b988b7",
    "claim": "Using only one attention head, thereby attending to only one context position at once, degrades the performance to less than the performance of 10 heads using the standard finetuning scheme.",
    "label": "supports",
    "table_id": "34083185-d444-4bd0-994e-d13f5afe8e82"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "c677c0e0-d305-42a3-b263-5626bd417d4a",
    "claim": "One reason is that when the reference action sequence is long, the probability of all actions being correct decreases.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "2aad1e34-9d55-4236-849e-9826c3e92730",
    "claim": "As for Success metric, some ambiguous start location can cause low score.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "e3b3b4b9-da35-4fa1-b2ca-bd86c3df2677",
    "claim": "[CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) does not outperform GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings does not outperform sentence embeddings alone, and does not exceed the upper boundary of BoC feature, in which again demonstrating the lack of competitiveness of BoC feature.",
    "label": "refutes",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "3de653b7-7800-41e0-8431-4f7ea3574f5d",
    "claim": "When increasing the number of terms to 10,000, the DocSub models using Europarl corpora performed better than when using TED Talks corpora.",
    "label": "supports",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "546f1892-7648-4fd4-a1e2-35ecc35cd23a",
    "claim": "humans do poorly on hard instances, which requires deeper inference rather than surface cues, and neural language models largely overcome this difficulty.",
    "label": "not enough info",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "On the difficulty of a distributional semantics of spoken language",
    "paper_id": "1803.08869v2",
    "table_caption": "Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
    "table_column_names": [
      "[EMPTY]",
      "Recall@10 (%)",
      "Median rank",
      "RSAimage"
    ],
    "table_content_values": [
      [
        "VGS",
        "27",
        "6",
        "0.4"
      ],
      [
        "SegMatch",
        "[BOLD] 10",
        "[BOLD] 37",
        "[BOLD] 0.5"
      ],
      [
        "Audio2vec-U",
        "5",
        "105",
        "0.0"
      ],
      [
        "Audio2vec-C",
        "2",
        "647",
        "0.0"
      ],
      [
        "Mean MFCC",
        "1",
        "1,414",
        "0.0"
      ],
      [
        "Chance",
        "0",
        "3,955",
        "0.0"
      ]
    ],
    "id": "7e8e2c4f-92ea-4c80-a27c-84cae2005a80",
    "claim": "SegMatch works slightly better than Audio2vec according to both criteria.",
    "label": "refutes",
    "table_id": "087b26ab-9679-4d8d-b96c-57140c1a8b7b"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 1: The scores of our three submitted runs for similarity threshold 50%.",
    "table_column_names": [
      "Run ID",
      "Official score",
      "Score with correction"
    ],
    "table_content_values": [
      [
        "ep_1",
        "60.29",
        "66.76"
      ],
      [
        "ep_2",
        "[BOLD] 60.90",
        "[BOLD] 67.35"
      ],
      [
        "ep_3",
        "60.61",
        "67.07"
      ]
    ],
    "id": "85a0c565-04f4-4ff4-9b4f-9f8c0e1be995",
    "claim": "The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] However, re-scoring our second submission after replacing these 10 files with the ones from our first submission resulted in a lower score of 67.07%.",
    "label": "refutes",
    "table_id": "c177a61b-e06b-43f7-b2e7-44e672b1956b"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "37504e87-a51e-4d90-bc49-27f562c1784e",
    "claim": "All G2S models have [CONTINUE] higher entailment compared to S2S.",
    "label": "supports",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] R-1",
      "[BOLD] R-2",
      "[BOLD] R-SU"
    ],
    "table_content_values": [
      [
        "First-1",
        "26.83",
        "7.25",
        "6.46"
      ],
      [
        "First-2",
        "35.99",
        "10.17",
        "12.06"
      ],
      [
        "First-3",
        "39.41",
        "11.77",
        "14.51"
      ],
      [
        "LexRank Erkan and Radev ( 2004 )",
        "38.27",
        "12.70",
        "13.20"
      ],
      [
        "TextRank Mihalcea and Tarau ( 2004 )",
        "38.44",
        "13.10",
        "13.50"
      ],
      [
        "MMR Carbonell and Goldstein ( 1998 )",
        "38.77",
        "11.98",
        "12.91"
      ],
      [
        "PG-Original Lebanoff et al. ( 2018 )",
        "41.85",
        "12.91",
        "16.46"
      ],
      [
        "PG-MMR Lebanoff et al. ( 2018 )",
        "40.55",
        "12.36",
        "15.87"
      ],
      [
        "PG-BRNN Gehrmann et al. ( 2018 )",
        "42.80",
        "14.19",
        "16.75"
      ],
      [
        "CopyTransformer Gehrmann et al. ( 2018 )",
        "[BOLD] 43.57",
        "14.03",
        "17.37"
      ],
      [
        "Hi-MAP (Our Model)",
        "43.47",
        "[BOLD] 14.89",
        "[BOLD] 17.41"
      ]
    ],
    "id": "6ff4558a-e94b-4bf7-88fc-ab7a7f88c1fa",
    "claim": "Our model does not outperform PG-MMR when trained and tested on the Multi-News dataset.",
    "label": "refutes",
    "table_id": "6f86cef3-2ee0-4782-b5b4-f37400c22dfb"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "6200a720-4ce2-40ec-a91f-ad7f8ddfd37c",
    "claim": "Compared to CMOW, the hybrid model shows significant differences.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
    "table_column_names": [
      "[BOLD] Language pair",
      "[BOLD] Model type",
      "[BOLD] Oracle model",
      "[BOLD] Decoder configuration  [BOLD] Uniform",
      "[BOLD] Decoder configuration  [BOLD] BI + IS"
    ],
    "table_content_values": [
      [
        "es-en",
        "Unadapted",
        "36.4",
        "34.7",
        "36.6"
      ],
      [
        "es-en",
        "No-reg",
        "36.6",
        "34.8",
        "-"
      ],
      [
        "es-en",
        "EWC",
        "37.0",
        "36.3",
        "[BOLD] 37.2"
      ],
      [
        "en-de",
        "Unadapted",
        "36.4",
        "26.8",
        "38.8"
      ],
      [
        "en-de",
        "No-reg",
        "41.7",
        "31.8",
        "-"
      ],
      [
        "en-de",
        "EWC",
        "42.1",
        "38.6",
        "[BOLD] 42.0"
      ]
    ],
    "id": "b3d0a10d-235d-4eb2-bede-f573f3a44812",
    "claim": "BI+IS decoding with single-domain trained models does not achieve gains over both the naive uniform approach and over oracle single-domain models.",
    "label": "refutes",
    "table_id": "a02493f5-adad-4462-a582-8a2cfe6f431d"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "295d9f73-4ef3-403e-928f-c6924a9ff349",
    "claim": "The models in the upper portion (1-6) use only dialogue history and turn-level user goals, which are assumed to be error-free.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "34c54516-c7a3-4764-9a4f-5877b980437c",
    "claim": "The first subset contains results by our system, second subset contains results by Refresh, and third subset contains results by ExtAbsRL.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "d977923d-c6e5-4ad6-8a6d-b142c7757162",
    "claim": "Support Vector Machines (SVM) were used as baseline and the results of other proposed methods have been compared with them.",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
    "table_column_names": [
      "[BOLD] Decoder configuration",
      "[BOLD] es-en  [BOLD] Health",
      "[BOLD] es-en  [BOLD] Bio",
      "[BOLD] en-de  [BOLD] News",
      "[BOLD] en-de  [BOLD] TED",
      "[BOLD] en-de  [BOLD] IT"
    ],
    "table_content_values": [
      [
        "Oracle model",
        "35.9",
        "36.1",
        "37.8",
        "24.1",
        "39.6"
      ],
      [
        "Uniform",
        "33.1",
        "36.4",
        "21.9",
        "18.4",
        "38.9"
      ],
      [
        "Identity-BI",
        "35.0",
        "36.6",
        "32.7",
        "25.3",
        "42.6"
      ],
      [
        "BI",
        "35.9",
        "36.5",
        "38.0",
        "26.1",
        "[BOLD] 44.7"
      ],
      [
        "IS",
        "[BOLD] 36.0",
        "36.8",
        "37.5",
        "25.6",
        "43.3"
      ],
      [
        "BI + IS",
        "[BOLD] 36.0",
        "[BOLD] 36.9",
        "[BOLD] 38.4",
        "[BOLD] 26.4",
        "[BOLD] 44.7"
      ]
    ],
    "id": "a3e27840-583c-4bc7-a60e-1acc2790dea2",
    "claim": "Table 5 shows that uniform ensembling outperforms all oracle models except es-en Bio, especially on general domains.",
    "label": "refutes",
    "table_id": "f27c1026-3ef4-450e-97a8-368ddf0d0848"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "ec1c8929-3671-4e7f-8738-bf6ed393975e",
    "claim": "The results show that it is better to add knowledge as features when the knowledge quality is high than compile them into constraints.",
    "label": "refutes",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
    "table_column_names": [
      "[BOLD] Model",
      "D",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN(1)",
        "300",
        "10.9M",
        "20.9",
        "52.0"
      ],
      [
        "DCGCN(2)",
        "180",
        "10.9M",
        "[BOLD] 22.2",
        "[BOLD] 52.3"
      ],
      [
        "DCGCN(2)",
        "240",
        "11.3M",
        "22.8",
        "52.8"
      ],
      [
        "DCGCN(4)",
        "180",
        "11.4M",
        "[BOLD] 23.4",
        "[BOLD] 53.4"
      ],
      [
        "DCGCN(1)",
        "420",
        "12.6M",
        "22.2",
        "52.4"
      ],
      [
        "DCGCN(2)",
        "300",
        "12.5M",
        "23.8",
        "53.8"
      ],
      [
        "DCGCN(3)",
        "240",
        "12.3M",
        "[BOLD] 23.9",
        "[BOLD] 54.1"
      ],
      [
        "DCGCN(2)",
        "360",
        "14.0M",
        "24.2",
        "[BOLD] 54.4"
      ],
      [
        "DCGCN(3)",
        "300",
        "14.0M",
        "[BOLD] 24.4",
        "54.2"
      ],
      [
        "DCGCN(2)",
        "420",
        "15.6M",
        "24.1",
        "53.7"
      ],
      [
        "DCGCN(4)",
        "300",
        "15.6M",
        "[BOLD] 24.6",
        "[BOLD] 54.8"
      ],
      [
        "DCGCN(3)",
        "420",
        "18.6M",
        "24.5",
        "54.6"
      ],
      [
        "DCGCN(4)",
        "360",
        "18.4M",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "5879912f-a133-4644-b984-422b306d3d34",
    "claim": "In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones.",
    "label": "supports",
    "table_id": "135bc50f-f12e-493b-854d-58858c4c5c86"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
    "table_column_names": [
      "[EMPTY]",
      "Acc",
      "Sim",
      "PP",
      "GM"
    ],
    "table_content_values": [
      [
        "M0: shen-1",
        "0.694",
        "0.728",
        "[BOLD] 22.3",
        "8.81"
      ],
      [
        "M1: M0 [ITALIC] +para",
        "0.702",
        "0.747",
        "23.6",
        "11.7"
      ],
      [
        "M2: M0 [ITALIC] +cyc",
        "0.692",
        "0.781",
        "49.9",
        "[BOLD] 12.8"
      ],
      [
        "M3: M0 [ITALIC] +cyc+lang",
        "0.698",
        "0.754",
        "39.2",
        "12.0"
      ],
      [
        "M4: M0 [ITALIC] +cyc+para",
        "0.702",
        "0.757",
        "33.9",
        "[BOLD] 12.8"
      ],
      [
        "M5: M0 [ITALIC] +cyc+para+lang",
        "0.688",
        "0.753",
        "28.6",
        "11.8"
      ],
      [
        "M6: M0 [ITALIC] +cyc+2d",
        "0.704",
        "[BOLD] 0.794",
        "63.2",
        "[BOLD] 12.8"
      ],
      [
        "M7: M6+ [ITALIC] para+lang",
        "0.706",
        "0.768",
        "49.0",
        "[BOLD] 12.8"
      ]
    ],
    "id": "08c3b235-a0d2-41d2-a7f1-046b2f03f269",
    "claim": "[CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation.",
    "label": "refutes",
    "table_id": "db289154-f4e4-4abd-b1f8-f8df8da1b2de"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
    "table_column_names": [
      "[BOLD] Decoder configuration",
      "[BOLD] es-en  [BOLD] Health",
      "[BOLD] es-en  [BOLD] Bio",
      "[BOLD] en-de  [BOLD] News",
      "[BOLD] en-de  [BOLD] TED",
      "[BOLD] en-de  [BOLD] IT"
    ],
    "table_content_values": [
      [
        "Oracle model",
        "35.9",
        "36.1",
        "37.8",
        "24.1",
        "39.6"
      ],
      [
        "Uniform",
        "33.1",
        "36.4",
        "21.9",
        "18.4",
        "38.9"
      ],
      [
        "Identity-BI",
        "35.0",
        "36.6",
        "32.7",
        "25.3",
        "42.6"
      ],
      [
        "BI",
        "35.9",
        "36.5",
        "38.0",
        "26.1",
        "[BOLD] 44.7"
      ],
      [
        "IS",
        "[BOLD] 36.0",
        "36.8",
        "37.5",
        "25.6",
        "43.3"
      ],
      [
        "BI + IS",
        "[BOLD] 36.0",
        "[BOLD] 36.9",
        "[BOLD] 38.4",
        "[BOLD] 26.4",
        "[BOLD] 44.7"
      ]
    ],
    "id": "dfc949b8-d660-4336-b9bd-4cd308f74cd4",
    "claim": "BI and IS both individually outperform the oracle for all but IS-News, [CONTINUE] With adaptive decoding, we do not need to assume whether a uniform ensemble or a single model might perform better for some potentially unknown domain.",
    "label": "supports",
    "table_id": "f27c1026-3ef4-450e-97a8-368ddf0d0848"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "2861b4ef-ab1a-4f84-b5ff-37cfcd01f477",
    "claim": "Despite the models having fewer examples of bigger graphs to learn from, this does not lead to worse performance when handling graphs with higher diameters.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "f355495c-ff8d-491e-9025-38c4710543de",
    "claim": "the main challenge of the sentiment classification task is to extract the information from the context.",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "952269e3-91ba-422b-9157-7a84243d785f",
    "claim": "Analyzing Table 3, we can observe that all values of precision using the English corpora have higher scores when compared with the Portuguese corpora.",
    "label": "refutes",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "1cff5f89-184c-479e-a5ff-7c03244c4ab8",
    "claim": "The contribution of the cue is clear: in particular, the relatively low precision of using the parser introduces more out of scope relations than in-scope.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
    "table_column_names": [
      "[BOLD] Representation",
      "[BOLD] Hyper parameters Filter size",
      "[BOLD] Hyper parameters Num. Feature maps",
      "[BOLD] Hyper parameters Activation func.",
      "[BOLD] Hyper parameters L2 Reg.",
      "[BOLD] Hyper parameters Learning rate",
      "[BOLD] Hyper parameters Dropout Prob.",
      "[BOLD] F1.(avg. in 5-fold) with default values",
      "[BOLD] F1.(avg. in 5-fold) with optimal values"
    ],
    "table_content_values": [
      [
        "CoNLL08",
        "4-5",
        "1000",
        "Softplus",
        "1.15e+01",
        "1.13e-03",
        "1",
        "73.34",
        "74.49"
      ],
      [
        "SB",
        "4-5",
        "806",
        "Sigmoid",
        "8.13e-02",
        "1.79e-03",
        "0.87",
        "72.83",
        "[BOLD] 75.05"
      ],
      [
        "UD v1.3",
        "5",
        "716",
        "Softplus",
        "1.66e+00",
        "9.63E-04",
        "1",
        "68.93",
        "69.57"
      ]
    ],
    "id": "b9d6270e-87be-44f9-982a-a037c7cfd0bd",
    "claim": "The results furthermore show that the sdps based on the Stanford Basic (SB) representation do not provide the best performance, followed by the CoNLL08 representation.",
    "label": "refutes",
    "table_id": "602023a1-3ab5-4788-8101-59ae8f0c6ce1"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
    "table_column_names": [
      "[BOLD] Emoji alias",
      "[BOLD] N",
      "[BOLD] emoji #",
      "[BOLD] emoji %",
      "[BOLD] no-emoji #",
      "[BOLD] no-emoji %",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "mask",
        "163",
        "154",
        "94.48",
        "134",
        "82.21",
        "- 12.27"
      ],
      [
        "two_hearts",
        "87",
        "81",
        "93.10",
        "77",
        "88.51",
        "- 4.59"
      ],
      [
        "heart_eyes",
        "122",
        "109",
        "89.34",
        "103",
        "84.43",
        "- 4.91"
      ],
      [
        "heart",
        "267",
        "237",
        "88.76",
        "235",
        "88.01",
        "- 0.75"
      ],
      [
        "rage",
        "92",
        "78",
        "84.78",
        "66",
        "71.74",
        "- 13.04"
      ],
      [
        "cry",
        "116",
        "97",
        "83.62",
        "83",
        "71.55",
        "- 12.07"
      ],
      [
        "sob",
        "490",
        "363",
        "74.08",
        "345",
        "70.41",
        "- 3.67"
      ],
      [
        "unamused",
        "167",
        "121",
        "72.46",
        "116",
        "69.46",
        "- 3.00"
      ],
      [
        "weary",
        "204",
        "140",
        "68.63",
        "139",
        "68.14",
        "- 0.49"
      ],
      [
        "joy",
        "978",
        "649",
        "66.36",
        "629",
        "64.31",
        "- 2.05"
      ],
      [
        "sweat_smile",
        "111",
        "73",
        "65.77",
        "75",
        "67.57",
        "1.80"
      ],
      [
        "confused",
        "77",
        "46",
        "59.74",
        "48",
        "62.34",
        "2.60"
      ]
    ],
    "id": "48134843-e7f0-4987-98fd-e830f58e1b3a",
    "claim": "[CONTINUE] When removing sweat smile and confused accuracy increased,",
    "label": "supports",
    "table_id": "2410b1c3-4d48-49e9-9279-dc23daa879d1"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "02bcff71-1cf2-4b28-9482-a7c0e3cab06a",
    "claim": "The comparison shows the powerful advantage of LSTM embeddings over the standard word embeddings in capturing word semantics, that is, semantic similarity.",
    "label": "not enough info",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "991b8ed1-5e3e-408a-9da7-915c600a7d0c",
    "claim": "These results demonstrate that NeuralTDabt indeed learns to generate non-extractive summaries and performs better than a regular extractive baseline, which randomly select sentences from the given document.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "dacd4e41-a392-455b-856c-0a23df7ceab8",
    "claim": "by comparing it to extractive baseline NeuralTD, our proposed abstractive model NeuralTDabt exhibits better performance than extractive baseline, improving 0.9 ROUGE-1 points and 0.3 ROUGE-L points.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 2: Experiment 1",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.001",
        "0.003",
        "-20.818",
        "***",
        "0.505"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.083",
        "0.048",
        "101.636",
        "***",
        "1.724"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.001",
        "0.001",
        "0.035",
        "[EMPTY]",
        "1.001"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.023",
        "0.012",
        "64.418",
        "***",
        "1.993"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.002",
        "0.001",
        "4.047",
        "***",
        "1.120"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.049",
        "0.019",
        "120.986",
        "***",
        "2.573"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.173",
        "0.065",
        "243.285",
        "***",
        "2.653"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.032",
        "0.023",
        "39.483",
        "***",
        "1.396"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.111",
        "0.061",
        "122.707",
        "***",
        "1.812"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.178",
        "0.080",
        "211.319",
        "***",
        "2.239"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.028",
        "0.015",
        "63.131",
        "***",
        "1.854"
      ]
    ],
    "id": "84aa63b6-4dad-4cc3-885a-d5d43259dae9",
    "claim": "For Waseem (2016) we see that there is a significant difference in the estimated rates at which tweets are classified as racist across groups, with higher rates for the white group.",
    "label": "refutes",
    "table_id": "661ddd6e-51d9-446c-add1-3e80f3b78109"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "84e4fdca-7ec9-45af-a79e-d7937dbea06f",
    "claim": "This can be attributed to the fact that the proposed approach relies on more than one concept words, while GloVe only uses the representation of the top concept word to classify the image.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] External",
      "B"
    ],
    "table_content_values": [
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "-",
        "22.0"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "-",
        "23.3"
      ],
      [
        "GCNSEQ (Damonte and Cohen,  2019 )",
        "-",
        "24.4"
      ],
      [
        "DCGCN(single)",
        "-",
        "25.9"
      ],
      [
        "DCGCN(ensemble)",
        "-",
        "[BOLD] 28.2"
      ],
      [
        "TSP (Song et al.,  2016 )",
        "ALL",
        "22.4"
      ],
      [
        "PBMT (Pourdamghani et al.,  2016 )",
        "ALL",
        "26.9"
      ],
      [
        "Tree2Str (Flanigan et al.,  2016 )",
        "ALL",
        "23.0"
      ],
      [
        "SNRG (Song et al.,  2017 )",
        "ALL",
        "25.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "0.2M",
        "27.4"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "0.2M",
        "28.2"
      ],
      [
        "DCGCN(single)",
        "0.1M",
        "29.0"
      ],
      [
        "DCGCN(single)",
        "0.2M",
        "[BOLD] 31.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "2M",
        "32.3"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "2M",
        "33.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "20M",
        "33.8"
      ],
      [
        "DCGCN(single)",
        "0.3M",
        "33.2"
      ],
      [
        "DCGCN(ensemble)",
        "0.3M",
        "[BOLD] 35.3"
      ]
    ],
    "id": "7b320190-e9bb-4603-8a20-b4a8f2c7fdbc",
    "claim": "DCGCN model is not able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a higher score of 33.6 by using 2M data and Seq2SeqK achieves an even higher score of 33.8 by using 20M data.",
    "label": "refutes",
    "table_id": "f7b025d4-ffc5-4764-9949-312c3463da35"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "4fe8ff2e-6610-4a8b-905b-0d92bebf29e1",
    "claim": "GGP-MBCM performs best in model 1, but is significantly worse than the other policies in models 2 and 3.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "423aa89a-9bca-4bf2-a8aa-78154555b63b",
    "claim": "For example, a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21.2% of COPA training instances.",
    "label": "supports",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "855a3b67-9407-44f3-89a7-0e9ca26da983",
    "claim": "However, the KL divergence between human dialog policy and RL agents policy is quite large, which means the training has more space to improve.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "88054a7a-5435-4928-9448-8ae40b53e01b",
    "claim": "we use the sum of the similarities between the question and the two sentences in the passage as a memory initialization",
    "label": "not enough info",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "7f397cbf-19ef-4fd7-a5e0-22e8af5690f9",
    "claim": "For example, a chatbot that generates a confusing or inappropriate response should be assigned a low efficiency score.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "c7dbd5dd-6bea-46f7-a832-677c3004a5da",
    "claim": "the performance of GloVe and Word2vec remain unchanged if concept words are unseen in the training corpus.",
    "label": "not enough info",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Keyphrase Generation for Scientific Articles using GANs",
    "paper_id": "1909.12229v1",
    "table_caption": "Table 2: α-nDCG@5 metrics",
    "table_column_names": [
      "Model",
      "Inspec",
      "Krapivin",
      "NUS",
      "KP20k"
    ],
    "table_content_values": [
      [
        "Catseq",
        "0.87803",
        "0.781",
        "0.82118",
        "0.804"
      ],
      [
        "Catseq-RL",
        "0.8602",
        "[BOLD] 0.786",
        "0.83",
        "0.809"
      ],
      [
        "GAN",
        "[BOLD] 0.891",
        "0.771",
        "[BOLD] 0.853",
        "[BOLD] 0.85"
      ]
    ],
    "id": "deefe413-93b4-46a5-915c-e5ff31c48ab8",
    "claim": "Our model does not obtain the best performance on three out of the four datasets.",
    "label": "refutes",
    "table_id": "db0cb6ad-ade2-4926-8a07-f92a9d74c055"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 5: Results of the Human Rating on CWC.",
    "table_column_names": [
      "System",
      "Succ. (%)",
      "Smoothness"
    ],
    "table_content_values": [
      [
        "Retrieval-Stgy ",
        "54.0",
        "2.48"
      ],
      [
        "PMI ",
        "46.0",
        "2.56"
      ],
      [
        "Neural ",
        "36.0",
        "2.50"
      ],
      [
        "Kernel ",
        "58.0",
        "2.48"
      ],
      [
        "DKRN (ours)",
        "[BOLD] 88.0",
        "[BOLD] 3.22"
      ]
    ],
    "id": "6c9eeac3-a181-46e5-9b58-03aece5eb01a",
    "claim": "All other agents outperform our DKRN agent with a large margin.",
    "label": "refutes",
    "table_id": "72fa0d72-3d74-45f3-a8d4-df9ee3f08663"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "57009ccc-b37d-4ba1-a3bf-60fbc2ac3dfc",
    "claim": "However, this alone cannot improve system-level ROUGE to the level that of the ROUGE-based decoder.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 1: The scores of our three submitted runs for similarity threshold 50%.",
    "table_column_names": [
      "Run ID",
      "Official score",
      "Score with correction"
    ],
    "table_content_values": [
      [
        "ep_1",
        "60.29",
        "66.76"
      ],
      [
        "ep_2",
        "[BOLD] 60.90",
        "[BOLD] 67.35"
      ],
      [
        "ep_3",
        "60.61",
        "67.07"
      ]
    ],
    "id": "e0687e8f-e6c6-43ea-854d-2257d5ec9015",
    "claim": "The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] Therefore, we report both the official score (from our second submission) and the result of re-scoring our second submission after replacing these 10 files with the ones from our first submission.",
    "label": "supports",
    "table_id": "c177a61b-e06b-43f7-b2e7-44e672b1956b"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 2: Image-caption ranking results for German (Multi30k)",
    "table_column_names": [
      "[EMPTY]",
      "Image to Text R@1",
      "Image to Text R@5",
      "Image to Text R@10",
      "Image to Text Mr",
      "Text to Image R@1",
      "Text to Image R@5",
      "Text to Image R@10",
      "Text to Image Mr",
      "Alignment"
    ],
    "table_content_values": [
      [
        "[BOLD] symmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Parallel gella:17",
        "28.2",
        "57.7",
        "71.3",
        "4",
        "20.9",
        "46.9",
        "59.3",
        "6",
        "-"
      ],
      [
        "Mono",
        "34.2",
        "67.5",
        "79.6",
        "3",
        "26.5",
        "54.7",
        "66.2",
        "4",
        "-"
      ],
      [
        "FME",
        "36.8",
        "69.4",
        "80.8",
        "2",
        "26.6",
        "56.2",
        "68.5",
        "4",
        "76.81%"
      ],
      [
        "AME",
        "[BOLD] 39.6",
        "[BOLD] 72.7",
        "[BOLD] 82.7",
        "[BOLD] 2",
        "[BOLD] 28.9",
        "[BOLD] 58.0",
        "[BOLD] 68.7",
        "[BOLD] 4",
        "66.91%"
      ],
      [
        "[BOLD] asymmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Pivot gella:17",
        "28.2",
        "61.9",
        "73.4",
        "3",
        "22.5",
        "49.3",
        "61.7",
        "6",
        "-"
      ],
      [
        "Parallel gella:17",
        "30.2",
        "60.4",
        "72.8",
        "3",
        "21.8",
        "50.5",
        "62.3",
        "5",
        "-"
      ],
      [
        "Mono",
        "[BOLD] 42.0",
        "72.5",
        "83.0",
        "2",
        "29.6",
        "58.4",
        "69.6",
        "4",
        "-"
      ],
      [
        "FME",
        "40.5",
        "73.3",
        "83.4",
        "2",
        "29.6",
        "59.2",
        "[BOLD] 72.1",
        "3",
        "76.81%"
      ],
      [
        "AME",
        "40.5",
        "[BOLD] 74.3",
        "[BOLD] 83.4",
        "[BOLD] 2",
        "[BOLD] 31.0",
        "[BOLD] 60.5",
        "70.6",
        "[BOLD] 3",
        "73.10%"
      ]
    ],
    "id": "8e604b9f-d3fa-4a47-bd09-06cd2caf18c1",
    "claim": "For German descriptions, The results are 11.05% better on average compared to (Gella et al., 2017) in symmetric mode.",
    "label": "supports",
    "table_id": "abdcea15-c5a5-40cd-91fe-ebc83872eccc"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "bd3a799a-c562-4476-ad82-23a43d147582",
    "claim": "The proposed CNN-LSTMOur-neg-Ant does not improve upon the simple CNNLSTM-w/o neg.",
    "label": "refutes",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "3085b0e6-eb83-487a-bb86-0533b9fe919b",
    "claim": "When using more natural language text as an additional training resource, the models’ performance is improved dramatically, outperforming the previous state-of-the-art by 10 absolute points.",
    "label": "not enough info",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "22787daa-80c1-4204-aced-20a547c65df4",
    "claim": "[CONTINUE] It can be observed that the learned reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise.",
    "label": "supports",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "d7e9fa5f-41af-46a2-b8e9-34424334ea6f",
    "claim": "On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the least distinctive features for a tweet not being labeled as a complaint.",
    "label": "refutes",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "0a742890-8fda-45d8-b92c-504a7c2153a8",
    "claim": "For all these systems, a three-sentence summarisation is required; so we set T=3 in our experiment.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).",
    "table_column_names": [
      "Model",
      "#Params",
      "Base",
      "+Elmo"
    ],
    "table_content_values": [
      [
        "rnet*",
        "-",
        "71.1/79.5",
        "-/-"
      ],
      [
        "LSTM",
        "2.67M",
        "[BOLD] 70.46/78.98",
        "75.17/82.79"
      ],
      [
        "GRU",
        "2.31M",
        "70.41/ [BOLD] 79.15",
        "75.81/83.12"
      ],
      [
        "ATR",
        "1.59M",
        "69.73/78.70",
        "75.06/82.76"
      ],
      [
        "SRU",
        "2.44M",
        "69.27/78.41",
        "74.56/82.50"
      ],
      [
        "LRN",
        "2.14M",
        "70.11/78.83",
        "[BOLD] 76.14/ [BOLD] 83.83"
      ]
    ],
    "id": "8b7d76ba-3bdd-45c0-9afe-72d3024ef13d",
    "claim": "Table 4 shows that LRN has the highest EM/F1 score.",
    "label": "refutes",
    "table_id": "e8718aba-7d2a-43da-ab9f-fe42d951b94a"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "2c9c4e5c-d05b-490f-8859-111e19020af5",
    "claim": "Note that GloVe is the pre-trained word vectors in the very basic representation.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "ac541d7d-74e8-4def-b326-4c2e2c8fc6bd",
    "claim": "Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models.",
    "label": "supports",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Parameters",
      "[BOLD] Validation AUC@0.05",
      "[BOLD] Test AUC@0.05"
    ],
    "table_content_values": [
      [
        "Base",
        "8.0M",
        "[BOLD] 0.871",
        "0.816"
      ],
      [
        "4L SRU → 2L LSTM",
        "7.3M",
        "0.864",
        "[BOLD] 0.829"
      ],
      [
        "4L SRU → 2L SRU",
        "7.8M",
        "0.856",
        "[BOLD] 0.829"
      ],
      [
        "Flat → hierarchical",
        "12.4M",
        "0.825",
        "0.559"
      ],
      [
        "Cross entropy → hinge loss",
        "8.0M",
        "0.765",
        "0.693"
      ],
      [
        "6.6M → 1M examples",
        "8.0M",
        "0.835",
        "0.694"
      ],
      [
        "6.6M → 100K examples",
        "8.0M",
        "0.565",
        "0.417"
      ],
      [
        "200 → 100 negatives",
        "8.0M",
        "0.864",
        "0.647"
      ],
      [
        "200 → 10 negatives",
        "8.0M",
        "0.720",
        "0.412"
      ]
    ],
    "id": "bab35b27-fc3c-4a34-802c-79f633a9de4f",
    "claim": "The model performs significantly worse when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function.",
    "label": "supports",
    "table_id": "5efab962-70b6-488d-9601-cbd5b4852a40"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "61264049-8c30-45ff-9a15-8903df2d4727",
    "claim": "the results show that InferSent yields the highest correlation between METEOR and human evaluation, in both ρ and r. However, we see that InferSent has the lowest precision on the “good” summaries and the highest precision on the “bad” summaries",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer aroma+palate",
        "Beer look",
        "74.41",
        "74.83",
        "74.94",
        "72.75",
        "76.41",
        "[BOLD] 79.53",
        "80.29"
      ],
      [
        "Beer look+palate",
        "Beer aroma",
        "68.57",
        "69.23",
        "67.55",
        "69.92",
        "76.45",
        "[BOLD] 77.94",
        "78.11"
      ],
      [
        "Beer look+aroma",
        "Beer palate",
        "63.88",
        "67.82",
        "65.72",
        "74.66",
        "73.40",
        "[BOLD] 75.24",
        "75.50"
      ]
    ],
    "id": "582f8cd2-36bc-478b-a34b-95e07733d714",
    "claim": "We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) outperforms ORACLE in all aspects.",
    "label": "refutes",
    "table_id": "461a04b0-4d80-4f54-a0ed-42b74709e562"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] External",
      "B"
    ],
    "table_content_values": [
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "-",
        "22.0"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "-",
        "23.3"
      ],
      [
        "GCNSEQ (Damonte and Cohen,  2019 )",
        "-",
        "24.4"
      ],
      [
        "DCGCN(single)",
        "-",
        "25.9"
      ],
      [
        "DCGCN(ensemble)",
        "-",
        "[BOLD] 28.2"
      ],
      [
        "TSP (Song et al.,  2016 )",
        "ALL",
        "22.4"
      ],
      [
        "PBMT (Pourdamghani et al.,  2016 )",
        "ALL",
        "26.9"
      ],
      [
        "Tree2Str (Flanigan et al.,  2016 )",
        "ALL",
        "23.0"
      ],
      [
        "SNRG (Song et al.,  2017 )",
        "ALL",
        "25.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "0.2M",
        "27.4"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "0.2M",
        "28.2"
      ],
      [
        "DCGCN(single)",
        "0.1M",
        "29.0"
      ],
      [
        "DCGCN(single)",
        "0.2M",
        "[BOLD] 31.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "2M",
        "32.3"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "2M",
        "33.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "20M",
        "33.8"
      ],
      [
        "DCGCN(single)",
        "0.3M",
        "33.2"
      ],
      [
        "DCGCN(ensemble)",
        "0.3M",
        "[BOLD] 35.3"
      ]
    ],
    "id": "7a24d9e2-8e4f-4318-83fc-7fdad95942a0",
    "claim": "When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM.",
    "label": "supports",
    "table_id": "f7b025d4-ffc5-4764-9949-312c3463da35"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "2bd4f990-c9ea-44f0-a468-c6d4f6ff7ae5",
    "claim": "That is, the agent is informative and successful but forgets to ask what type of food users want to order occasionally.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 7: Scores for initialization strategies on probing tasks.",
    "table_column_names": [
      "Initialization",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "N(0,0.1)",
        "29.7",
        "71.5",
        "82.0",
        "78.5",
        "60.1",
        "80.5",
        "76.3",
        "74.7",
        "[BOLD] 51.3",
        "52.5"
      ],
      [
        "Glorot",
        "31.3",
        "[BOLD] 72.3",
        "81.8",
        "78.7",
        "59.4",
        "81.3",
        "76.6",
        "[BOLD] 74.6",
        "50.4",
        "57.0"
      ],
      [
        "Our paper",
        "[BOLD] 35.1",
        "70.8",
        "[BOLD] 82.0",
        "[BOLD] 80.2",
        "[BOLD] 61.8",
        "[BOLD] 82.8",
        "[BOLD] 79.7",
        "74.2",
        "50.7",
        "[BOLD] 72.9"
      ]
    ],
    "id": "51729d04-4ea1-4472-8452-f5cb29bddfdc",
    "claim": "While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is improved by a wide [CONTINUE] margin by our initialization strategy.",
    "label": "supports",
    "table_id": "180ff878-5e26-496d-9b71-ff1aeb454328"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "[BOLD] Baselines",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen ( 2015a )",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. ( 2018 )",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "[BOLD] Model Variants",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "ba7d16da-1cc2-4350-aac4-6bc6e31ada78",
    "claim": "The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model.",
    "label": "refutes",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 3: Performance comparison of our model with different values of m on the two datasets.",
    "table_column_names": [
      "[ITALIC] m",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "1",
        "0.541",
        "0.595",
        "[BOLD] 0.566",
        "0.495",
        "0.621",
        "0.551"
      ],
      [
        "2",
        "0.521",
        "0.597",
        "0.556",
        "0.482",
        "0.656",
        "0.555"
      ],
      [
        "3",
        "0.490",
        "0.617",
        "0.547",
        "0.509",
        "0.633",
        "0.564"
      ],
      [
        "4",
        "0.449",
        "0.623",
        "0.522",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "5",
        "0.467",
        "0.609",
        "0.529",
        "0.488",
        "0.677",
        "0.567"
      ]
    ],
    "id": "4df10533-fe4d-4bca-a7ae-e944285f1f9a",
    "claim": "These experiments show that the number of factors giving the best performance may vary depending on the underlying data distribution.",
    "label": "supports",
    "table_id": "fbd16514-f1a6-4567-8557-22b8d673c5b6"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "08e785fd-5276-4bfa-89cb-743853a254f3",
    "claim": "The results in Table 3 show that translation quality of LRN is significantly worse than that of GRU (-0.57 BLEU).",
    "label": "refutes",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.",
    "table_column_names": [
      "[ITALIC] k",
      "Ar",
      "Es",
      "Fr",
      "Ru",
      "Zh",
      "En"
    ],
    "table_content_values": [
      [
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy"
      ],
      [
        "0",
        "88.0",
        "87.9",
        "87.9",
        "87.8",
        "87.7",
        "87.4"
      ],
      [
        "1",
        "92.4",
        "91.9",
        "92.1",
        "92.1",
        "91.5",
        "89.4"
      ],
      [
        "2",
        "91.9",
        "91.8",
        "91.8",
        "91.8",
        "91.3",
        "88.3"
      ],
      [
        "3",
        "92.0",
        "92.3",
        "92.1",
        "91.6",
        "91.2",
        "87.9"
      ],
      [
        "4",
        "92.1",
        "92.4",
        "92.5",
        "92.0",
        "90.5",
        "86.9"
      ],
      [
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy"
      ],
      [
        "0",
        "81.9",
        "81.9",
        "81.8",
        "81.8",
        "81.8",
        "81.2"
      ],
      [
        "1",
        "87.9",
        "87.7",
        "87.8",
        "87.9",
        "87.7",
        "84.5"
      ],
      [
        "2",
        "87.4",
        "87.5",
        "87.4",
        "87.3",
        "87.2",
        "83.2"
      ],
      [
        "3",
        "87.8",
        "87.9",
        "87.9",
        "87.3",
        "87.3",
        "82.9"
      ],
      [
        "4",
        "88.3",
        "88.6",
        "88.4",
        "88.1",
        "87.7",
        "82.1"
      ],
      [
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU"
      ],
      [
        "[EMPTY]",
        "32.7",
        "49.1",
        "38.5",
        "34.2",
        "32.1",
        "96.6"
      ]
    ],
    "id": "42cd498e-f308-4f89-ad6b-90f76be30594",
    "claim": "Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 4 and does not improve at lower layers, with some drops at layers 1 and 2.",
    "label": "refutes",
    "table_id": "430d822d-f0b9-4d4b-b9a6-b995a9e11686"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>ADDED</bold>",
      "<bold>MISS</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "47.34",
        "37.14"
      ],
      [
        "G2S-GIN",
        "48.67",
        "33.64"
      ],
      [
        "G2S-GAT",
        "48.24",
        "33.73"
      ],
      [
        "G2S-GGNN",
        "48.66",
        "34.06"
      ],
      [
        "GOLD",
        "50.77",
        "28.35"
      ],
      [
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ]
    ],
    "id": "127db6c3-81b7-450c-8784-9b686fdb189b",
    "claim": "As shown in Table 8, G2S approaches outperform the S2S baseline.",
    "label": "supports",
    "table_id": "623385a5-d0a0-41c3-90d3-29ada42c8827"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "1bcfc28e-4aa6-4a9b-a26f-00cb129437cb",
    "claim": "In analogy 2, all relations are different.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
    "table_column_names": [
      "[EMPTY]",
      "MSCOCO spice",
      "MSCOCO cider",
      "MSCOCO rouge [ITALIC] L",
      "MSCOCO bleu4",
      "MSCOCO meteor",
      "MSCOCO rep↓",
      "Flickr30k spice",
      "Flickr30k cider",
      "Flickr30k rouge [ITALIC] L",
      "Flickr30k bleu4",
      "Flickr30k meteor",
      "Flickr30k rep↓"
    ],
    "table_content_values": [
      [
        "softmax",
        "18.4",
        "0.967",
        "52.9",
        "29.9",
        "24.9",
        "3.76",
        "13.5",
        "0.443",
        "44.2",
        "19.9",
        "19.1",
        "6.09"
      ],
      [
        "sparsemax",
        "[BOLD] 18.9",
        "[BOLD] 0.990",
        "[BOLD] 53.5",
        "[BOLD] 31.5",
        "[BOLD] 25.3",
        "3.69",
        "[BOLD] 13.7",
        "[BOLD] 0.444",
        "[BOLD] 44.3",
        "[BOLD] 20.7",
        "[BOLD] 19.3",
        "5.84"
      ],
      [
        "TVmax",
        "18.5",
        "0.974",
        "53.1",
        "29.9",
        "25.1",
        "[BOLD] 3.17",
        "13.3",
        "0.438",
        "44.2",
        "20.5",
        "19.0",
        "[BOLD] 3.97"
      ]
    ],
    "id": "966334f3-986a-4d6b-8dcd-efda97f994b6",
    "claim": "[CONTINUE] Selective attention mechanisms like sparsemax and especially TVMAX reduce repetition, as measured by the REP metric reported in Table 1.",
    "label": "supports",
    "table_id": "55b8ec67-1f65-4852-943b-d1530519e837"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "845e45a0-fc97-4bc4-afbd-da01039c86c7",
    "claim": "For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB.",
    "label": "supports",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "3a961802-e664-47df-a85d-d017f7b3250f",
    "claim": "On the same dataset, we have competitive results to Damonte and Cohen (2019).",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "db362f10-8379-467f-820b-0db06904c36e",
    "claim": "The word analogy test was first introduced in [32] to assess the quality of word vectors.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "8a0a5630-4ceb-4984-86ac-cfebf28d6f81",
    "claim": "Adding either the global node or the linear combination does not improve the baseline models with only dense connections.",
    "label": "refutes",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "0caf7d9c-4732-4d3f-9f66-d8b7b5105251",
    "claim": "[CONTINUE] In addition, other words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are also distinctive for tweets that are not complaints.",
    "label": "supports",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "13188ec1-d49c-4625-a0dd-b375315eb448",
    "claim": "The first one models the agenda state space as discrete and predefined, while the other agent encodes a stochastic latent space for agenda representation.",
    "label": "not enough info",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 9: Performance of models trained with tweets from one domain and tested on other domains. All results are reported in ROC AUC. The All line displays results on training on all categories except the category in testing.",
    "table_column_names": [
      "[BOLD] Test",
      "F&B",
      "A",
      "R",
      "Ca",
      "Se",
      "So",
      "T",
      "E",
      "O"
    ],
    "table_content_values": [
      [
        "[BOLD] Train",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Food & Bev.",
        "–",
        "58.1",
        "52.5",
        "66.4",
        "59.7",
        "58.9",
        "54.1",
        "61.4",
        "53.7"
      ],
      [
        "Apparel",
        "63.9",
        "–",
        "74.4",
        "65.1",
        "70.8",
        "71.2",
        "68.5",
        "76.9",
        "85.6"
      ],
      [
        "Retail",
        "58.8",
        "74.4",
        "–",
        "70.1",
        "72.6",
        "69.9",
        "68.7",
        "69.6",
        "82.7"
      ],
      [
        "Cars",
        "68.7",
        "61.1",
        "65.1",
        "–",
        "58.8",
        "67.",
        "59.3",
        "62.9",
        "68.2"
      ],
      [
        "Services",
        "65.",
        "74.2",
        "75.8",
        "74.",
        "–",
        "68.8",
        "74.2",
        "77.9",
        "77.9"
      ],
      [
        "Software",
        "62.",
        "74.2",
        "68.",
        "67.9",
        "72.8",
        "–",
        "72.8",
        "72.1",
        "80.6"
      ],
      [
        "Transport",
        "59.3",
        "71.7",
        "72.4",
        "67.",
        "74.6",
        "75.",
        "–",
        "72.6",
        "81.7"
      ],
      [
        "Electronics",
        "61.6",
        "75.2",
        "71.",
        "68.",
        "75.",
        "69.9",
        "68.2",
        "–",
        "78.7"
      ],
      [
        "Other",
        "56.1",
        "71.3",
        "72.4",
        "70.2",
        "73.5",
        "67.2",
        "68.5",
        "71.",
        "–"
      ],
      [
        "All",
        "70.3",
        "77.7",
        "79.5",
        "82.0",
        "79.6",
        "80.1",
        "76.8",
        "81.7",
        "88.2"
      ]
    ],
    "id": "53df7857-7812-405d-909a-cebde5395c17",
    "claim": "We observe that predictive performance is relatively consistent across all domains with two exceptions ('Food & Beverage' consistently shows lower performance, while 'Other' achieves higher performance) when using all the data available from the other domains.",
    "label": "supports",
    "table_id": "e89df000-9c98-49e5-9d45-5f1b320134c7"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "7b4d6ea4-ff74-450c-bbb3-ad38ced11725",
    "claim": "Such case is the most difficult task for this model.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 9: Performance of models trained with tweets from one domain and tested on other domains. All results are reported in ROC AUC. The All line displays results on training on all categories except the category in testing.",
    "table_column_names": [
      "[BOLD] Test",
      "F&B",
      "A",
      "R",
      "Ca",
      "Se",
      "So",
      "T",
      "E",
      "O"
    ],
    "table_content_values": [
      [
        "[BOLD] Train",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Food & Bev.",
        "–",
        "58.1",
        "52.5",
        "66.4",
        "59.7",
        "58.9",
        "54.1",
        "61.4",
        "53.7"
      ],
      [
        "Apparel",
        "63.9",
        "–",
        "74.4",
        "65.1",
        "70.8",
        "71.2",
        "68.5",
        "76.9",
        "85.6"
      ],
      [
        "Retail",
        "58.8",
        "74.4",
        "–",
        "70.1",
        "72.6",
        "69.9",
        "68.7",
        "69.6",
        "82.7"
      ],
      [
        "Cars",
        "68.7",
        "61.1",
        "65.1",
        "–",
        "58.8",
        "67.",
        "59.3",
        "62.9",
        "68.2"
      ],
      [
        "Services",
        "65.",
        "74.2",
        "75.8",
        "74.",
        "–",
        "68.8",
        "74.2",
        "77.9",
        "77.9"
      ],
      [
        "Software",
        "62.",
        "74.2",
        "68.",
        "67.9",
        "72.8",
        "–",
        "72.8",
        "72.1",
        "80.6"
      ],
      [
        "Transport",
        "59.3",
        "71.7",
        "72.4",
        "67.",
        "74.6",
        "75.",
        "–",
        "72.6",
        "81.7"
      ],
      [
        "Electronics",
        "61.6",
        "75.2",
        "71.",
        "68.",
        "75.",
        "69.9",
        "68.2",
        "–",
        "78.7"
      ],
      [
        "Other",
        "56.1",
        "71.3",
        "72.4",
        "70.2",
        "73.5",
        "67.2",
        "68.5",
        "71.",
        "–"
      ],
      [
        "All",
        "70.3",
        "77.7",
        "79.5",
        "82.0",
        "79.6",
        "80.1",
        "76.8",
        "81.7",
        "88.2"
      ]
    ],
    "id": "6b921c63-e9aa-422c-9036-ad53b45fc2fb",
    "claim": "We observe that predictive performance is not consistent across all domains, with 'Food & Beverage' consistently showing lower performance and 'Other' achieving higher performance when using all the data available from the other domains.",
    "label": "refutes",
    "table_id": "e89df000-9c98-49e5-9d45-5f1b320134c7"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "851a3937-519e-4d91-8e18-bb809245164e",
    "claim": "On the WinoCoref dataset, it improves by 15%.",
    "label": "supports",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "44134077-f8ab-42f9-9e85-66bebe5b1a6e",
    "claim": "These results confirm that simultaneously learning the tasks enhances the performance of a DPP model.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 2: Ablation study results.",
    "table_column_names": [
      "[BOLD] Variation",
      "[BOLD] Accuracy (%)",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "Submitted",
        "[BOLD] 69.23",
        "-"
      ],
      [
        "No emoji",
        "68.36",
        "- 0.87"
      ],
      [
        "No ELMo",
        "65.52",
        "- 3.71"
      ],
      [
        "Concat Pooling",
        "68.47",
        "- 0.76"
      ],
      [
        "LSTM hidden=4096",
        "69.10",
        "- 0.13"
      ],
      [
        "LSTM hidden=1024",
        "68.93",
        "- 0.30"
      ],
      [
        "LSTM hidden=512",
        "68.43",
        "- 0.80"
      ],
      [
        "POS emb dim=100",
        "68.99",
        "- 0.24"
      ],
      [
        "POS emb dim=75",
        "68.61",
        "- 0.62"
      ],
      [
        "POS emb dim=50",
        "69.33",
        "+ 0.10"
      ],
      [
        "POS emb dim=25",
        "69.21",
        "- 0.02"
      ],
      [
        "SGD optim lr=1",
        "64.33",
        "- 4.90"
      ],
      [
        "SGD optim lr=0.1",
        "66.11",
        "- 3.12"
      ],
      [
        "SGD optim lr=0.01",
        "60.72",
        "- 8.51"
      ],
      [
        "SGD optim lr=0.001",
        "30.49",
        "- 38.74"
      ]
    ],
    "id": "b8b4f008-0499-4c41-98ce-62faa07f200c",
    "claim": "[CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 50-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al.",
    "label": "supports",
    "table_id": "aca64ad8-2778-467a-93ff-0acc12c969e6"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "cef0aae6-12a0-42f5-b9ba-e491d86727e7",
    "claim": "Each extractive summaries of a subset is rated by three annotators who are asked to rank the summaries based on the following criteria: structure, meaning preservation, and relevance, on a 1-5 Likert scale",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "4e3c4acc-419f-40ad-99d3-b60a432f2e43",
    "claim": "Its productivity of 57.5% expresses that it appears in incorrect alternatives 7.5% more often than expected by random chance.",
    "label": "refutes",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Orig",
      "Italian Debias",
      "German Orig",
      "German Debias"
    ],
    "table_content_values": [
      [
        "SimLex",
        "0.280",
        "[BOLD] 0.288",
        "0.343",
        "[BOLD] 0.356"
      ],
      [
        "WordSim",
        "0.548",
        "[BOLD] 0.577",
        "0.547",
        "[BOLD] 0.553"
      ]
    ],
    "id": "43802ed8-5f82-4f5a-b44a-fa832e72a6c7",
    "claim": "In both cases, the original embeddings perform better than the new ones.",
    "label": "refutes",
    "table_id": "b897f892-83c1-4817-847f-fa52a995ccab"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "de0a2326-e4df-4552-9b76-e80436ea40e1",
    "claim": "We can also observe that the combination of learned reward and coverage penalty in our system further boosts the performance of NeuralTD with learned rewards, relative to using normal ROUGE or the learned reward only",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "9a2c00b9-5f14-4637-8bc2-cac94c5b48ee",
    "claim": "Furthermore, our model generates longer sentences whose lengths are comparable with human arguments, both with about 22 words per sentence.",
    "label": "supports",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
    "table_column_names": [
      "Uni",
      "POS",
      "0 87.9",
      "1 92.0",
      "2 91.7",
      "3 91.8",
      "4 91.9"
    ],
    "table_content_values": [
      [
        "Uni",
        "SEM",
        "81.8",
        "87.8",
        "87.4",
        "87.6",
        "88.2"
      ],
      [
        "Bi",
        "POS",
        "87.9",
        "93.3",
        "92.9",
        "93.2",
        "92.8"
      ],
      [
        "Bi",
        "SEM",
        "81.9",
        "91.3",
        "90.8",
        "91.9",
        "91.9"
      ],
      [
        "Res",
        "POS",
        "87.9",
        "92.5",
        "91.9",
        "92.0",
        "92.4"
      ],
      [
        "Res",
        "SEM",
        "81.9",
        "88.2",
        "87.5",
        "87.6",
        "88.5"
      ]
    ],
    "id": "ff1ab06e-56c2-42af-bae0-f25b765a45dc",
    "claim": "Some of our bidirectional models obtain 92-93% accuracy.",
    "label": "supports",
    "table_id": "56a61e48-903a-4dc4-8cbf-2048d2c8ee3c"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "3307f33e-0d6e-4edb-b335-5897109ca94d",
    "claim": "our model achieved the best results in terms of appropriateness and diversity.",
    "label": "not enough info",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  },
  {
    "paper": "Low-supervision urgency detection and transfer in short crisis messages",
    "paper_id": "1907.06745v1",
    "table_caption": "TABLE II: Details on datasets used for experiments.",
    "table_column_names": [
      "Dataset",
      "Unlabeled / Labeled Messages",
      "Urgent / Non-urgent Messages",
      "Unique Tokens",
      "Avg. Tokens / Message",
      "Time Range"
    ],
    "table_content_values": [
      [
        "Nepal",
        "6,063/400",
        "201/199",
        "1,641",
        "14",
        "04/05/2015-05/06/2015"
      ],
      [
        "Macedonia",
        "0/205",
        "92/113",
        "129",
        "18",
        "09/18/2018-09/21/2018"
      ],
      [
        "Kerala",
        "92,046/400",
        "125/275",
        "19,393",
        "15",
        "08/17/2018-08/22/2018"
      ]
    ],
    "id": "1515785d-efa6-40d2-af51-95a8c13ee95c",
    "claim": "Table II shows that Nepal and Macedonia are roughly balanced, while Kerala is imbalanced.",
    "label": "refutes",
    "table_id": "3c2720f2-239d-4696-81ea-ba4e9525afd4"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "4cd6be97-c5bd-44fa-a79d-e54753f6893d",
    "claim": "Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied.",
    "label": "supports",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers",
    "paper_id": "1708.00160v2",
    "table_caption": "Table 1: Impact of linguistic features on deep-coref models on the CoNLL development set.",
    "table_column_names": [
      "[EMPTY]",
      "MUC",
      "<italic>B</italic>3",
      "CEAF<italic>e</italic>",
      "CoNLL",
      "LEA"
    ],
    "table_content_values": [
      [
        "ranking",
        "74.31",
        "64.23",
        "59.73",
        "66.09",
        "60.47"
      ],
      [
        "+linguistic",
        "74.35",
        "63.96",
        "60.19",
        "66.17",
        "60.20"
      ],
      [
        "top-pairs",
        "73.95",
        "63.98",
        "59.52",
        "65.82",
        "60.07"
      ],
      [
        "+linguistic",
        "74.32",
        "64.45",
        "60.19",
        "66.32",
        "60.62"
      ]
    ],
    "id": "862ee0dc-0f85-4a85-a1c3-b15e962ba324",
    "claim": "[CONTINUE] However, it does not improve significantly over \"ranking\".",
    "label": "supports",
    "table_id": "b411f14d-f9a4-4bff-8407-abe59ace511c"
  },
  {
    "paper": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns",
    "paper_id": "1810.05201v1",
    "table_caption": "Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
    "table_column_names": [
      "[EMPTY]",
      "M",
      "F",
      "B",
      "O"
    ],
    "table_content_values": [
      [
        "Random",
        "47.5",
        "50.5",
        "[ITALIC] 1.06",
        "49.0"
      ],
      [
        "Token Distance",
        "50.6",
        "47.5",
        "[ITALIC] 0.94",
        "49.1"
      ],
      [
        "Topical Entity",
        "50.2",
        "47.3",
        "[ITALIC] 0.94",
        "48.8"
      ],
      [
        "Syntactic Distance",
        "66.7",
        "66.7",
        "[ITALIC]  [BOLD] 1.00",
        "66.7"
      ],
      [
        "Parallelism",
        "[BOLD] 69.3",
        "[BOLD] 69.2",
        "[ITALIC]  [BOLD] 1.00",
        "[BOLD] 69.2"
      ],
      [
        "Parallelism+URL",
        "[BOLD] 74.2",
        "[BOLD] 71.6",
        "[ITALIC]  [BOLD] 0.96",
        "[BOLD] 72.9"
      ],
      [
        "Transformer-Single",
        "59.6",
        "56.6",
        "[ITALIC] 0.95",
        "58.1"
      ],
      [
        "Transformer-Multi",
        "62.9",
        "61.7",
        "[ITALIC] 0.98",
        "62.3"
      ]
    ],
    "id": "fd89ce86-c88f-4273-882a-21c474839874",
    "claim": "RANDOM is the best performing baseline here, and other baselines are far from gender-parity.",
    "label": "refutes",
    "table_id": "dc3c5fd8-0e21-4c46-a1db-b82788e58324"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 2: Experiment 1",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.001",
        "0.003",
        "-20.818",
        "***",
        "0.505"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.083",
        "0.048",
        "101.636",
        "***",
        "1.724"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.001",
        "0.001",
        "0.035",
        "[EMPTY]",
        "1.001"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.023",
        "0.012",
        "64.418",
        "***",
        "1.993"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.002",
        "0.001",
        "4.047",
        "***",
        "1.120"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.049",
        "0.019",
        "120.986",
        "***",
        "2.573"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.173",
        "0.065",
        "243.285",
        "***",
        "2.653"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.032",
        "0.023",
        "39.483",
        "***",
        "1.396"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.111",
        "0.061",
        "122.707",
        "***",
        "1.812"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.178",
        "0.080",
        "211.319",
        "***",
        "2.239"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.028",
        "0.015",
        "63.131",
        "***",
        "1.854"
      ]
    ],
    "id": "039561ae-0905-4fd8-85e4-f75db58ff616",
    "claim": "Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus, but the difference is not statistically significant.",
    "label": "refutes",
    "table_id": "661ddd6e-51d9-446c-add1-3e80f3b78109"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "[BOLD] Baselines",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen ( 2015a )",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. ( 2018 )",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "[BOLD] Model Variants",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "20733675-9d80-4d43-9f7d-92d3d2a434bf",
    "claim": "[CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model.",
    "label": "supports",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section 3).",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] Part",
      "[BOLD] MRs",
      "[BOLD] Refs",
      "[BOLD] SER(%)"
    ],
    "table_content_values": [
      [
        "Original",
        "Train",
        "4,862",
        "42,061",
        "17.69"
      ],
      [
        "Original",
        "Dev",
        "547",
        "4,672",
        "11.42"
      ],
      [
        "Original",
        "Test",
        "630",
        "4,693",
        "11.49"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Train",
        "8,362",
        "33,525",
        "(0.00)"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Dev",
        "1,132",
        "4,299",
        "(0.00)"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Test",
        "1,358",
        "4,693",
        "(0.00)"
      ]
    ],
    "id": "2b0d604e-1c16-41b1-9485-1f37db56aebb",
    "claim": "This means that the cleaned dataset is less complex overall, with more references per MR and fewer diverse MRs.",
    "label": "refutes",
    "table_id": "82c0eea1-ccb0-4495-a12a-2ecf9bed5f45"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "1722ed89-381a-467b-a50f-39d73e119b85",
    "claim": "For both datasets, our approach does not substantially outperform the baselines.",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Original",
      "Italian Debiased",
      "Italian English",
      "Italian Reduction",
      "German Original",
      "German Debiased",
      "German English",
      "German Reduction"
    ],
    "table_content_values": [
      [
        "Same Gender",
        "0.442",
        "0.434",
        "0.424",
        "–",
        "0.491",
        "0.478",
        "0.446",
        "–"
      ],
      [
        "Different Gender",
        "0.385",
        "0.421",
        "0.415",
        "–",
        "0.415",
        "0.435",
        "0.403",
        "–"
      ],
      [
        "difference",
        "0.057",
        "0.013",
        "0.009",
        "[BOLD] 91.67%",
        "0.076",
        "0.043",
        "0.043",
        "[BOLD] 100%"
      ]
    ],
    "id": "5142bc85-da69-4450-bd65-28cd5ce2831e",
    "claim": "[CONTINUE] As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much lower.",
    "label": "supports",
    "table_id": "a3f7a8f4-e69f-4e32-beb4-281137d9a4a5"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "78540156-ca72-40ea-bf46-e0aae6172d16",
    "claim": "As can be seen in the results presented in Table 3 the models using TVMAX in the output attention layer outperform the models using softmax and sparsemax.",
    "label": "supports",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "a22077ab-b54d-465a-ab9a-67f73866caa7",
    "claim": "“Coverage” represents how much text a system extracts for a document (higher is better); “Overlap” represents the percentage of words that are in the extractive summarization (higher is better) “Avg.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "The MeMAD Submission to the WMT18 Multimodal Translation Task",
    "paper_id": "1808.10802v2",
    "table_caption": "Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.",
    "table_column_names": [
      "[EMPTY]",
      "en-fr",
      "flickr16",
      "flickr17",
      "mscoco17"
    ],
    "table_content_values": [
      [
        "A",
        "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
        "66.3",
        "60.5",
        "52.1"
      ],
      [
        "A",
        "+domain-tuned",
        "66.8",
        "60.6",
        "52.0"
      ],
      [
        "A",
        "+labels",
        "[BOLD] 67.2",
        "60.4",
        "51.7"
      ],
      [
        "T",
        "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
        "66.9",
        "60.3",
        "[BOLD] 52.8"
      ],
      [
        "T",
        "+labels",
        "[BOLD] 67.2",
        "[BOLD] 60.9",
        "52.7"
      ],
      [
        "[EMPTY]",
        "en-de",
        "flickr16",
        "flickr17",
        "mscoco17"
      ],
      [
        "A",
        "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
        "43.1",
        "39.0",
        "35.1"
      ],
      [
        "A",
        "+domain-tuned",
        "43.9",
        "39.4",
        "35.8"
      ],
      [
        "A",
        "+labels",
        "43.2",
        "39.3",
        "34.3"
      ],
      [
        "T",
        "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
        "[BOLD] 44.4",
        "39.4",
        "35.0"
      ],
      [
        "T",
        "+labels",
        "44.1",
        "[BOLD] 39.8",
        "[BOLD] 36.5"
      ]
    ],
    "id": "9064b2de-b304-408b-9aa9-81c8f1be3e65",
    "claim": "For Marian amun, the effect of adding domain labels is significant as we can see in Table 3.",
    "label": "refutes",
    "table_id": "a5edc0f5-cf14-4850-9c3f-c1c7662cdcee"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Present",
      "[BOLD] Not Present"
    ],
    "table_content_values": [
      [
        "Emoji",
        "4805 (76.6%)",
        "23952 (68.0%)"
      ],
      [
        "Hashtags",
        "2122 (70.5%)",
        "26635 (69.4%)"
      ]
    ],
    "id": "d8b4c1e7-8cf5-4916-87af-b646b8ef4b6b",
    "claim": "Tweets containing emoji seem to be harder for the model to classify than those without.",
    "label": "refutes",
    "table_id": "76b1831c-65dd-45d3-bdc5-b490b3a9cee2"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
    "table_column_names": [
      "[BOLD] Representation",
      "[BOLD] Hyper parameters Filter size",
      "[BOLD] Hyper parameters Num. Feature maps",
      "[BOLD] Hyper parameters Activation func.",
      "[BOLD] Hyper parameters L2 Reg.",
      "[BOLD] Hyper parameters Learning rate",
      "[BOLD] Hyper parameters Dropout Prob.",
      "[BOLD] F1.(avg. in 5-fold) with default values",
      "[BOLD] F1.(avg. in 5-fold) with optimal values"
    ],
    "table_content_values": [
      [
        "CoNLL08",
        "4-5",
        "1000",
        "Softplus",
        "1.15e+01",
        "1.13e-03",
        "1",
        "73.34",
        "74.49"
      ],
      [
        "SB",
        "4-5",
        "806",
        "Sigmoid",
        "8.13e-02",
        "1.79e-03",
        "0.87",
        "72.83",
        "[BOLD] 75.05"
      ],
      [
        "UD v1.3",
        "5",
        "716",
        "Softplus",
        "1.66e+00",
        "9.63E-04",
        "1",
        "68.93",
        "69.57"
      ]
    ],
    "id": "55e35248-14b5-4372-91ab-184449934829",
    "claim": "We observe that the results for the UD representation are quite a bit lower than the two others.",
    "label": "supports",
    "table_id": "602023a1-3ab5-4788-8101-59ae8f0c6ce1"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.",
    "table_column_names": [
      "[BOLD] Decoder configuration",
      "[BOLD] es-en  [BOLD] Health",
      "[BOLD] es-en  [BOLD] Bio",
      "[BOLD] en-de  [BOLD] News",
      "[BOLD] en-de  [BOLD] TED",
      "[BOLD] en-de  [BOLD] IT"
    ],
    "table_content_values": [
      [
        "Oracle model",
        "35.9",
        "37.8",
        "37.8",
        "27.0",
        "57.0"
      ],
      [
        "Uniform",
        "36.0",
        "36.4",
        "[BOLD] 38.9",
        "26.0",
        "43.5"
      ],
      [
        "BI + IS",
        "[BOLD] 36.2",
        "[BOLD] 38.0",
        "38.7",
        "[BOLD] 26.1",
        "[BOLD] 56.4"
      ]
    ],
    "id": "adadfacf-0744-4462-951f-d4678d921ee0",
    "claim": "[CONTINUE] EWC models perform well over multiple domains, so the improvement over uniform ensembling is less striking than for unadapted models.",
    "label": "supports",
    "table_id": "eda25ffd-d637-46ae-9e22-e89836a2ffed"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "f31c781e-b429-446a-9493-6900c16e04ef",
    "claim": "In future work, we are also looking into a systematic way of identifying the markers as well as introducing negation for the LSTM which may be able to capture the negation aspects better.",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "2b3c80a8-30f1-48d4-9751-a7353290f19e",
    "claim": "So, the score of analogy 2 will be 0.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 4: Lexicon member coverage (%)",
    "table_column_names": [
      "target",
      "VN",
      "WN-V",
      "WN-N"
    ],
    "table_content_values": [
      [
        "type",
        "81",
        "66",
        "47"
      ],
      [
        "x+POS",
        "54",
        "39",
        "43"
      ],
      [
        "lemma",
        "88",
        "76",
        "53"
      ],
      [
        "x+POS",
        "79",
        "63",
        "50"
      ],
      [
        "shared",
        "54",
        "39",
        "41"
      ]
    ],
    "id": "711764b0-0eb4-4498-8325-afce0b6667b5",
    "claim": "WN-N shows high coverage containing many high-frequency members.",
    "label": "refutes",
    "table_id": "9a7863da-4529-4b1d-87c0-1184b0fcd5dd"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 4: Experiment 2, t= “b*tch”",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.010",
        "0.010",
        "-0.632",
        "[EMPTY]",
        "0.978"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.963",
        "0.944",
        "20.064",
        "***",
        "1.020"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.011",
        "0.011",
        "-1.254",
        "[EMPTY]",
        "0.955"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.349",
        "0.290",
        "28.803",
        "***",
        "1.203"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.012",
        "0.012",
        "-0.162",
        "[EMPTY]",
        "0.995"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.017",
        "0.015",
        "4.698",
        "***",
        "1.152"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.988",
        "0.991",
        "-6.289",
        "***",
        "0.997"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.099",
        "0.091",
        "6.273",
        "***",
        "1.091"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.074",
        "0.027",
        "46.054",
        "***",
        "2.728"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.925",
        "0.968",
        "-41.396",
        "***",
        "0.956"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.010",
        "0.010",
        "0.000",
        "[EMPTY]",
        "1.000"
      ]
    ],
    "id": "84408bed-7687-4049-9b6b-35bed42eda8f",
    "claim": "In both cases the classifiers trained upon their data are still more likely to flag white-aligned tweets as sexism.",
    "label": "refutes",
    "table_id": "def49c38-6913-4cba-9692-08a2b37b5640"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "3a6e389b-f580-4a8a-a2be-ac2fe511e577",
    "claim": "However, the model using TVMAX in the final attention layer does not necessarily achieve the highest accuracy, showing that features obtained using the TVMAX transformation are not necessarily a better complement to bounding box features.",
    "label": "refutes",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "bca20741-c631-4e3e-9086-cb068cfcb160",
    "claim": "The key advantage of this method is that one does not need a large human-annotated corpus for RL training but can use a simulated corpus for supervised and RL training",
    "label": "not enough info",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer aroma+palate",
        "Beer look",
        "74.41",
        "74.83",
        "74.94",
        "72.75",
        "76.41",
        "[BOLD] 79.53",
        "80.29"
      ],
      [
        "Beer look+palate",
        "Beer aroma",
        "68.57",
        "69.23",
        "67.55",
        "69.92",
        "76.45",
        "[BOLD] 77.94",
        "78.11"
      ],
      [
        "Beer look+aroma",
        "Beer palate",
        "63.88",
        "67.82",
        "65.72",
        "74.66",
        "73.40",
        "[BOLD] 75.24",
        "75.50"
      ]
    ],
    "id": "ac292ba0-cc9c-4235-8e92-4901c60e2903",
    "claim": "It closely matches the performance of ORACLE with only 0.40% absolute difference.",
    "label": "supports",
    "table_id": "461a04b0-4d80-4f54-a0ed-42b74709e562"
  },
  {
    "paper": "Towards Scalable and Reliable Capsule Networksfor Challenging NLP Applications",
    "paper_id": "1906.02829v1",
    "table_caption": "Table 2: Comparisons of our NLP-Cap approach and baselines on two text classification benchmarks, where ’-’ denotes methods that failed to scale due to memory issues.",
    "table_column_names": [
      "<bold>Datasets</bold>",
      "<bold>Metrics</bold>",
      "<bold>FastXML</bold>",
      "<bold>PD-Sparse</bold>",
      "<bold>FastText</bold>",
      "<bold>Bow-CNN</bold>",
      "<bold>CNN-Kim</bold>",
      "<bold>XML-CNN</bold>",
      "<bold>Cap-Zhao</bold>",
      "<bold>NLP-Cap</bold>",
      "<bold>Impv</bold>"
    ],
    "table_content_values": [
      [
        "RCV1",
        "PREC@1",
        "94.62",
        "95.16",
        "95.40",
        "96.40",
        "93.54",
        "96.86",
        "96.63",
        "<bold>97.05</bold>",
        "+0.20%"
      ],
      [
        "RCV1",
        "PREC@3",
        "78.40",
        "79.46",
        "79.96",
        "81.17",
        "76.15",
        "81.11",
        "81.02",
        "<bold>81.27</bold>",
        "+0.20%"
      ],
      [
        "RCV1",
        "PREC@5",
        "54.82",
        "55.61",
        "55.64",
        "<bold>56.74</bold>",
        "52.94",
        "56.07",
        "56.12",
        "56.33",
        "-0.72%"
      ],
      [
        "[EMPTY]",
        "NDCG@1",
        "94.62",
        "95.16",
        "95.40",
        "96.40",
        "93.54",
        "96.88",
        "96.63",
        "<bold>97.05</bold>",
        "+0.20%"
      ],
      [
        "[EMPTY]",
        "NDCG@3",
        "89.21",
        "90.29",
        "90.95",
        "92.04",
        "87.26",
        "92.22",
        "92.31",
        "<bold>92.47</bold>",
        "+0.17%"
      ],
      [
        "[EMPTY]",
        "NDCG@5",
        "90.27",
        "91.29",
        "91.68",
        "92.89",
        "88.20",
        "92.63",
        "92.75",
        "<bold>93.11</bold>",
        "+0.52%"
      ],
      [
        "EUR-Lex",
        "PREC@1",
        "68.12",
        "72.10",
        "71.51",
        "64.99",
        "68.35",
        "75.65",
        "-",
        "<bold>80.20</bold>",
        "+6.01%"
      ],
      [
        "EUR-Lex",
        "PREC@3",
        "57.93",
        "57.74",
        "60.37",
        "51.68",
        "54.45",
        "61.81",
        "-",
        "<bold>65.48</bold>",
        "+5.93%"
      ],
      [
        "EUR-Lex",
        "PREC@5",
        "48.97",
        "47.48",
        "50.41",
        "42.32",
        "44.07",
        "50.90",
        "-",
        "<bold>52.83</bold>",
        "+3.79%"
      ],
      [
        "[EMPTY]",
        "NDCG@1",
        "68.12",
        "72.10",
        "71.51",
        "64.99",
        "68.35",
        "75.65",
        "-",
        "<bold>80.20</bold>",
        "+6.01%"
      ],
      [
        "[EMPTY]",
        "NDCG@3",
        "60.66",
        "61.33",
        "63.32",
        "55.03",
        "59.81",
        "66.71",
        "-",
        "<bold>71.11</bold>",
        "+6.59%"
      ],
      [
        "[EMPTY]",
        "NDCG@5",
        "56.42",
        "55.93",
        "58.56",
        "49.92",
        "57.99",
        "64.45",
        "-",
        "<bold>68.80</bold>",
        "+6.75%"
      ]
    ],
    "id": "6e133404-6df1-467e-b5af-f0a895d24779",
    "claim": "In Table 2, we can see a noticeable margin brought by our capsule-based approach over the strong baselines on EUR-Lex, and competitive results on RCV1.",
    "label": "supports",
    "table_id": "59a5a6f8-3968-434e-a516-646a136fde69"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "f4879cd6-63b6-4f55-9bca-036a3a0a0b90",
    "claim": "Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain low degree nodes.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "b5999ce8-28d9-47e9-a847-e7d72dcfde52",
    "claim": "Still, both LRN and oLRN translate sentences faster than SRU (+15%/+6%).",
    "label": "supports",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "c1eb6469-562e-4b25-9c8e-0d11fb645a96",
    "claim": "Compared to CMOW, the hybrid model shows rather small differences.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "d69416ad-80be-4be2-bc76-6806f2a74b90",
    "claim": "MLP with BERT as encoder does not have the best overall performance.",
    "label": "refutes",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
    "table_column_names": [
      "System",
      "All P",
      "All R",
      "All F1",
      "In  [ITALIC] E+ P",
      "In  [ITALIC] E+ R",
      "In  [ITALIC] E+ F1"
    ],
    "table_content_values": [
      [
        "Name matching",
        "15.03",
        "15.03",
        "15.03",
        "29.13",
        "29.13",
        "29.13"
      ],
      [
        "MIL (model 1)",
        "35.87",
        "35.87",
        "35.87 ±0.72",
        "69.38",
        "69.38",
        "69.38 ±1.29"
      ],
      [
        "MIL-ND (model 2)",
        "37.42",
        "[BOLD] 37.42",
        "37.42 ±0.35",
        "72.50",
        "[BOLD] 72.50",
        "[BOLD] 72.50 ±0.68"
      ],
      [
        "[ITALIC] τMIL-ND (model 2)",
        "[BOLD] 38.91",
        "36.73",
        "[BOLD] 37.78 ±0.26",
        "[BOLD] 73.19",
        "71.15",
        "72.16 ±0.48"
      ],
      [
        "Supervised learning",
        "42.90",
        "42.90",
        "42.90 ±0.59",
        "83.12",
        "83.12",
        "83.12 ±1.15"
      ]
    ],
    "id": "dbbed763-8147-4853-9749-d8a28167584b",
    "claim": "MIL-ND does not significantly outperform MIL: the 95% confidence intervals for them overlap.",
    "label": "refutes",
    "table_id": "ce21ef70-1b61-4a48-9c97-69426861922c"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "76927b0e-70f9-4c0a-b09f-c2af489a85dd",
    "claim": "in general, 5.2% of tokens are negation cues, 26.1% of tokens are negated, and 11.2% of tokens are negated but are not cues.",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] Full UAS",
      "[BOLD] PPA Acc."
    ],
    "table_content_values": [
      [
        "RBG",
        "94.17",
        "88.51"
      ],
      [
        "RBG + HPCD (full)",
        "94.19",
        "89.59"
      ],
      [
        "RBG + LSTM-PP",
        "94.14",
        "86.35"
      ],
      [
        "RBG + OntoLSTM-PP",
        "94.30",
        "90.11"
      ],
      [
        "RBG + Oracle PP",
        "94.60",
        "98.97"
      ]
    ],
    "id": "75819ab9-8d94-432d-bb30-590df24c67b7",
    "claim": "However, when gold PP attachment are used, we note only a small improvement of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which suggests that adding PP predictions as features is not an effective approach.",
    "label": "refutes",
    "table_id": "e727bc8f-c0e1-4e2b-a7cf-cc55c64f45d4"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
    "table_column_names": [
      "Model",
      "Val. Accuracy",
      "Loss",
      "Val. Loss",
      "Pretraining Time",
      "Finetuning Time"
    ],
    "table_content_values": [
      [
        "Siamese Networks",
        "77.42%",
        "0.5601",
        "0.5329",
        "[EMPTY]",
        "4m per epoch"
      ],
      [
        "BERT",
        "87.47%",
        "0.4655",
        "0.4419",
        "66 hours",
        "2m per epoch"
      ],
      [
        "GPT-2",
        "90.99%",
        "0.2172",
        "0.1826",
        "78 hours",
        "4m per epoch"
      ],
      [
        "ULMFiT",
        "91.59%",
        "0.3750",
        "0.1972",
        "11 hours",
        "2m per epoch"
      ],
      [
        "ULMFiT (no LM Finetuning)",
        "78.11%",
        "0.5512",
        "0.5409",
        "11 hours",
        "2m per epoch"
      ],
      [
        "BERT + Multitasking",
        "91.20%",
        "0.3155",
        "0.3023",
        "66 hours",
        "4m per epoch"
      ],
      [
        "GPT-2 + Multitasking",
        "96.28%",
        "0.2609",
        "0.2197",
        "78 hours",
        "5m per epoch"
      ]
    ],
    "id": "a86eec51-fb50-4409-9bde-27aa6dd58d44",
    "claim": "BERT achieved a final accuracy of 87.47%, lower than ULMFiT's full performance.",
    "label": "refutes",
    "table_id": "8ef20dc7-9293-47b5-91f4-23b850e09731"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.",
    "table_column_names": [
      "[EMPTY]",
      "Italian → En",
      "Italian En →",
      "German → En",
      "German En →"
    ],
    "table_content_values": [
      [
        "Orig",
        "58.73",
        "59.68",
        "47.58",
        "50.48"
      ],
      [
        "Debias",
        "[BOLD] 60.03",
        "[BOLD] 60.96",
        "[BOLD] 47.89",
        "[BOLD] 51.76"
      ]
    ],
    "id": "9b0e1193-f48e-4334-b899-f5e92f4df3da",
    "claim": "The results reported in Table 7 show that precision on BDI indeed increases as a result of the reduced effect of grammatical gender on the embeddings for German and Italian, i.e.",
    "label": "supports",
    "table_id": "13fef5cb-6185-4d5a-aeac-e5586df49ae7"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "9ba61c9d-3bb7-4573-b8ca-8e1240271ace",
    "claim": "All metrics have good correlations and become more informative when BERT embeddings are used",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "6a0f3f3f-8f25-43cb-938d-3808d00199ac",
    "claim": "Replacing the attention normalizing function with softmax operation also reduces the F1 score marginally (A3−A5).",
    "label": "supports",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "6f84236f-e476-4ea2-9bba-f83a1b157df1",
    "claim": "[CONTINUE] A distinctive part-of-speech pattern common in complaints is possessive pronouns followed by nouns (PRP$ NN) which refer to items of services possessed by the complainer (e.g., my account, my order).",
    "label": "supports",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "e58cd347-4775-4698-9703-16d155e90bc7",
    "claim": "The HAN models do not outperform MEAD in terms of sentence prediction.",
    "label": "refutes",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "718a9e4f-712c-4c8b-bb79-52a817600a8e",
    "claim": "Interestingly, G2S-GIN has better performance among our models.",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)",
    "table_column_names": [
      "System",
      "All LOC",
      "All ORG",
      "All PER",
      "All MISC",
      "In  [ITALIC] E+ LOC",
      "In  [ITALIC] E+ ORG",
      "In  [ITALIC] E+ PER",
      "In  [ITALIC] E+ MISC"
    ],
    "table_content_values": [
      [
        "Name matching",
        "96.26",
        "89.48",
        "57.38",
        "96.60",
        "92.32",
        "76.87",
        "47.40",
        "76.29"
      ],
      [
        "MIL",
        "57.09",
        "[BOLD] 76.30",
        "41.35",
        "93.35",
        "11.90",
        "[BOLD] 47.90",
        "27.60",
        "53.61"
      ],
      [
        "MIL-ND",
        "57.15",
        "77.15",
        "35.95",
        "92.47",
        "12.02",
        "49.77",
        "20.94",
        "47.42"
      ],
      [
        "[ITALIC] τMIL-ND",
        "[BOLD] 55.15",
        "76.56",
        "[BOLD] 34.03",
        "[BOLD] 92.15",
        "[BOLD] 11.14",
        "51.18",
        "[BOLD] 20.59",
        "[BOLD] 40.00"
      ],
      [
        "Supervised learning",
        "55.58",
        "61.32",
        "24.98",
        "89.96",
        "8.80",
        "14.95",
        "7.40",
        "29.90"
      ]
    ],
    "id": "f5a1f7ce-a335-4908-a0e1-baea0d34c863",
    "claim": "[CONTINUE] For LOC, it turns out that candidate selection is not a bottleneck: when candidate selection was flawless, the models made only about 55% errors, down from about 96%.",
    "label": "refutes",
    "table_id": "8ee12371-dc8c-4338-acf5-528fb3099f41"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "ebd1548f-ff05-41b1-917c-9a5e04da6635",
    "claim": "TF has the best values of recall and f-measure for all corpora but the English version of TED Talks which has in DF the best value of recall and in DocSub the best value of f-measure.",
    "label": "supports",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.",
    "table_column_names": [
      "AMR Anno.",
      "BLEU"
    ],
    "table_content_values": [
      [
        "Automatic",
        "16.8"
      ],
      [
        "Gold",
        "[BOLD] *17.5*"
      ]
    ],
    "id": "5ecc4b82-ccbe-480e-af2d-c5e567617179",
    "claim": "Table 4 shows the BLEU scores of our Dual2seq model taking gold or automatic AMRs as inputs.",
    "label": "supports",
    "table_id": "d3d1985b-cf33-4981-9332-d03be820d004"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. † denotes the previous best state-of-the-art model.",
    "table_column_names": [
      "Model",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "CNN zeng2014relation",
        "0.413",
        "0.591",
        "0.486",
        "0.444",
        "0.625",
        "0.519"
      ],
      [
        "PCNN zeng2015distant",
        "0.380",
        "[BOLD] 0.642",
        "0.477",
        "0.446",
        "0.679",
        "0.538†"
      ],
      [
        "EA huang2016attention",
        "0.443",
        "0.638",
        "0.523†",
        "0.419",
        "0.677",
        "0.517"
      ],
      [
        "BGWA jat2018attention",
        "0.364",
        "0.632",
        "0.462",
        "0.417",
        "[BOLD] 0.692",
        "0.521"
      ],
      [
        "BiLSTM-CNN",
        "0.490",
        "0.507",
        "0.498",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "Our model",
        "[BOLD] 0.541",
        "0.595",
        "[BOLD] 0.566*",
        "[BOLD] 0.507",
        "0.652",
        "[BOLD] 0.571*"
      ]
    ],
    "id": "8dac475d-fd22-4945-a778-12d64244ffb8",
    "claim": "Our model does not improve the precision scores on both datasets with good recall scores.",
    "label": "refutes",
    "table_id": "c6b2cd01-ccb9-4fdd-90ab-a8ed5ae0d132"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "Logistic Regression",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Sentiment – MPQA",
        "64.2",
        "39.1",
        "0.499"
      ],
      [
        "Sentiment – NRC",
        "63.9",
        "42.2",
        "0.599"
      ],
      [
        "Sentiment – V&B",
        "68.9",
        "60.0",
        "0.696"
      ],
      [
        "Sentiment – VADER",
        "66.0",
        "54.2",
        "0.654"
      ],
      [
        "Sentiment – Stanford",
        "68.0",
        "55.6",
        "0.696"
      ],
      [
        "Complaint Specific (all)",
        "65.7",
        "55.2",
        "0.634"
      ],
      [
        "Request",
        "64.2",
        "39.1",
        "0.583"
      ],
      [
        "Intensifiers",
        "64.5",
        "47.3",
        "0.639"
      ],
      [
        "Downgraders",
        "65.4",
        "49.8",
        "0.615"
      ],
      [
        "Temporal References",
        "64.2",
        "43.7",
        "0.535"
      ],
      [
        "Pronoun Types",
        "64.1",
        "39.1",
        "0.545"
      ],
      [
        "POS Bigrams",
        "72.2",
        "66.8",
        "0.756"
      ],
      [
        "LIWC",
        "71.6",
        "65.8",
        "0.784"
      ],
      [
        "Word2Vec Clusters",
        "67.7",
        "58.3",
        "0.738"
      ],
      [
        "Bag-of-Words",
        "79.8",
        "77.5",
        "0.866"
      ],
      [
        "All Features",
        "[BOLD] 80.5",
        "[BOLD] 78.0",
        "[BOLD] 0.873"
      ],
      [
        "Neural Networks",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "MLP",
        "78.3",
        "76.2",
        "0.845"
      ],
      [
        "LSTM",
        "80.2",
        "77.0",
        "0.864"
      ]
    ],
    "id": "4c9a4b70-f8f6-4aac-b271-616dbbee6ad4",
    "claim": "The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics perform in the same range as the part of speech tags.",
    "label": "supports",
    "table_id": "caac7a4b-bc0e-4b38-bc24-9d0ad39f2fa7"
  },
  {
    "paper": "Zero-Shot Grounding of Objects from Natural Language Queries",
    "paper_id": "1908.07129v1",
    "table_caption": "Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600×600",
    "table_column_names": [
      "Model",
      "Accuracy on RefClef"
    ],
    "table_content_values": [
      [
        "BM + Softmax",
        "48.54"
      ],
      [
        "BM + BCE",
        "55.20"
      ],
      [
        "BM + FL",
        "57.13"
      ],
      [
        "BM + FL + Img-Resize",
        "[BOLD] 61.75"
      ]
    ],
    "id": "ed36c8eb-dda1-48b8-8fa3-7a9456fdfd05",
    "claim": "[CONTINUE] However, the highest accuracy was achieved by using Binary Cross Entropy, with a score of 55.20.",
    "label": "refutes",
    "table_id": "f8635440-21d1-4472-85d8-b852ab096a46"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
    "table_column_names": [
      "Cue",
      "[ITALIC] SCOPA",
      "[ITALIC] SB_COPA",
      "Diff.",
      "Prod."
    ],
    "table_content_values": [
      [
        "woman",
        "7.98",
        "4.84",
        "-3.14",
        "0.25"
      ],
      [
        "mother",
        "5.16",
        "3.95",
        "-1.21",
        "0.75"
      ],
      [
        "went",
        "6.00",
        "5.15",
        "-0.85",
        "0.73"
      ],
      [
        "down",
        "5.52",
        "4.93",
        "-0.58",
        "0.71"
      ],
      [
        "into",
        "4.07",
        "3.51",
        "-0.56",
        "0.40"
      ]
    ],
    "id": "50fff713-1425-4f87-9a4c-dbcde001032f",
    "claim": "despite their sensitivity to these semantic clues, BERT models trained with their own distributions alone make better decisions when we combine their outputs.",
    "label": "not enough info",
    "table_id": "6609f902-7f48-422d-b5e9-6596405c71dd"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "a56519fa-2e7c-416d-aed9-07c1bd180793",
    "claim": "for example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9).",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "0dd6ed9b-2602-4bfc-ab94-2ff443c53a74",
    "claim": "However, it is not as robust as MQAN, suffering a dramatic decrease in performance on QA-SRL.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "6e2893ee-586e-4dd1-8a74-1fd4f036362e",
    "claim": "The use of annotated NLDs as supervision does not improve the generalization ability of question answering.",
    "label": "refutes",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "92824377-ced2-44b7-ba48-5fa9e9434005",
    "claim": "If the user simulator may select the same action only in a row, this allows the action space to be reduced to 6 possible action sequences.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "97ea818f-e60b-4bac-8c61-9a29c1b2c587",
    "claim": "Next sentence prediction (NSP) has a positive impact.",
    "label": "not enough info",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "3ebe7506-55ba-4319-86df-794421f4d65f",
    "claim": "[CONTINUE] Pretraining the HAN models yields significantly better results than those without.",
    "label": "refutes",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 2: Human evaluation results on MSCOCO.",
    "table_column_names": [
      "[EMPTY]",
      "caption",
      "attention relevance"
    ],
    "table_content_values": [
      [
        "softmax",
        "3.50",
        "3.38"
      ],
      [
        "sparsemax",
        "3.71",
        "3.89"
      ],
      [
        "TVmax",
        "[BOLD] 3.87",
        "[BOLD] 4.10"
      ]
    ],
    "id": "8dd76692-4ea1-4658-9ce7-d242e640238e",
    "claim": "Despite performing slightly worse than sparsemax under automatic metrics, TVMAX does not outperform sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2.",
    "label": "refutes",
    "table_id": "77097db2-3bbc-4070-a3a0-2eaad08e47de"
  },
  {
    "paper": "Suggestion Mining from Online Reviews using ULMFiT",
    "paper_id": "1904.09076v1",
    "table_caption": "Table 3: Performance of different models on the provided train and test dataset for Sub Task A.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] F1 (train)",
      "[BOLD] F1 (test)"
    ],
    "table_content_values": [
      [
        "[BOLD] Multinomial Naive Bayes (using Count Vectorizer)",
        "0.641",
        "0.517"
      ],
      [
        "[BOLD] Logistic Regression (using Count Vectorizer)",
        "0.679",
        "0.572"
      ],
      [
        "[BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer)",
        "0.695",
        "0.576"
      ],
      [
        "[BOLD] LSTM (128 LSTM Units)",
        "0.731",
        "0.591"
      ],
      [
        "[BOLD] Provided Baseline",
        "0.720",
        "0.267"
      ],
      [
        "[BOLD] ULMFit*",
        "0.861",
        "0.701"
      ]
    ],
    "id": "083d7f85-28ec-4a2f-87e7-0c54bd378f36",
    "claim": "[CONTINUE] The ULMFiT model achieved the best results with a F1-score of 0.861 on the training dataset and a F1-score of 0.701 on the test dataset.",
    "label": "supports",
    "table_id": "27322e32-7392-4d54-a916-ac0742e29c2e"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "7b4f6a72-1867-4e14-b9ff-5414a76d5834",
    "claim": "For example, the is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 17.0% of COPA training instances.",
    "label": "refutes",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
    "table_column_names": [
      "Dataset",
      "Metric",
      "Illinois",
      "IlliCons",
      "rahman2012resolving",
      "KnowFeat",
      "KnowCons",
      "KnowComb"
    ],
    "table_content_values": [
      [
        "[ITALIC] Winograd",
        "Precision",
        "51.48",
        "53.26",
        "73.05",
        "71.81",
        "74.93",
        "[BOLD] 76.41"
      ],
      [
        "[ITALIC] WinoCoref",
        "AntePre",
        "68.37",
        "74.32",
        "—–",
        "88.48",
        "88.95",
        "[BOLD] 89.32"
      ]
    ],
    "id": "167f52a1-a645-4d80-8cfc-577a4d19e5d3",
    "claim": "The best performing system is not KnowComb.",
    "label": "refutes",
    "table_id": "7163c23a-3450-4990-97ce-2b79bbbd5763"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "a62ed321-045e-4c34-9771-274b95b428c9",
    "claim": "The results for testing on cleaned data (Table 3, top half) do not confirm the positive impact of cleaned training data and also show that the cleaned test data is not more challenging (cf.",
    "label": "refutes",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "Language Independent Sequence Labelling for Opinion Target Extraction",
    "paper_id": "1901.09755v1",
    "table_caption": "Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.",
    "table_column_names": [
      "Language",
      "System",
      "F1"
    ],
    "table_content_values": [
      [
        "es",
        "GTI",
        "68.51"
      ],
      [
        "es",
        "L +  [BOLD] CW600 + W2VW300",
        "[BOLD] 69.92"
      ],
      [
        "es",
        "Baseline",
        "51.91"
      ],
      [
        "fr",
        "IIT-T",
        "66.67"
      ],
      [
        "fr",
        "L +  [BOLD] CW100",
        "[BOLD] 69.50"
      ],
      [
        "fr",
        "Baseline",
        "45.45"
      ],
      [
        "nl",
        "IIT-T",
        "56.99"
      ],
      [
        "nl",
        "L +  [BOLD] W2VW400",
        "[BOLD] 66.39"
      ],
      [
        "nl",
        "Baseline",
        "50.64"
      ],
      [
        "ru",
        "Danii.",
        "33.47"
      ],
      [
        "ru",
        "L +  [BOLD] CW500",
        "[BOLD] 65.53"
      ],
      [
        "ru",
        "Baseline",
        "49.31"
      ],
      [
        "tr",
        "L +  [BOLD] BW",
        "[BOLD] 60.22"
      ],
      [
        "tr",
        "Baseline",
        "41.86"
      ]
    ],
    "id": "388ba17a-36df-4217-ac48-845683103ee5",
    "claim": "Table 6 shows that our system outperforms the best previous approaches across the five languages.",
    "label": "supports",
    "table_id": "ee6e6baf-f2d4-4eed-88b7-bd2c39612e5e"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 4: Precisions on the Wikidata dataset with different choice of d.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC",
      "Time"
    ],
    "table_content_values": [
      [
        "[ITALIC] d=1",
        "0.602",
        "0.487",
        "0.403",
        "0.367",
        "4h"
      ],
      [
        "[ITALIC] d=32",
        "0.645",
        "0.501",
        "0.393",
        "0.370",
        "-"
      ],
      [
        "[ITALIC] d=16",
        "0.655",
        "0.518",
        "0.413",
        "0.413",
        "20h"
      ],
      [
        "[ITALIC] d=8",
        "0.650",
        "0.519",
        "0.422",
        "0.405",
        "8h"
      ]
    ],
    "id": "335ebc70-0582-4dfd-a396-c07c3b0995de",
    "claim": "increasing the number of items in each set does not help, since the simple [ITALIC] nearest-neighbour method starts with a prohibitively high precision, which cannot be improved by introducing more instances",
    "label": "not enough info",
    "table_id": "b3925d14-8042-410e-841f-73dce02fc49b"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "a120da2d-bd9e-4499-bde4-f927dc66e638",
    "claim": "According to the table, the drop of precision demonstrates that the capsule net is more useful than the word-level attention.",
    "label": "refutes",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
    "table_column_names": [
      "Metric",
      "Method of validation",
      "Yelp",
      "Lit."
    ],
    "table_content_values": [
      [
        "Acc",
        "% of machine and human judgments that match",
        "94",
        "84"
      ],
      [
        "Sim",
        "Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation",
        "0.79",
        "0.75"
      ],
      [
        "PP",
        "Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency",
        "0.81",
        "0.67"
      ]
    ],
    "id": "53788df3-ebe9-4242-bcca-f4aae9867517",
    "claim": "[CONTINUE] We validate Sim and PP by computing sentence-level Spearman's ρ between the metric and human judgments [CONTINUE] From Table 5, all validations show weak correlations on the Yelp dataset and poor correlations on Literature.",
    "label": "refutes",
    "table_id": "11046f41-73be-47e9-9f30-8fe50765c22d"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.",
    "table_column_names": [
      "[EMPTY]",
      "dev perp ↓",
      "dev acc ↑",
      "dev wer ↓",
      "test perp ↓",
      "test acc ↑",
      "test wer ↓"
    ],
    "table_content_values": [
      [
        "Spanish-only-LM",
        "329.68",
        "26.6",
        "30.47",
        "322.26",
        "25.1",
        "29.62"
      ],
      [
        "English-only-LM",
        "320.92",
        "29.3",
        "32.02",
        "314.04",
        "30.3",
        "32.51"
      ],
      [
        "All:CS-last-LM",
        "76.64",
        "47.8",
        "14.56",
        "76.97",
        "49.2",
        "14.13"
      ],
      [
        "All:Shuffled-LM",
        "68.00",
        "51.8",
        "13.64",
        "68.72",
        "51.4",
        "13.89"
      ],
      [
        "CS-only-LM",
        "43.20",
        "60.7",
        "12.60",
        "43.42",
        "57.9",
        "12.18"
      ],
      [
        "CS-only+vocab-LM",
        "45.61",
        "61.0",
        "12.56",
        "45.79",
        "58.8",
        "12.49"
      ],
      [
        "Fine-Tuned-LM",
        "39.76",
        "66.9",
        "10.71",
        "40.11",
        "65.4",
        "10.17"
      ],
      [
        "CS-only-disc",
        "–",
        "72.0",
        "6.35",
        "–",
        "70.5",
        "6.70"
      ],
      [
        "Fine-Tuned-disc",
        "–",
        "[BOLD] 74.2",
        "[BOLD] 5.85",
        "–",
        "[BOLD] 75.5",
        "[BOLD] 5.59"
      ]
    ],
    "id": "be8b847f-1195-49a2-85e7-98fd7b3de34a",
    "claim": "We gain further improvement by adding monolingual data and get an accuracy of 74.2%, which is only 0.3 points higher than the best language model.",
    "label": "refutes",
    "table_id": "d4c8c3ca-899e-4e8c-8efe-81b447163aa5"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 4: Lexicon member coverage (%)",
    "table_column_names": [
      "target",
      "VN",
      "WN-V",
      "WN-N"
    ],
    "table_content_values": [
      [
        "type",
        "81",
        "66",
        "47"
      ],
      [
        "x+POS",
        "54",
        "39",
        "43"
      ],
      [
        "lemma",
        "88",
        "76",
        "53"
      ],
      [
        "x+POS",
        "79",
        "63",
        "50"
      ],
      [
        "shared",
        "54",
        "39",
        "41"
      ]
    ],
    "id": "295db104-a00f-4932-b5c0-740e1efa09b9",
    "claim": "POS-disambiguation, in turn, fragments the vocabulary and consistently reduces the coverage with the effect being less pronounced for lemmatized targets.",
    "label": "supports",
    "table_id": "9a7863da-4529-4b1d-87c0-1184b0fcd5dd"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "73b4f514-f040-4c25-bced-8300abf76759",
    "claim": "[CONTINUE] Under system setup, our model CANDELA does not statistically significantly outperform all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005).",
    "label": "refutes",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "d4cd2af4-72ab-4b05-875f-a67603d4d891",
    "claim": "One work-around for this would be to leverage the sequential nature of the user simulator action selection.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "c57d39b0-c9f8-49b1-81bc-c9040e3f8b30",
    "claim": "Unlike [14], we do not use HypeNET because the code is not publicly available.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "7228065d-6c7a-44a1-86e6-7d8408d8557d",
    "claim": "Opinion distance methods do not generally outperform the competition on both ARI and Silhouette coefficient.",
    "label": "refutes",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc∗: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.",
    "table_column_names": [
      "Model",
      "BLEU",
      "Acc∗"
    ],
    "table_content_values": [
      [
        "fu-1",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Multi-decoder",
        "7.6",
        "0.792"
      ],
      [
        "Style embed.",
        "15.4",
        "0.095"
      ],
      [
        "simple-transfer",
        "simple-transfer",
        "simple-transfer"
      ],
      [
        "Template",
        "18.0",
        "0.867"
      ],
      [
        "Delete/Retrieve",
        "12.6",
        "0.909"
      ],
      [
        "yang2018unsupervised",
        "yang2018unsupervised",
        "yang2018unsupervised"
      ],
      [
        "LM",
        "13.4",
        "0.854"
      ],
      [
        "LM + classifier",
        "[BOLD] 22.3",
        "0.900"
      ],
      [
        "Untransferred",
        "[BOLD] 31.4",
        "0.024"
      ]
    ],
    "id": "6708b66f-e81e-4a90-bc90-eb9ba3b61e0b",
    "claim": "However, at similar levels of Acc, our models have higher BLEU scores than prior work.",
    "label": "supports",
    "table_id": "a78a417b-c642-4ade-8df2-6998a00348c2"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
    "table_column_names": [
      "[BOLD] Model",
      "D",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN(1)",
        "300",
        "10.9M",
        "20.9",
        "52.0"
      ],
      [
        "DCGCN(2)",
        "180",
        "10.9M",
        "[BOLD] 22.2",
        "[BOLD] 52.3"
      ],
      [
        "DCGCN(2)",
        "240",
        "11.3M",
        "22.8",
        "52.8"
      ],
      [
        "DCGCN(4)",
        "180",
        "11.4M",
        "[BOLD] 23.4",
        "[BOLD] 53.4"
      ],
      [
        "DCGCN(1)",
        "420",
        "12.6M",
        "22.2",
        "52.4"
      ],
      [
        "DCGCN(2)",
        "300",
        "12.5M",
        "23.8",
        "53.8"
      ],
      [
        "DCGCN(3)",
        "240",
        "12.3M",
        "[BOLD] 23.9",
        "[BOLD] 54.1"
      ],
      [
        "DCGCN(2)",
        "360",
        "14.0M",
        "24.2",
        "[BOLD] 54.4"
      ],
      [
        "DCGCN(3)",
        "300",
        "14.0M",
        "[BOLD] 24.4",
        "54.2"
      ],
      [
        "DCGCN(2)",
        "420",
        "15.6M",
        "24.1",
        "53.7"
      ],
      [
        "DCGCN(4)",
        "300",
        "15.6M",
        "[BOLD] 24.6",
        "[BOLD] 54.8"
      ],
      [
        "DCGCN(3)",
        "420",
        "18.6M",
        "24.5",
        "54.6"
      ],
      [
        "DCGCN(4)",
        "360",
        "18.4M",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "0773f240-5761-43fd-a7d3-d55af3879cfd",
    "claim": "For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9).",
    "label": "supports",
    "table_id": "135bc50f-f12e-493b-854d-58858c4c5c86"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "1c8b2caa-9827-4d03-9c99-7d4d00a815da",
    "claim": "In conclusion, these results above can show the robustness and effectiveness of our DCGCN models.",
    "label": "supports",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "On the difficulty of a distributional semantics of spoken language",
    "paper_id": "1803.08869v2",
    "table_caption": "Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
    "table_column_names": [
      "[EMPTY]",
      "Recall@10 (%)",
      "Median rank",
      "RSAimage"
    ],
    "table_content_values": [
      [
        "VGS",
        "27",
        "6",
        "0.4"
      ],
      [
        "SegMatch",
        "[BOLD] 10",
        "[BOLD] 37",
        "[BOLD] 0.5"
      ],
      [
        "Audio2vec-U",
        "5",
        "105",
        "0.0"
      ],
      [
        "Audio2vec-C",
        "2",
        "647",
        "0.0"
      ],
      [
        "Mean MFCC",
        "1",
        "1,414",
        "0.0"
      ],
      [
        "Chance",
        "0",
        "3,955",
        "0.0"
      ]
    ],
    "id": "a4769518-b932-490d-990d-4465bce4aa8f",
    "claim": "SegMatch works much better than Audio2vec according to both criteria.",
    "label": "supports",
    "table_id": "087b26ab-9679-4d8d-b96c-57140c1a8b7b"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "ae185a9a-f330-4515-8005-8ecce9a6d6df",
    "claim": "In contrast, our DCGCN models cannot be trained using a large number of layers.",
    "label": "refutes",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "f80d1fe5-769f-49f8-ba8e-2014665930eb",
    "claim": "[CONTINUE] Logistic Regression outperforms other classifiers in extracting most relations.",
    "label": "refutes",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "<bold>Baselines</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>)",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "<bold>Model Variants</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "<bold>79.5</bold>"
      ]
    ],
    "id": "f2f7734e-5b8b-4a24-bae8-acd7f6758765",
    "claim": "Our joint model outperforms all the base [CONTINUE] The results reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016).",
    "label": "supports",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "62eed765-15d0-4c11-9d2b-d12a4be5f764",
    "claim": "Moreover, the model using TVMAX in the final attention layer achieves the highest accuracy, showing that features obtained using the TVMAX transformation are a better complement to bounding box features.",
    "label": "supports",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "Logistic Regression",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Sentiment – MPQA",
        "64.2",
        "39.1",
        "0.499"
      ],
      [
        "Sentiment – NRC",
        "63.9",
        "42.2",
        "0.599"
      ],
      [
        "Sentiment – V&B",
        "68.9",
        "60.0",
        "0.696"
      ],
      [
        "Sentiment – VADER",
        "66.0",
        "54.2",
        "0.654"
      ],
      [
        "Sentiment – Stanford",
        "68.0",
        "55.6",
        "0.696"
      ],
      [
        "Complaint Specific (all)",
        "65.7",
        "55.2",
        "0.634"
      ],
      [
        "Request",
        "64.2",
        "39.1",
        "0.583"
      ],
      [
        "Intensifiers",
        "64.5",
        "47.3",
        "0.639"
      ],
      [
        "Downgraders",
        "65.4",
        "49.8",
        "0.615"
      ],
      [
        "Temporal References",
        "64.2",
        "43.7",
        "0.535"
      ],
      [
        "Pronoun Types",
        "64.1",
        "39.1",
        "0.545"
      ],
      [
        "POS Bigrams",
        "72.2",
        "66.8",
        "0.756"
      ],
      [
        "LIWC",
        "71.6",
        "65.8",
        "0.784"
      ],
      [
        "Word2Vec Clusters",
        "67.7",
        "58.3",
        "0.738"
      ],
      [
        "Bag-of-Words",
        "79.8",
        "77.5",
        "0.866"
      ],
      [
        "All Features",
        "[BOLD] 80.5",
        "[BOLD] 78.0",
        "[BOLD] 0.873"
      ],
      [
        "Neural Networks",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "MLP",
        "78.3",
        "76.2",
        "0.845"
      ],
      [
        "LSTM",
        "80.2",
        "77.0",
        "0.864"
      ]
    ],
    "id": "3f58abfe-7545-46fa-85bb-fc60fb38b406",
    "claim": "Syntactic part-ofspeech features do not obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section do not hold high predictive accuracy for the task.",
    "label": "refutes",
    "table_id": "caac7a4b-bc0e-4b38-bc24-9d0ad39f2fa7"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "ec6aee2f-4336-4c2a-8da2-10020466e7fc",
    "claim": "[CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly well compared to TF-IDF.",
    "label": "refutes",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
    "table_column_names": [
      "# of Heads",
      "Accuracy",
      "Val. Loss",
      "Effect"
    ],
    "table_content_values": [
      [
        "1",
        "89.44%",
        "0.2811",
        "-6.84%"
      ],
      [
        "2",
        "91.20%",
        "0.2692",
        "-5.08%"
      ],
      [
        "4",
        "93.85%",
        "0.2481",
        "-2.43%"
      ],
      [
        "8",
        "96.02%",
        "0.2257",
        "-0.26%"
      ],
      [
        "10",
        "96.28%",
        "0.2197",
        "[EMPTY]"
      ],
      [
        "16",
        "96.32%",
        "0.2190",
        "+0.04"
      ]
    ],
    "id": "b1c90f7c-a41c-4043-8869-314be2024b72",
    "claim": "Using only one attention head, thereby attending to only one context position at once, does not degrade the performance to less than the performance of 10 heads using the standard finetuning scheme.",
    "label": "refutes",
    "table_id": "34083185-d444-4bd0-994e-d13f5afe8e82"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "c1aa17dd-837b-44a3-bd8f-76d596b45bfb",
    "claim": "it outperforms the baseline on three out of the four test datasets, achieving the best results on Glockner.",
    "label": "not enough info",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "Predicting Discourse Structure using Distant Supervision from Sentiment",
    "paper_id": "1910.14176v1",
    "table_caption": "Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined",
    "table_column_names": [
      "Approach",
      "RST-DTtest",
      "Instr-DTtest"
    ],
    "table_content_values": [
      [
        "Right Branching",
        "54.64",
        "58.47"
      ],
      [
        "Left Branching",
        "53.73",
        "48.15"
      ],
      [
        "Hier. Right Branch.",
        "[BOLD] 70.82",
        "[BOLD] 67.86"
      ],
      [
        "Hier. Left Branch.",
        "70.58",
        "63.49"
      ],
      [
        "[BOLD] Intra-Domain Evaluation",
        "[BOLD] Intra-Domain Evaluation",
        "[BOLD] Intra-Domain Evaluation"
      ],
      [
        "HILDAHernault et al. ( 2010 )",
        "83.00",
        "—"
      ],
      [
        "DPLPJi and Eisenstein ( 2014 )",
        "82.08",
        "—"
      ],
      [
        "CODRAJoty et al. ( 2015 )",
        "83.84",
        "[BOLD] 82.88"
      ],
      [
        "Two-StageWang et al. ( 2017 )",
        "[BOLD] 86.00",
        "77.28"
      ],
      [
        "[BOLD] Inter-Domain Evaluation",
        "[BOLD] Inter-Domain Evaluation",
        "[BOLD] Inter-Domain Evaluation"
      ],
      [
        "Two-StageRST-DT",
        "×",
        "73.65"
      ],
      [
        "Two-StageInstr-DT",
        "74.48",
        "×"
      ],
      [
        "Two-StageOurs(avg)",
        "76.42",
        "[BOLD] 74.22"
      ],
      [
        "Two-StageOurs(max)",
        "[BOLD] 77.24",
        "73.12"
      ],
      [
        "Human Morey et al. ( 2017 )",
        "88.30",
        "—"
      ]
    ],
    "id": "c0b78cbb-c152-43d1-9c65-f0c6632ad296",
    "claim": "The first set of results in Table 3 shows that the completely right/left branching baselines dominate the hierarchical right/left branching ones.",
    "label": "refutes",
    "table_id": "00d328ef-0661-439e-bcf1-d6ba0a8e1c50"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Original",
      "Italian Debiased",
      "Italian English",
      "Italian Reduction",
      "German Original",
      "German Debiased",
      "German English",
      "German Reduction"
    ],
    "table_content_values": [
      [
        "Same Gender",
        "0.442",
        "0.434",
        "0.424",
        "–",
        "0.491",
        "0.478",
        "0.446",
        "–"
      ],
      [
        "Different Gender",
        "0.385",
        "0.421",
        "0.415",
        "–",
        "0.415",
        "0.435",
        "0.403",
        "–"
      ],
      [
        "difference",
        "0.057",
        "0.013",
        "0.009",
        "[BOLD] 91.67%",
        "0.076",
        "0.043",
        "0.043",
        "[BOLD] 100%"
      ]
    ],
    "id": "d6ea265e-a56d-4792-928f-4db6f95be5ef",
    "claim": "In Italian, we get a reduction of 91.67% of the gap with respect to English.",
    "label": "supports",
    "table_id": "a3f7a8f4-e69f-4e32-beb4-281137d9a4a5"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The “+” indicates that the true response is added to the whitelist.",
    "table_column_names": [
      "[BOLD] Whitelist",
      "[BOLD] R@1",
      "[BOLD] R@3",
      "[BOLD] R@5",
      "[BOLD] R@10",
      "[BOLD] BLEU"
    ],
    "table_content_values": [
      [
        "Random 10K+",
        "0.252",
        "0.400",
        "0.472",
        "0.560",
        "37.71"
      ],
      [
        "Frequency 10K+",
        "0.257",
        "0.389",
        "0.455",
        "0.544",
        "41.34"
      ],
      [
        "Clustering 10K+",
        "0.230",
        "0.376",
        "0.447",
        "0.541",
        "37.59"
      ],
      [
        "Random 1K+",
        "0.496",
        "0.663",
        "0.728",
        "0.805",
        "59.28"
      ],
      [
        "Frequency 1K+",
        "0.513",
        "0.666",
        "0.726",
        "0.794",
        "67.05"
      ],
      [
        "Clustering 1K+",
        "0.481",
        "0.667",
        "0.745",
        "0.835",
        "61.88"
      ],
      [
        "Frequency 10K",
        "0.136",
        "0.261",
        "0.327",
        "0.420",
        "30.46"
      ],
      [
        "Clustering 10K",
        "0.164",
        "0.292",
        "0.360",
        "0.457",
        "31.47"
      ],
      [
        "Frequency 1K",
        "0.273",
        "0.465",
        "0.550",
        "0.658",
        "47.13"
      ],
      [
        "Clustering 1K",
        "0.331",
        "0.542",
        "0.650",
        "0.782",
        "49.26"
      ]
    ],
    "id": "1f5cde7c-845c-4204-beed-21e7759d023c",
    "claim": "The results in Table 5 show that the three types of whitelists perform comparably to each other when the true response is added.",
    "label": "supports",
    "table_id": "8cf7b802-0e8b-4a89-98f5-9dcaddd2a600"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "0842a1ac-fe0d-40b9-9dee-f56c674b3783",
    "claim": "This can be observed in both Balanced COPA and Textual Entailment experiments.",
    "label": "not enough info",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 3: Performance comparison of our model with different values of m on the two datasets.",
    "table_column_names": [
      "[ITALIC] m",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "1",
        "0.541",
        "0.595",
        "[BOLD] 0.566",
        "0.495",
        "0.621",
        "0.551"
      ],
      [
        "2",
        "0.521",
        "0.597",
        "0.556",
        "0.482",
        "0.656",
        "0.555"
      ],
      [
        "3",
        "0.490",
        "0.617",
        "0.547",
        "0.509",
        "0.633",
        "0.564"
      ],
      [
        "4",
        "0.449",
        "0.623",
        "0.522",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "5",
        "0.467",
        "0.609",
        "0.529",
        "0.488",
        "0.677",
        "0.567"
      ]
    ],
    "id": "027fddad-0ece-4a91-a14f-dff863674aa2",
    "claim": "On the NYT11 dataset, m = 5 gives the best performance.",
    "label": "refutes",
    "table_id": "fbd16514-f1a6-4567-8557-22b8d673c5b6"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 3: AUC and AUC@p of our model on the propriety help desk dataset.",
    "table_column_names": [
      "[BOLD] Metric",
      "[BOLD] Validation",
      "[BOLD] Test"
    ],
    "table_content_values": [
      [
        "AUC",
        "0.991",
        "0.977"
      ],
      [
        "AUC@0.1",
        "0.925",
        "0.885"
      ],
      [
        "AUC@0.05",
        "0.871",
        "0.816"
      ],
      [
        "AUC@0.01",
        "0.677",
        "0.630"
      ]
    ],
    "id": "d66c90ac-981c-4740-bc02-d771a854990f",
    "claim": "The high AUC indicates that our model can easily distinguish between the true response and negative responses.",
    "label": "supports",
    "table_id": "18ea6ba7-47d1-46ac-b0e2-7d8ef81b8087"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "18c2d2ba-85c7-48cd-a247-349356765ca5",
    "claim": "However, this reflects the high variability of the test set.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
    "table_column_names": [
      "ID LSTM-800",
      "5-fold CV 70.56",
      "Δ 0.66",
      "Single model 67.54",
      "Δ 0.78",
      "Ensemble 67.65",
      "Δ 0.30"
    ],
    "table_content_values": [
      [
        "LSTM-400",
        "70.50",
        "0.60",
        "[BOLD] 67.59",
        "0.83",
        "[BOLD] 68.00",
        "0.65"
      ],
      [
        "IN-TITLE",
        "70.11",
        "0.21",
        "[EMPTY]",
        "[EMPTY]",
        "67.52",
        "0.17"
      ],
      [
        "[BOLD] SUBMISSION",
        "69.90",
        "–",
        "66.76",
        "–",
        "67.35",
        "–"
      ],
      [
        "NO-HIGHWAY",
        "69.72",
        "−0.18",
        "66.42",
        "−0.34",
        "66.64",
        "−0.71"
      ],
      [
        "NO-OVERLAPS",
        "69.46",
        "−0.44",
        "65.07",
        "−1.69",
        "66.47",
        "−0.88"
      ],
      [
        "LSTM-400-DROPOUT",
        "69.45",
        "−0.45",
        "65.53",
        "−1.23",
        "67.28",
        "−0.07"
      ],
      [
        "NO-TRANSLATIONS",
        "69.42",
        "−0.48",
        "65.92",
        "−0.84",
        "67.23",
        "−0.12"
      ],
      [
        "NO-ELMO-FINETUNING",
        "67.71",
        "−2.19",
        "65.16",
        "−1.60",
        "65.42",
        "−1.93"
      ]
    ],
    "id": "151436ea-346d-43d1-bbfc-c82ce0fec3c7",
    "claim": "[CONTINUE] Also, our data augmentation technique (NO-TRANSLATIONS) seem to have far smaller impact on the final score then we expected.",
    "label": "supports",
    "table_id": "0980bab3-642c-413e-8b15-6abd868956a8"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "b4c82e00-9bdb-4181-afaa-cdb6340d36f6",
    "claim": "For all batch sizes, the training throughput on the linear dataset is the highest, while the throughput on the balanced dataset is the lowest.",
    "label": "refutes",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
    "table_column_names": [
      "Corpus",
      "Metric",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "Europarl",
        "TotalTerms:",
        "957",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "836",
        "1,000"
      ],
      [
        "Europarl",
        "TotalRoots:",
        "44",
        "1",
        "1",
        "1",
        "1",
        "43",
        "1"
      ],
      [
        "Europarl",
        "NumberRels:",
        "1,588",
        "1,025",
        "1,028",
        "1,185",
        "1,103",
        "1,184",
        "999"
      ],
      [
        "Europarl",
        "MaxDepth:",
        "21",
        "921",
        "901",
        "788",
        "835",
        "8",
        "15"
      ],
      [
        "Europarl",
        "MinDepth:",
        "1",
        "921",
        "901",
        "788",
        "835",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgDepth:",
        "11.82",
        "921",
        "901",
        "788",
        "835",
        "3.05",
        "8.46"
      ],
      [
        "Europarl",
        "DepthCohesion:",
        "1.78",
        "1",
        "1",
        "1",
        "1",
        "2.62",
        "1.77"
      ],
      [
        "Europarl",
        "MaxWidth:",
        "20",
        "2",
        "3",
        "4",
        "3",
        "88",
        "41"
      ],
      [
        "Europarl",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgWidth:",
        "1.99",
        "1.03",
        "1.03",
        "1.19",
        "1.10",
        "4.20",
        "2.38"
      ],
      [
        "TED Talks",
        "TotalTerms:",
        "476",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000"
      ],
      [
        "TED Talks",
        "TotalRoots:",
        "164",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "NumberRels:",
        "521",
        "1,029",
        "1,331",
        "3,025",
        "3,438",
        "3,802",
        "1,009"
      ],
      [
        "TED Talks",
        "MaxDepth:",
        "16",
        "915",
        "658",
        "454",
        "395",
        "118",
        "12"
      ],
      [
        "TED Talks",
        "MinDepth:",
        "1",
        "913",
        "658",
        "454",
        "395",
        "110",
        "1"
      ],
      [
        "TED Talks",
        "AvgDepth:",
        "5.82",
        "914",
        "658",
        "454",
        "395",
        "112.24",
        "5.95"
      ],
      [
        "TED Talks",
        "DepthCohesion:",
        "2.75",
        "1",
        "1",
        "1",
        "1",
        "1.05",
        "2.02"
      ],
      [
        "TED Talks",
        "MaxWidth:",
        "25",
        "2",
        "77",
        "13",
        "12",
        "66",
        "98"
      ],
      [
        "TED Talks",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "AvgWidth:",
        "1.83",
        "1.03",
        "1.36",
        "3.03",
        "3.44",
        "6.64",
        "2.35"
      ]
    ],
    "id": "3490236e-fba6-4622-8f84-7a5db25b3965",
    "claim": "The Patt model was able to generate relations for all terms in the Europarl and TED Talks corpora, as evidenced by the metrics in Table 6.",
    "label": "refutes",
    "table_id": "7fd32b5d-95c3-40d7-8e3d-16391cad7f14"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "53148ebf-8660-4f07-a9c8-b0f61608cced",
    "claim": "We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets.7 As Table 4 shows, previous models perform similarly on both subsets, with the exception of Sasaki et al.",
    "label": "supports",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 5: Scores for different training objectives on the supervised downstream tasks.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CMOW-C",
        "85.9",
        "72.1",
        "69.4",
        "87.0",
        "[BOLD] 71.9",
        "85.4",
        "74.2",
        "73.8",
        "37.6",
        "54.6",
        "71.3"
      ],
      [
        "CMOW-R",
        "[BOLD] 87.5",
        "[BOLD] 73.4",
        "[BOLD] 70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "[BOLD] 77.2",
        "[BOLD] 74.7",
        "[BOLD] 37.9",
        "[BOLD] 56.5",
        "[BOLD] 76.2"
      ],
      [
        "CBOW-C",
        "[BOLD] 90.0",
        "[BOLD] 79.3",
        "[BOLD] 74.6",
        "[BOLD] 87.5",
        "[BOLD] 72.9",
        "85.0",
        "[BOLD] 80.0",
        "78.4",
        "41.0",
        "60.5",
        "[BOLD] 79.2"
      ],
      [
        "CBOW-R",
        "[BOLD] 90.0",
        "79.2",
        "74.0",
        "87.1",
        "71.6",
        "[BOLD] 85.6",
        "78.9",
        "[BOLD] 78.5",
        "[BOLD] 42.1",
        "[BOLD] 61.0",
        "78.1"
      ]
    ],
    "id": "53124f16-be1d-471c-b61f-74a9c8bb30cf",
    "claim": "Consequently, CMOW-R does not outperform CMOW-C on 10 out of 11 supervised downstream tasks. On average over all downstream tasks, the relative improvement is not 20.8%.",
    "label": "refutes",
    "table_id": "561d4da9-91e0-414b-bd80-5d62284815c0"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 4: Lexicon member coverage (%)",
    "table_column_names": [
      "target",
      "VN",
      "WN-V",
      "WN-N"
    ],
    "table_content_values": [
      [
        "type",
        "81",
        "66",
        "47"
      ],
      [
        "x+POS",
        "54",
        "39",
        "43"
      ],
      [
        "lemma",
        "88",
        "76",
        "53"
      ],
      [
        "x+POS",
        "79",
        "63",
        "50"
      ],
      [
        "shared",
        "54",
        "39",
        "41"
      ]
    ],
    "id": "2d32e95f-0000-4e62-933b-42e10261b687",
    "claim": "WN-N shows low coverage containing many low-frequency members.",
    "label": "supports",
    "table_id": "9a7863da-4529-4b1d-87c0-1184b0fcd5dd"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "21f6eced-042c-41eb-94ed-dd2980bbe9de",
    "claim": "with the same model and decoding scheme, for the 5-action experiments, data augmentation improves the Action BLEU by 0.2 and the Slot F1 by 1.89 on average.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "38.4",
        "0.958"
      ],
      [
        "Wiener filter",
        "41.0",
        "0.775"
      ],
      [
        "Minimizing DCE",
        "31.1",
        "[BOLD] 0.392"
      ],
      [
        "FSEGAN",
        "29.1",
        "0.421"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "27.7",
        "0.476"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 26.1",
        "0.462"
      ],
      [
        "Clean speech",
        "9.3",
        "0.0"
      ]
    ],
    "id": "1c0742fa-63b7-44ed-b226-bfa550dabe1c",
    "claim": "acoustic supervision (27.7%) and multi-task learning (26.1%) show lower WER than minimizing DCE (31.1%) and FSEGAN (29.1%)).",
    "label": "supports",
    "table_id": "f609290e-ef88-4bfc-acb0-8d92e9cca315"
  },
  {
    "paper": "Neural End-to-End Learning for Computational Argumentation Mining",
    "paper_id": "1704.06104v2",
    "table_caption": "Table 4: C-F1 (100%) in % for the two indicated systems; essay vs. paragraph level. Note that the mean performances are lower than the majority performances over the runs given in Table 2.",
    "table_column_names": [
      "[EMPTY]",
      "STagBLCC",
      "LSTM-Parser"
    ],
    "table_content_values": [
      [
        "Essay",
        "60.62±3.54",
        "9.40±13.57"
      ],
      [
        "Paragraph",
        "64.74±1.97",
        "56.24±2.87"
      ]
    ],
    "id": "5655d55f-686a-4173-ae58-87964c81a390",
    "claim": "The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%.",
    "label": "supports",
    "table_id": "80bca38f-4847-4d19-823a-20b44ae15b09"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "201a8927-705e-4154-837e-2527d42e84a1",
    "claim": "However, our proposed method does not outperform the original GloVe embeddings.",
    "label": "refutes",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "7fa6751d-e5c9-44c7-9775-48f316b11f2b",
    "claim": "[CONTINUE] Regarding the probing tasks, we observe that CMOW embeddings better encode the linguistic prop [CONTINUE] erties of sentences than CBOW.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1038",
        "0.0170",
        "0.0490",
        "0.0641",
        "0.0641",
        "0.0613",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1282",
        "0.0291",
        "0.0410",
        "0.0270",
        "0.0270",
        "0.1154",
        "0.0661"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.6185",
        "0.3744",
        "0.4144",
        "0.4394",
        "0.4394",
        "[BOLD] 0.7553",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.6308",
        "0.4124",
        "0.4404",
        "0.4515",
        "0.4945",
        "[BOLD] 0.8609",
        "0.5295"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "[BOLD] 0.0021",
        "0.0004",
        "0.0011",
        "0.0014",
        "0.0014",
        "0.0013",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0011",
        "0.0008",
        "0.0011",
        "0.0008",
        "0.0008",
        "[BOLD] 0.0030",
        "0.0018"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0012",
        "0.0008",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0016",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0003",
        "0.0009",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0017",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "[BOLD] 0.0041",
        "0.0007",
        "0.0021",
        "0.0027",
        "0.0027",
        "0.0026",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0022",
        "0.0016",
        "0.0022",
        "0.0015",
        "0.0015",
        "[BOLD] 0.0058",
        "0.0036"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0024",
        "0.0016",
        "0.0018",
        "0.0019",
        "0.0019",
        "[BOLD] 0.0031",
        "0.0023"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0005",
        "0.0018",
        "0.0018",
        "0.0020",
        "0.0021",
        "[BOLD] 0.0034",
        "0.0022"
      ]
    ],
    "id": "c92525f9-3875-4445-bc8e-5e3cac3e8e9e",
    "claim": "On the other hand, choosing the best hypernym did not work very well for DocSub which obtained the lowest precision for the Portuguese corpora.",
    "label": "refutes",
    "table_id": "7ff90dc3-0887-4d7b-b7bf-bb5149801b4e"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 4: Experiment 2, t= “b*tch”",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.010",
        "0.010",
        "-0.632",
        "[EMPTY]",
        "0.978"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.963",
        "0.944",
        "20.064",
        "***",
        "1.020"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.011",
        "0.011",
        "-1.254",
        "[EMPTY]",
        "0.955"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.349",
        "0.290",
        "28.803",
        "***",
        "1.203"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.012",
        "0.012",
        "-0.162",
        "[EMPTY]",
        "0.995"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.017",
        "0.015",
        "4.698",
        "***",
        "1.152"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.988",
        "0.991",
        "-6.289",
        "***",
        "0.997"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.099",
        "0.091",
        "6.273",
        "***",
        "1.091"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.074",
        "0.027",
        "46.054",
        "***",
        "2.728"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.925",
        "0.968",
        "-41.396",
        "***",
        "0.956"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.010",
        "0.010",
        "0.000",
        "[EMPTY]",
        "1.000"
      ]
    ],
    "id": "d5553a4a-b710-4865-adb7-3ae9adb2f279",
    "claim": "In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
    "label": "supports",
    "table_id": "def49c38-6913-4cba-9692-08a2b37b5640"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "54b24d05-5f41-461c-a0d5-53d4d9bb2b16",
    "claim": "As we can observe, it seems that clustering semantically related terms does not necessarily increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected.",
    "label": "refutes",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "368b72b0-fb17-4ab7-b573-ba6f59ddc2a4",
    "claim": "G2S models also generate sentences that contradict the reference sentences less.",
    "label": "supports",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "1a2b1366-c934-4fb3-96f8-d469d5994e36",
    "claim": "Table 8: The contribution of each unsupervised learning for detecting negation triggers.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "3b8e8d3d-432a-4be3-9875-accd92112337",
    "claim": "Dual2seq is consistently better than the other systems under all three metrics, [CONTINUE] Dual2seq is better than both OpenNMT-tf and Transformer-tf .",
    "label": "supports",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "Baseline (No SA)Anderson et al. ( 2018 )",
        "55.00",
        "0M"
      ],
      [
        "SA (S: 1,2,3 - B: 1)",
        "55.11",
        "} 0.107M"
      ],
      [
        "SA (S: 1,2,3 - B: 2)",
        "55.17",
        "} 0.107M"
      ],
      [
        "[BOLD] SA (S: 1,2,3 - B: 3)",
        "[BOLD] 55.27",
        "} 0.107M"
      ]
    ],
    "id": "5854ce90-8b2b-4494-8ab0-d273e8e50cf7",
    "claim": "We empirically found that self-attention was not the most efficient in the 3rd stage.",
    "label": "refutes",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "What do Deep Networks Like to Read?",
    "paper_id": "1909.04547v1",
    "table_caption": "Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.",
    "table_column_names": [
      "Orig",
      "<u> turns in a <u> screenplay that <u> at the edges ; it ’s so clever you want to hate it ."
    ],
    "table_content_values": [
      [
        "DAN",
        "<u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate"
      ],
      [
        "CNN",
        "she turns on a on ( ( in in the the the edges ’s so clever “ want to hate it ”"
      ],
      [
        "RNN",
        "<u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it ."
      ]
    ],
    "id": "fa3f471e-4365-44ac-bbe9-91cb1d44dee1",
    "claim": "In contrast, DAN does not always mask out punctuation and determiners using words indicative of the class label, as evidenced by the example sentence in the table.",
    "label": "refutes",
    "table_id": "a2709d0e-c174-42ae-8a8b-4610a47e1991"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "[BOLD] 71.2"
      ]
    ],
    "id": "759631a8-3754-40cb-8f1e-d00481c7037b",
    "claim": "Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
    "label": "refutes",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "9b3caf85-932d-41fa-83f8-e5b72f7ffc93",
    "claim": "[CONTINUE] When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under Meteor (around 5 points) is greater than that under BLEU (around 3 points).",
    "label": "supports",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "b418dccb-cdbf-4f19-8f22-9cf2beec69ff",
    "claim": "The proposed method achieves the competitive accuracies using single vector compared to several vector models.",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "41e926f4-dc54-48d5-a985-eb56c45a2131",
    "claim": "The resulting cross-dataset improvements on the SNLI and Glockner datasets are not larger than those on the SICK dataset.",
    "label": "refutes",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Training scheme",
      "[BOLD] News",
      "[BOLD] TED",
      "[BOLD] IT"
    ],
    "table_content_values": [
      [
        "1",
        "News",
        "37.8",
        "25.3",
        "35.3"
      ],
      [
        "2",
        "TED",
        "23.7",
        "24.1",
        "14.4"
      ],
      [
        "3",
        "IT",
        "1.6",
        "1.8",
        "39.6"
      ],
      [
        "4",
        "News and TED",
        "38.2",
        "25.5",
        "35.4"
      ],
      [
        "5",
        "1 then TED, No-reg",
        "30.6",
        "[BOLD] 27.0",
        "22.1"
      ],
      [
        "6",
        "1 then TED, L2",
        "37.9",
        "26.7",
        "31.8"
      ],
      [
        "7",
        "1 then TED, EWC",
        "[BOLD] 38.3",
        "[BOLD] 27.0",
        "33.1"
      ],
      [
        "8",
        "5 then IT, No-reg",
        "8.0",
        "6.9",
        "56.3"
      ],
      [
        "9",
        "6 then IT, L2",
        "32.3",
        "22.6",
        "56.9"
      ],
      [
        "10",
        "7 then IT, EWC",
        "35.8",
        "24.6",
        "[BOLD] 57.0"
      ]
    ],
    "id": "0a65243e-547d-4f38-abe3-ed90345ed44d",
    "claim": "In the en-de News/TED task (Table 4), all fine-tuning schemes give similar improvements on TED.",
    "label": "supports",
    "table_id": "2c8215aa-6c63-49db-ae71-e1c029b6e82c"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "ec93a8d7-8518-41cb-be5e-f605bd53b660",
    "claim": "G2S-GAT has a better performance in handling graphs with node out-degrees higher than 9.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "1cb952cc-2280-4358-85a8-e2afbc341742",
    "claim": "we can see that our classifier does not quite achieve the results of the BiLSTM+scope, but is more robust in extracting the expression.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "38.4",
        "0.958"
      ],
      [
        "Wiener filter",
        "41.0",
        "0.775"
      ],
      [
        "Minimizing DCE",
        "31.1",
        "[BOLD] 0.392"
      ],
      [
        "FSEGAN",
        "29.1",
        "0.421"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "27.7",
        "0.476"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 26.1",
        "0.462"
      ],
      [
        "Clean speech",
        "9.3",
        "0.0"
      ]
    ],
    "id": "535d1def-2141-41d5-8a69-da4175cacf77",
    "claim": "The Wiener filtering method shows lower DCE, but higher WER than no enhancement.",
    "label": "supports",
    "table_id": "f609290e-ef88-4bfc-acb0-8d92e9cca315"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
    "table_column_names": [
      "Cue",
      "[ITALIC] SCOPA",
      "[ITALIC] SB_COPA",
      "Diff.",
      "Prod."
    ],
    "table_content_values": [
      [
        "woman",
        "7.98",
        "4.84",
        "-3.14",
        "0.25"
      ],
      [
        "mother",
        "5.16",
        "3.95",
        "-1.21",
        "0.75"
      ],
      [
        "went",
        "6.00",
        "5.15",
        "-0.85",
        "0.73"
      ],
      [
        "down",
        "5.52",
        "4.93",
        "-0.58",
        "0.71"
      ],
      [
        "into",
        "4.07",
        "3.51",
        "-0.56",
        "0.40"
      ]
    ],
    "id": "0da5aa22-c92f-4d85-bc7c-5164be31f090",
    "claim": "we see that superficial cues for COPA are also significant for SB-COPA, showing that SB-COPA mirrors our human intuitions at least for this phenomenon.",
    "label": "not enough info",
    "table_id": "6609f902-7f48-422d-b5e9-6596405c71dd"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
    "table_column_names": [
      "[EMPTY]",
      "Difference Function",
      "Seanad Abolition",
      "Video Games",
      "Pornography"
    ],
    "table_content_values": [
      [
        "OD-parse",
        "Absolute",
        "0.01",
        "-0.01",
        "0.07"
      ],
      [
        "OD-parse",
        "JS div.",
        "0.01",
        "-0.01",
        "-0.01"
      ],
      [
        "OD-parse",
        "EMD",
        "0.07",
        "0.01",
        "-0.01"
      ],
      [
        "OD",
        "Absolute",
        "[BOLD] 0.54",
        "[BOLD] 0.56",
        "[BOLD] 0.41"
      ],
      [
        "OD",
        "JS div.",
        "0.07",
        "-0.01",
        "-0.02"
      ],
      [
        "OD",
        "EMD",
        "0.26",
        "-0.01",
        "0.01"
      ],
      [
        "OD (no polarity shifters)",
        "Absolute",
        "0.23",
        "0.08",
        "0.04"
      ],
      [
        "OD (no polarity shifters)",
        "JS div.",
        "0.09",
        "-0.01",
        "-0.02"
      ],
      [
        "OD (no polarity shifters)",
        "EMD",
        "0.10",
        "0.01",
        "-0.01"
      ]
    ],
    "id": "c4fe7068-9584-4aac-900d-e743f0919833",
    "claim": "This is evident from the insignificant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters.",
    "label": "refutes",
    "table_id": "fba0fe3a-58cc-47a0-a3ab-5d091d2b7fe7"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "LR-All Features – Original Data",
        "80.5",
        "78.0",
        "0.873"
      ],
      [
        "Dist. Supervision + Pooling",
        "77.2",
        "75.7",
        "0.853"
      ],
      [
        "Dist. Supervision + EasyAdapt",
        "[BOLD] 81.2",
        "[BOLD] 79.0",
        "[BOLD] 0.885"
      ]
    ],
    "id": "c2032a31-8e78-411f-aa54-87bf791b98b3",
    "claim": "Results presented in Table 7 show that the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5) and ROC AUC by 0.012.",
    "label": "supports",
    "table_id": "6dbcb51c-9427-4e68-be86-351890ad3d0a"
  },
  {
    "paper": "Aligning Vector-spaces with Noisy Supervised Lexicons",
    "paper_id": "1903.10238v1",
    "table_caption": "Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
    "table_column_names": [
      "Method",
      "En→It best",
      "En→It avg",
      "En→It iters",
      "En→De best",
      "En→De avg",
      "En→De iters",
      "En→Fi best",
      "En→Fi avg",
      "En→Fi iters",
      "En→Es best",
      "En→Es avg",
      "En→Es iters"
    ],
    "table_content_values": [
      [
        "Artetxe et al., 2018b",
        "[BOLD] 48.53",
        "48.13",
        "573",
        "48.47",
        "48.19",
        "773",
        "33.50",
        "32.63",
        "988",
        "37.60",
        "37.33",
        "808"
      ],
      [
        "Noise-aware Alignment",
        "[BOLD] 48.53",
        "[BOLD] 48.20",
        "471",
        "[BOLD] 49.67",
        "[BOLD] 48.89",
        "568",
        "[BOLD] 33.98",
        "[BOLD] 33.68",
        "502",
        "[BOLD] 38.40",
        "[BOLD] 37.79",
        "551"
      ]
    ],
    "id": "291119c8-4554-4704-a71f-715bfb6ea711",
    "claim": "In addition, the noise-aware model is more stable and therefore requires fewer iterations to converge.",
    "label": "supports",
    "table_id": "279e3d12-df99-48f8-83ea-d3e42b8bbfcc"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
    "table_column_names": [
      "# of Heads",
      "Accuracy",
      "Val. Loss",
      "Effect"
    ],
    "table_content_values": [
      [
        "1",
        "89.44%",
        "0.2811",
        "-6.84%"
      ],
      [
        "2",
        "91.20%",
        "0.2692",
        "-5.08%"
      ],
      [
        "4",
        "93.85%",
        "0.2481",
        "-2.43%"
      ],
      [
        "8",
        "96.02%",
        "0.2257",
        "-0.26%"
      ],
      [
        "10",
        "96.28%",
        "0.2197",
        "[EMPTY]"
      ],
      [
        "16",
        "96.32%",
        "0.2190",
        "+0.04"
      ]
    ],
    "id": "91e9f947-9e27-4fe2-9913-b45c570f1d05",
    "claim": "This shows that more attention heads, thereby attending to multiple different contexts at once, does not necessarily lead to state-of-the-art results.",
    "label": "refutes",
    "table_id": "34083185-d444-4bd0-994e-d13f5afe8e82"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer aroma+palate",
        "Beer look",
        "74.41",
        "74.83",
        "74.94",
        "72.75",
        "76.41",
        "[BOLD] 79.53",
        "80.29"
      ],
      [
        "Beer look+palate",
        "Beer aroma",
        "68.57",
        "69.23",
        "67.55",
        "69.92",
        "76.45",
        "[BOLD] 77.94",
        "78.11"
      ],
      [
        "Beer look+aroma",
        "Beer palate",
        "63.88",
        "67.82",
        "65.72",
        "74.66",
        "73.40",
        "[BOLD] 75.24",
        "75.50"
      ]
    ],
    "id": "24a53156-9a34-46d3-8bd8-acd2a28bdb82",
    "claim": "Our model (OURS) does not obtain substantial gains in accuracy over the baselines across all three target aspects.",
    "label": "refutes",
    "table_id": "461a04b0-4d80-4f54-a0ed-42b74709e562"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "2a8dabe5-b9db-45df-a2d3-8f7ad54d75d2",
    "claim": "In comparison, GDPL is still comparable with ACER and PPO, obtains a better match rate, and even achieves higher task success.",
    "label": "supports",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "893265ca-c355-4c56-914b-a7e0fc559077",
    "claim": "However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is only 2.9 percent (71.9% vs. 69.0%).",
    "label": "supports",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "b565802b-953b-444b-8f62-8ec3c2ab776f",
    "claim": "This reflects the dialog efficiency of all methods but ACER decreases with the time extension, which is the opposite with human’s preference.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "81206d38-8fec-4a92-95c9-b53a0785ab95",
    "claim": "We observe that our model exhibits the best performances.",
    "label": "supports",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "202d9083-e874-49ff-8926-97f4f3f5bc91",
    "claim": "G2S models generate sentences that contradict the reference sentences more.",
    "label": "refutes",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "d48650ee-1b73-4a22-af9d-5d59d1d0d522",
    "claim": "the overall results suggest that the combination of our negative opinion words with external sentiment lexicon outperform other methods",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "The MeMAD Submission to the WMT18 Multimodal Translation Task",
    "paper_id": "1808.10802v2",
    "table_caption": "Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.",
    "table_column_names": [
      "[EMPTY]",
      "en-fr",
      "flickr16",
      "flickr17",
      "mscoco17"
    ],
    "table_content_values": [
      [
        "A",
        "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
        "66.3",
        "60.5",
        "52.1"
      ],
      [
        "A",
        "+domain-tuned",
        "66.8",
        "60.6",
        "52.0"
      ],
      [
        "A",
        "+labels",
        "[BOLD] 67.2",
        "60.4",
        "51.7"
      ],
      [
        "T",
        "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
        "66.9",
        "60.3",
        "[BOLD] 52.8"
      ],
      [
        "T",
        "+labels",
        "[BOLD] 67.2",
        "[BOLD] 60.9",
        "52.7"
      ],
      [
        "[EMPTY]",
        "en-de",
        "flickr16",
        "flickr17",
        "mscoco17"
      ],
      [
        "A",
        "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
        "43.1",
        "39.0",
        "35.1"
      ],
      [
        "A",
        "+domain-tuned",
        "43.9",
        "39.4",
        "35.8"
      ],
      [
        "A",
        "+labels",
        "43.2",
        "39.3",
        "34.3"
      ],
      [
        "T",
        "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
        "[BOLD] 44.4",
        "39.4",
        "35.0"
      ],
      [
        "T",
        "+labels",
        "44.1",
        "[BOLD] 39.8",
        "[BOLD] 36.5"
      ]
    ],
    "id": "8d5740c5-abf5-47c2-beb2-4150cb29d77f",
    "claim": "[CONTINUE] For Marian amun, the effect is negligible as we can see in Table 3.",
    "label": "supports",
    "table_id": "a5edc0f5-cf14-4850-9c3f-c1c7662cdcee"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "f2ea1a3c-fbc4-448e-aeab-30d25c8c4969",
    "claim": "However, our summary is often significantly longer than the actual reference summary.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "cad7b2c8-2a39-4cb7-84fc-68280ec753d8",
    "claim": "For example, on AMR17, the ensemble model of Seq2SeqB is 1 BLEU point higher than the single DCGCN model.",
    "label": "refutes",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Neural End-to-End Learning for Computational Argumentation Mining",
    "paper_id": "1704.06104v2",
    "table_caption": "Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.",
    "table_column_names": [
      "[EMPTY]",
      "C-F1 100%",
      "C-F1 50%",
      "R-F1 100%",
      "R-F1 50%",
      "F1 100%",
      "F1 50%"
    ],
    "table_content_values": [
      [
        "Y-3",
        "49.59",
        "65.37",
        "26.28",
        "37.00",
        "34.35",
        "47.25"
      ],
      [
        "Y-3:Y<italic>C</italic>-1",
        "54.71",
        "66.84",
        "28.44",
        "37.35",
        "37.40",
        "47.92"
      ],
      [
        "Y-3:Y<italic>R</italic>-1",
        "51.32",
        "66.49",
        "26.92",
        "37.18",
        "35.31",
        "47.69"
      ],
      [
        "Y-3:Y<italic>C</italic>-3",
        "<bold>54.58</bold>",
        "67.66",
        "<bold>30.22</bold>",
        "<bold>40.30</bold>",
        "<bold>38.90</bold>",
        "<bold>50.51</bold>"
      ],
      [
        "Y-3:Y<italic>R</italic>-3",
        "53.31",
        "66.71",
        "26.65",
        "35.86",
        "35.53",
        "46.64"
      ],
      [
        "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
        "52.95",
        "<bold>67.84</bold>",
        "27.90",
        "39.71",
        "36.54",
        "50.09"
      ],
      [
        "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
        "54.55",
        "67.60",
        "28.30",
        "38.26",
        "37.26",
        "48.86"
      ]
    ],
    "id": "5a6b1a51-03be-40dd-9c08-02b7829a9750",
    "claim": "Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: [CONTINUE] as in Eq.",
    "label": "supports",
    "table_id": "d0039005-f056-4745-9652-8796a2c2b307"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "a8563f9b-8e6c-4b68-b8e5-7d8c75675538",
    "claim": "Again, one possible explanation is that cleaning the missing slots provided more complex training examples.",
    "label": "supports",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] R-1",
      "[BOLD] R-2",
      "[BOLD] R-SU"
    ],
    "table_content_values": [
      [
        "First-1",
        "26.83",
        "7.25",
        "6.46"
      ],
      [
        "First-2",
        "35.99",
        "10.17",
        "12.06"
      ],
      [
        "First-3",
        "39.41",
        "11.77",
        "14.51"
      ],
      [
        "LexRank Erkan and Radev ( 2004 )",
        "38.27",
        "12.70",
        "13.20"
      ],
      [
        "TextRank Mihalcea and Tarau ( 2004 )",
        "38.44",
        "13.10",
        "13.50"
      ],
      [
        "MMR Carbonell and Goldstein ( 1998 )",
        "38.77",
        "11.98",
        "12.91"
      ],
      [
        "PG-Original Lebanoff et al. ( 2018 )",
        "41.85",
        "12.91",
        "16.46"
      ],
      [
        "PG-MMR Lebanoff et al. ( 2018 )",
        "40.55",
        "12.36",
        "15.87"
      ],
      [
        "PG-BRNN Gehrmann et al. ( 2018 )",
        "42.80",
        "14.19",
        "16.75"
      ],
      [
        "CopyTransformer Gehrmann et al. ( 2018 )",
        "[BOLD] 43.57",
        "14.03",
        "17.37"
      ],
      [
        "Hi-MAP (Our Model)",
        "43.47",
        "[BOLD] 14.89",
        "[BOLD] 17.41"
      ]
    ],
    "id": "20f98547-11fd-48bd-a892-284b3df13a83",
    "claim": "The Transformer performs best in terms of R-1 while Hi-MAP does not outperform it on R-2 and R-SU.",
    "label": "refutes",
    "table_id": "6f86cef3-2ee0-4782-b5b4-f37400c22dfb"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "<bold>Baselines</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>)",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "<bold>Model Variants</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "<bold>79.5</bold>"
      ]
    ],
    "id": "e76cbbbb-e973-4cdf-9ab0-a1a39fec7cfc",
    "claim": "[CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 9.9 CoNLL F1 points on events.",
    "label": "refutes",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "90056cab-a079-4e5e-8c6c-cd8f2605b030",
    "claim": "the performance of the proposed method, which takes into account all kinds of semantic orientations and measures word relationships on the basis of \"receptivity\", does not show much difference.",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "Logistic Regression",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Sentiment – MPQA",
        "64.2",
        "39.1",
        "0.499"
      ],
      [
        "Sentiment – NRC",
        "63.9",
        "42.2",
        "0.599"
      ],
      [
        "Sentiment – V&B",
        "68.9",
        "60.0",
        "0.696"
      ],
      [
        "Sentiment – VADER",
        "66.0",
        "54.2",
        "0.654"
      ],
      [
        "Sentiment – Stanford",
        "68.0",
        "55.6",
        "0.696"
      ],
      [
        "Complaint Specific (all)",
        "65.7",
        "55.2",
        "0.634"
      ],
      [
        "Request",
        "64.2",
        "39.1",
        "0.583"
      ],
      [
        "Intensifiers",
        "64.5",
        "47.3",
        "0.639"
      ],
      [
        "Downgraders",
        "65.4",
        "49.8",
        "0.615"
      ],
      [
        "Temporal References",
        "64.2",
        "43.7",
        "0.535"
      ],
      [
        "Pronoun Types",
        "64.1",
        "39.1",
        "0.545"
      ],
      [
        "POS Bigrams",
        "72.2",
        "66.8",
        "0.756"
      ],
      [
        "LIWC",
        "71.6",
        "65.8",
        "0.784"
      ],
      [
        "Word2Vec Clusters",
        "67.7",
        "58.3",
        "0.738"
      ],
      [
        "Bag-of-Words",
        "79.8",
        "77.5",
        "0.866"
      ],
      [
        "All Features",
        "[BOLD] 80.5",
        "[BOLD] 78.0",
        "[BOLD] 0.873"
      ],
      [
        "Neural Networks",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "MLP",
        "78.3",
        "76.2",
        "0.845"
      ],
      [
        "LSTM",
        "80.2",
        "77.0",
        "0.864"
      ]
    ],
    "id": "319f8071-f254-4ea5-9d12-2d01758e5686",
    "claim": "However, best predictive performance is obtained using bag-of-word features, reaching an F1 of up to 77.5 and AUC of 0.866.",
    "label": "supports",
    "table_id": "caac7a4b-bc0e-4b38-bc24-9d0ad39f2fa7"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "99e5b5e6-f575-4873-b1b5-cf1ecc803e78",
    "claim": "We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels. However, our model does not outperform the other models, as evidenced by the lower AUC score.",
    "label": "refutes",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "78a80fec-1eb9-49ce-9e20-ffc00a53a2b1",
    "claim": "[CONTINUE] However, CMOW does not in general supersede CBOW embeddings.",
    "label": "supports",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
    "table_column_names": [
      "[BOLD] Training data",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] Disfl"
    ],
    "table_content_values": [
      [
        "Original",
        "0",
        "22",
        "0",
        "14"
      ],
      [
        "Cleaned added",
        "0",
        "23",
        "0",
        "14"
      ],
      [
        "Cleaned missing",
        "0",
        "1",
        "0",
        "2"
      ],
      [
        "Cleaned",
        "0",
        "0",
        "0",
        "5"
      ]
    ],
    "id": "6a39e1b2-806a-4c81-953d-0a1db9b7d962",
    "claim": "All fluency problems we found were very slight, but added and wrong-valued slots were still found, so missed slots are not the only problem.",
    "label": "refutes",
    "table_id": "82ce68c2-64df-452b-ac0b-064c7fdaefa8"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 4: Image-caption ranking results for Japanese (MS-COCO)",
    "table_column_names": [
      "[EMPTY]",
      "Image to Text R@1",
      "Image to Text R@5",
      "Image to Text R@10",
      "Image to Text Mr",
      "Text to Image R@1",
      "Text to Image R@5",
      "Text to Image R@10",
      "Text to Image Mr",
      "Alignment"
    ],
    "table_content_values": [
      [
        "[BOLD] symmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Mono",
        "42.7",
        "77.7",
        "88.5",
        "2",
        "33.1",
        "69.8",
        "84.3",
        "3",
        "-"
      ],
      [
        "FME",
        "40.7",
        "77.7",
        "88.3",
        "2",
        "30.0",
        "68.9",
        "83.1",
        "3",
        "92.70%"
      ],
      [
        "AME",
        "[BOLD] 50.2",
        "[BOLD] 85.6",
        "[BOLD] 93.1",
        "[BOLD] 1",
        "[BOLD] 40.2",
        "[BOLD] 76.7",
        "[BOLD] 87.8",
        "[BOLD] 2",
        "82.54%"
      ],
      [
        "[BOLD] asymmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Mono",
        "49.9",
        "83.4",
        "93.7",
        "2",
        "39.7",
        "76.5",
        "88.3",
        "[BOLD] 2",
        "-"
      ],
      [
        "FME",
        "48.8",
        "81.9",
        "91.9",
        "2",
        "37.0",
        "74.8",
        "87.0",
        "[BOLD] 2",
        "92.70%"
      ],
      [
        "AME",
        "[BOLD] 55.5",
        "[BOLD] 87.9",
        "[BOLD] 95.2",
        "[BOLD] 1",
        "[BOLD] 44.9",
        "[BOLD] 80.7",
        "[BOLD] 89.3",
        "[BOLD] 2",
        "84.99%"
      ]
    ],
    "id": "66c277ce-a633-4441-911f-9346c0b73c74",
    "claim": "For the Japanese captions, AME reaches 6.25% and 3.66% better results on average compared to monolingual model in symmetric and asymmetric modes, respectively.",
    "label": "supports",
    "table_id": "32469655-2bf5-4a17-84a4-3600d3a2caf5"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 4: Results of Self-Play Evaluation.",
    "table_column_names": [
      "System",
      "TGPC Succ. (%)",
      "TGPC #Turns",
      "CWC Succ. (%)",
      "CWC #Turns"
    ],
    "table_content_values": [
      [
        "Retrieval ",
        "7.16",
        "4.17",
        "0",
        "-"
      ],
      [
        "Retrieval-Stgy ",
        "47.80",
        "6.7",
        "44.6",
        "7.42"
      ],
      [
        "PMI ",
        "35.36",
        "6.38",
        "47.4",
        "5.29"
      ],
      [
        "Neural ",
        "54.76",
        "4.73",
        "47.6",
        "5.16"
      ],
      [
        "Kernel ",
        "62.56",
        "4.65",
        "53.2",
        "4.08"
      ],
      [
        "DKRN (ours)",
        "[BOLD] 89.0",
        "5.02",
        "[BOLD] 84.4",
        "4.20"
      ]
    ],
    "id": "f480c688-06c4-459b-affc-8737fc822e2b",
    "claim": "This table refutes the effectiveness of our approach.",
    "label": "refutes",
    "table_id": "5a8c9b40-6ff6-44b7-af2e-4bf75b46492f"
  },
  {
    "paper": "Zero-Shot Grounding of Objects from Natural Language Queries",
    "paper_id": "1908.07129v1",
    "table_caption": "Table 3: Category-wise performance with the default split of Flickr30k Entities.",
    "table_column_names": [
      "Method",
      "Overall",
      "people",
      "clothing",
      "bodyparts",
      "animals",
      "vehicles",
      "instruments",
      "scene",
      "other"
    ],
    "table_content_values": [
      [
        "QRC - VGG(det)",
        "60.21",
        "75.08",
        "55.9",
        "20.27",
        "73.36",
        "68.95",
        "45.68",
        "65.27",
        "38.8"
      ],
      [
        "CITE - VGG(det)",
        "61.89",
        "[BOLD] 75.95",
        "58.50",
        "30.78",
        "[BOLD] 77.03",
        "[BOLD] 79.25",
        "48.15",
        "58.78",
        "43.24"
      ],
      [
        "ZSGNet - VGG (cls)",
        "60.12",
        "72.52",
        "60.57",
        "38.51",
        "63.61",
        "64.47",
        "49.59",
        "64.66",
        "41.09"
      ],
      [
        "ZSGNet - Res50 (cls)",
        "[BOLD] 63.39",
        "73.87",
        "[BOLD] 66.18",
        "[BOLD] 45.27",
        "73.79",
        "71.38",
        "[BOLD] 58.54",
        "[BOLD] 66.49",
        "[BOLD] 45.53"
      ]
    ],
    "id": "0cf1197a-668e-4bba-a1b1-ccc35b2fd72f",
    "claim": "[CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\"), however, the ZSGNet model with Res50 (cls) performs better than the other models on all categories.",
    "label": "refutes",
    "table_id": "347fd2b9-f84e-4288-a2e3-b793a47f6266"
  },
  {
    "paper": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection",
    "paper_id": "1904.04388v1",
    "table_caption": "Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.",
    "table_column_names": [
      "[BOLD] Type",
      "[BOLD] Reparandum Length  [BOLD] 1-2",
      "[BOLD] Reparandum Length  [BOLD] 3-5"
    ],
    "table_content_values": [
      [
        "content-content",
        "0.61 (30%)",
        "0.58 (52%)"
      ],
      [
        "content-function",
        "0.77 (20%)",
        "0.66 (17%)"
      ],
      [
        "function-function",
        "0.83 (50%)",
        "0.80 (32%)"
      ]
    ],
    "id": "681f0a81-e820-45f2-9d6b-64137a6a6c7c",
    "claim": "We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies.",
    "label": "supports",
    "table_id": "d5f567cd-d943-4265-b564-1829f3c072c3"
  },
  {
    "paper": "Filling Conversation Ellipsis for Better Social Dialog Understanding",
    "paper_id": "1911.10776v1",
    "table_caption": "Table 6: Dialog act prediction performance using different selection methods.",
    "table_column_names": [
      "[BOLD] Selection Method",
      "[BOLD] Prec.(%)",
      "[BOLD] Rec.(%)",
      "[BOLD] F1(%)"
    ],
    "table_content_values": [
      [
        "Max Logits",
        "80.19",
        "80.50",
        "79.85"
      ],
      [
        "Add Logits",
        "81.30",
        "81.28",
        "80.85"
      ],
      [
        "Add Logits+Expert",
        "[BOLD] 81.30",
        "[BOLD] 81.41",
        "[BOLD] 80.90"
      ],
      [
        "Concat Hidden",
        "80.24",
        "80.04",
        "79.65"
      ],
      [
        "Max Hidden",
        "80.30",
        "80.04",
        "79.63"
      ],
      [
        "Add Hidden",
        "80.82",
        "80.28",
        "80.08"
      ]
    ],
    "id": "249235b7-0bb2-431e-b55c-3adcbc63a9d2",
    "claim": "We can see from Table 6 that empirically adding logits from two models after classifiers performs the best.",
    "label": "supports",
    "table_id": "e5b471a7-1235-4fa7-b78c-79de04738582"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "52aa5baf-fd99-4ce9-845f-90ae579128c5",
    "claim": "Intrusion by Noise Word: the imparted knowledge often adds words that are grammatical, but are out of context.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "5eb5ab6e-0556-435d-b7c3-f73a75086415",
    "claim": "Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points.",
    "label": "supports",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "2a748b24-0923-494a-b41b-5b290c77df35",
    "claim": "The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are not statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "label": "refutes",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "7ff4400d-a41b-4b6e-9fbc-20980c38e5fa",
    "claim": "The improvements due to shared representations and a disjoint entity span model are approximately equal, but the two models in combination together achieve the highest results, increasing joint <italic>F</italic>1 to 71.2.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Filling Conversation Ellipsis for Better Social Dialog Understanding",
    "paper_id": "1911.10776v1",
    "table_caption": "Table 6: Dialog act prediction performance using different selection methods.",
    "table_column_names": [
      "[BOLD] Selection Method",
      "[BOLD] Prec.(%)",
      "[BOLD] Rec.(%)",
      "[BOLD] F1(%)"
    ],
    "table_content_values": [
      [
        "Max Logits",
        "80.19",
        "80.50",
        "79.85"
      ],
      [
        "Add Logits",
        "81.30",
        "81.28",
        "80.85"
      ],
      [
        "Add Logits+Expert",
        "[BOLD] 81.30",
        "[BOLD] 81.41",
        "[BOLD] 80.90"
      ],
      [
        "Concat Hidden",
        "80.24",
        "80.04",
        "79.65"
      ],
      [
        "Max Hidden",
        "80.30",
        "80.04",
        "79.63"
      ],
      [
        "Add Hidden",
        "80.82",
        "80.28",
        "80.08"
      ]
    ],
    "id": "e4c0799b-5c10-4ff0-a19a-4f43a14dc253",
    "claim": "We can see from Table 6 that empirically adding logits from two models after classifiers does not perform the best.",
    "label": "refutes",
    "table_id": "e5b471a7-1235-4fa7-b78c-79de04738582"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
    "table_column_names": [
      "[EMPTY]",
      "MSCOCO spice",
      "MSCOCO cider",
      "MSCOCO rouge [ITALIC] L",
      "MSCOCO bleu4",
      "MSCOCO meteor",
      "MSCOCO rep↓",
      "Flickr30k spice",
      "Flickr30k cider",
      "Flickr30k rouge [ITALIC] L",
      "Flickr30k bleu4",
      "Flickr30k meteor",
      "Flickr30k rep↓"
    ],
    "table_content_values": [
      [
        "softmax",
        "18.4",
        "0.967",
        "52.9",
        "29.9",
        "24.9",
        "3.76",
        "13.5",
        "0.443",
        "44.2",
        "19.9",
        "19.1",
        "6.09"
      ],
      [
        "sparsemax",
        "[BOLD] 18.9",
        "[BOLD] 0.990",
        "[BOLD] 53.5",
        "[BOLD] 31.5",
        "[BOLD] 25.3",
        "3.69",
        "[BOLD] 13.7",
        "[BOLD] 0.444",
        "[BOLD] 44.3",
        "[BOLD] 20.7",
        "[BOLD] 19.3",
        "5.84"
      ],
      [
        "TVmax",
        "18.5",
        "0.974",
        "53.1",
        "29.9",
        "25.1",
        "[BOLD] 3.17",
        "13.3",
        "0.438",
        "44.2",
        "20.5",
        "19.0",
        "[BOLD] 3.97"
      ]
    ],
    "id": "6f1ef6d3-b841-4e1e-ab21-64d50093f881",
    "claim": "As can be seen in Table 1, softmax achieves better results overall when compared with sparsemax and TVMAX, indicating that the use of selective attention does not necessarily lead to better captions.",
    "label": "refutes",
    "table_id": "55b8ec67-1f65-4852-943b-d1530519e837"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "f2719604-1c66-4880-9cef-38422fcdc053",
    "claim": "The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6.",
    "label": "supports",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
    "table_column_names": [
      "Cue",
      "[ITALIC] SCOPA",
      "[ITALIC] SB_COPA",
      "Diff.",
      "Prod."
    ],
    "table_content_values": [
      [
        "woman",
        "7.98",
        "4.84",
        "-3.14",
        "0.25"
      ],
      [
        "mother",
        "5.16",
        "3.95",
        "-1.21",
        "0.75"
      ],
      [
        "went",
        "6.00",
        "5.15",
        "-0.85",
        "0.73"
      ],
      [
        "down",
        "5.52",
        "4.93",
        "-0.58",
        "0.71"
      ],
      [
        "into",
        "4.07",
        "3.51",
        "-0.56",
        "0.40"
      ]
    ],
    "id": "588089f9-61e0-4931-8f00-ce35a0d525b9",
    "claim": "(production) column shows their product.",
    "label": "not enough info",
    "table_id": "6609f902-7f48-422d-b5e9-6596405c71dd"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "d4055277-83e0-4021-a90a-417f90def791",
    "claim": "We observe that the average scope length is quite small, with the majority having a scope length of 1.",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 1: Effect of using the shortest dependency path on each relation type.",
    "table_column_names": [
      "[BOLD] Relation",
      "[BOLD] best F1 (in 5-fold) without sdp",
      "[BOLD] best F1 (in 5-fold) with sdp",
      "[BOLD] Diff."
    ],
    "table_content_values": [
      [
        "USAGE",
        "60.34",
        "80.24",
        "+ 19.90"
      ],
      [
        "MODEL-FEATURE",
        "48.89",
        "70.00",
        "+ 21.11"
      ],
      [
        "PART_WHOLE",
        "29.51",
        "70.27",
        "+40.76"
      ],
      [
        "TOPIC",
        "45.80",
        "91.26",
        "+45.46"
      ],
      [
        "RESULT",
        "54.35",
        "81.58",
        "+27.23"
      ],
      [
        "COMPARE",
        "20.00",
        "61.82",
        "+ 41.82"
      ],
      [
        "macro-averaged",
        "50.10",
        "76.10",
        "+26.00"
      ]
    ],
    "id": "c1eced18-5360-4a8e-af31-06277e4a832e",
    "claim": "We find that the effect of syntactic structure varies between the different relation types.",
    "label": "supports",
    "table_id": "1eca16bf-a63a-45a1-a4a3-84f12cf74c95"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "Base ACC",
      "Base Time",
      "+LN ACC",
      "+LN Time",
      "+BERT ACC",
      "+BERT Time",
      "+LN+BERT ACC",
      "+LN+BERT Time"
    ],
    "table_content_values": [
      [
        "Rocktäschel et al. ( 2016 )",
        "Rocktäschel et al. ( 2016 )",
        "250K",
        "83.50",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-"
      ],
      [
        "This",
        "LSTM",
        "8.36M",
        "84.27",
        "0.262",
        "86.03",
        "0.432",
        "89.95",
        "0.544",
        "[BOLD] 90.49",
        "0.696"
      ],
      [
        "This",
        "GRU",
        "6.41M",
        "[BOLD] 85.71",
        "0.245",
        "[BOLD] 86.05",
        "0.419",
        "[BOLD] 90.29",
        "0.529",
        "90.10",
        "0.695"
      ],
      [
        "This",
        "ATR",
        "2.87M",
        "84.88",
        "0.210",
        "85.81",
        "0.307",
        "90.00",
        "0.494",
        "90.28",
        "0.580"
      ],
      [
        "Work",
        "SRU",
        "5.48M",
        "84.28",
        "0.258",
        "85.32",
        "0.283",
        "89.98",
        "0.543",
        "90.09",
        "0.555"
      ],
      [
        "[EMPTY]",
        "LRN",
        "4.25M",
        "84.88",
        "[BOLD] 0.209",
        "85.06",
        "[BOLD] 0.223",
        "89.98",
        "[BOLD] 0.488",
        "89.93",
        "[BOLD] 0.506"
      ]
    ],
    "id": "a63f189d-7408-49c3-bc3c-2b89a01e30cf",
    "claim": "LRN is still the fastest model, outperforming other recurrent units by 8%∼27%.",
    "label": "supports",
    "table_id": "ea085965-3113-4bed-894c-61d30409640d"
  },
  {
    "paper": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
    "paper_id": "1909.02622v2",
    "table_caption": "Table 5: Comparison on hard and soft alignments.",
    "table_column_names": [
      "Metrics",
      "cs-en",
      "de-en",
      "fi-en",
      "lv-en"
    ],
    "table_content_values": [
      [
        "RUSE",
        "0.624",
        "0.644",
        "0.750",
        "0.697"
      ],
      [
        "Hmd-F1 + BERT",
        "0.655",
        "0.681",
        "0.821",
        "0.712"
      ],
      [
        "Hmd-Recall + BERT",
        "0.651",
        "0.658",
        "0.788",
        "0.681"
      ],
      [
        "Hmd-Prec + BERT",
        "0.624",
        "0.669",
        "0.817",
        "0.707"
      ],
      [
        "Wmd-unigram + BERT",
        "0.651",
        "0.686",
        "<bold>0.823</bold>",
        "0.710"
      ],
      [
        "Wmd-bigram + BERT",
        "<bold>0.665</bold>",
        "<bold>0.688</bold>",
        "0.821",
        "<bold>0.712</bold>"
      ]
    ],
    "id": "97d49102-06a2-4887-84ba-121e1a200ed6",
    "claim": "We also observe that WMD-UNIGRAMS slightly outperforms WMD-BIGRAMS on 3 out of 4 language pairs.",
    "label": "refutes",
    "table_id": "ce450197-224e-43a0-8c7e-7bd213655261"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 3: Results of Turn-level Evaluation.",
    "table_column_names": [
      "Dataset",
      "System",
      "Keyword Prediction  [ITALIC] Rw@1",
      "Keyword Prediction  [ITALIC] Rw@3",
      "Keyword Prediction  [ITALIC] Rw@5",
      "Keyword Prediction P@1",
      "Response Retrieval  [ITALIC] R20@1",
      "Response Retrieval  [ITALIC] R20@3",
      "Response Retrieval  [ITALIC] R20@5",
      "Response Retrieval MRR"
    ],
    "table_content_values": [
      [
        "TGPC",
        "Retrieval ",
        "-",
        "-",
        "-",
        "-",
        "0.5063",
        "0.7615",
        "0.8676",
        "0.6589"
      ],
      [
        "TGPC",
        "PMI ",
        "0.0585",
        "0.1351",
        "0.1872",
        "0.0871",
        "0.5441",
        "0.7839",
        "0.8716",
        "0.6847"
      ],
      [
        "TGPC",
        "Neural ",
        "0.0708",
        "0.1438",
        "0.1820",
        "0.1321",
        "0.5311",
        "0.7905",
        "0.8800",
        "0.6822"
      ],
      [
        "TGPC",
        "Kernel ",
        "0.0632",
        "0.1377",
        "0.1798",
        "0.1172",
        "0.5386",
        "0.8012",
        "0.8924",
        "0.6877"
      ],
      [
        "TGPC",
        "DKRN (ours)",
        "[BOLD] 0.0909",
        "[BOLD] 0.1903",
        "[BOLD] 0.2477",
        "[BOLD] 0.1685",
        "[BOLD] 0.5729",
        "[BOLD] 0.8132",
        "[BOLD] 0.8966",
        "[BOLD] 0.7110"
      ],
      [
        "CWC",
        "Retrieval ",
        "-",
        "-",
        "-",
        "-",
        "0.5785",
        "0.8101",
        "0.8999",
        "0.7141"
      ],
      [
        "CWC",
        "PMI ",
        "0.0555",
        "0.1001",
        "0.1212",
        "0.0969",
        "0.5945",
        "0.8185",
        "0.9054",
        "0.7257"
      ],
      [
        "CWC",
        "Neural ",
        "0.0654",
        "0.1194",
        "0.1450",
        "0.1141",
        "0.6044",
        "0.8233",
        "0.9085",
        "0.7326"
      ],
      [
        "CWC",
        "Kernel ",
        "0.0592",
        "0.1113",
        "0.1337",
        "0.1011",
        "0.6017",
        "0.8234",
        "0.9087",
        "0.7320"
      ],
      [
        "CWC",
        "DKRN (ours)",
        "[BOLD] 0.0680",
        "[BOLD] 0.1254",
        "[BOLD] 0.1548",
        "[BOLD] 0.1185",
        "[BOLD] 0.6324",
        "[BOLD] 0.8416",
        "[BOLD] 0.9183",
        "[BOLD] 0.7533"
      ]
    ],
    "id": "9a337795-1c06-4d0e-91f3-3ec46743dc82",
    "claim": "Our approach DKRN outperforms all state-of-the-art methods in terms of all metrics on both datasets with two tasks.",
    "label": "supports",
    "table_id": "907466a1-844b-4d00-9d93-5667496de4b9"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "fc18a2d7-0c48-44fa-af1f-ec7c4d640927",
    "claim": "These observations match our intuition that the learned policy reward will work best with the encoder trained jointly with the policy network",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "8d09348a-d27e-4eb0-98fd-fbd2f76e5dba",
    "claim": "the accuracies for a single vector models are in par with the several vector models.",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "84d8d7a2-2811-41a4-b5a3-1777e9b8af8a",
    "claim": "ALDM even gets worse performance than ACER and PPO.",
    "label": "supports",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
    "table_column_names": [
      "[BOLD] Emoji alias",
      "[BOLD] N",
      "[BOLD] emoji #",
      "[BOLD] emoji %",
      "[BOLD] no-emoji #",
      "[BOLD] no-emoji %",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "mask",
        "163",
        "154",
        "94.48",
        "134",
        "82.21",
        "- 12.27"
      ],
      [
        "two_hearts",
        "87",
        "81",
        "93.10",
        "77",
        "88.51",
        "- 4.59"
      ],
      [
        "heart_eyes",
        "122",
        "109",
        "89.34",
        "103",
        "84.43",
        "- 4.91"
      ],
      [
        "heart",
        "267",
        "237",
        "88.76",
        "235",
        "88.01",
        "- 0.75"
      ],
      [
        "rage",
        "92",
        "78",
        "84.78",
        "66",
        "71.74",
        "- 13.04"
      ],
      [
        "cry",
        "116",
        "97",
        "83.62",
        "83",
        "71.55",
        "- 12.07"
      ],
      [
        "sob",
        "490",
        "363",
        "74.08",
        "345",
        "70.41",
        "- 3.67"
      ],
      [
        "unamused",
        "167",
        "121",
        "72.46",
        "116",
        "69.46",
        "- 3.00"
      ],
      [
        "weary",
        "204",
        "140",
        "68.63",
        "139",
        "68.14",
        "- 0.49"
      ],
      [
        "joy",
        "978",
        "649",
        "66.36",
        "629",
        "64.31",
        "- 2.05"
      ],
      [
        "sweat_smile",
        "111",
        "73",
        "65.77",
        "75",
        "67.57",
        "1.80"
      ],
      [
        "confused",
        "77",
        "46",
        "59.74",
        "48",
        "62.34",
        "2.60"
      ]
    ],
    "id": "c9d32538-fe25-40c4-a010-0b5e2a167331",
    "claim": "Contrary to intuition, the sob emoji contributes more than cry, despite representing a stronger emotion.",
    "label": "refutes",
    "table_id": "2410b1c3-4d48-49e9-9279-dc23daa879d1"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Iteration=1",
        "0.531",
        "0.455",
        "0.353",
        "0.201"
      ],
      [
        "Iteration=2",
        "0.592",
        "0.498",
        "0.385",
        "0.375"
      ],
      [
        "Iteration=3",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ],
      [
        "Iteration=4",
        "0.601",
        "0.505",
        "0.422",
        "0.385"
      ],
      [
        "Iteration=5",
        "0.575",
        "0.495",
        "0.394",
        "0.376"
      ]
    ],
    "id": "4287ef04-6401-4a5f-bf7e-a2d4784e15f4",
    "claim": "If a correct relation is retrieved by the model but is not linked in the knowledge base, the precision increases as the recall rate increases.",
    "label": "not enough info",
    "table_id": "e8d636ab-66cd-4c48-97ea-e05fd1ff69c0"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "e1a148e5-952a-4c09-a758-a6b498726764",
    "claim": "G-Pre, for example, indicates that for the “good” summaries, an average of 39.2% of their words overlap with those from references, suggesting that a good summary has much more than an “ad-hoc” fraction of correct words.",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Assessing Gender Bias in Machine Translation – A Case Study with Google Translate",
    "paper_id": "1809.02208v4",
    "table_caption": "Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
    "table_column_names": [
      "Category",
      "Female (%)",
      "Male (%)",
      "Neutral (%)"
    ],
    "table_content_values": [
      [
        "Office and administrative support",
        "11.015",
        "58.812",
        "16.954"
      ],
      [
        "Architecture and engineering",
        "2.299",
        "72.701",
        "10.92"
      ],
      [
        "Farming, fishing, and forestry",
        "12.179",
        "62.179",
        "14.744"
      ],
      [
        "Management",
        "11.232",
        "66.667",
        "12.681"
      ],
      [
        "Community and social service",
        "20.238",
        "62.5",
        "10.119"
      ],
      [
        "Healthcare support",
        "25.0",
        "43.75",
        "17.188"
      ],
      [
        "Sales and related",
        "8.929",
        "62.202",
        "16.964"
      ],
      [
        "Installation, maintenance, and repair",
        "5.22",
        "58.333",
        "17.125"
      ],
      [
        "Transportation and material moving",
        "8.81",
        "62.976",
        "17.5"
      ],
      [
        "Legal",
        "11.905",
        "72.619",
        "10.714"
      ],
      [
        "Business and financial operations",
        "7.065",
        "67.935",
        "15.58"
      ],
      [
        "Life, physical, and social science",
        "5.882",
        "73.284",
        "10.049"
      ],
      [
        "Arts, design, entertainment, sports, and media",
        "10.36",
        "67.342",
        "11.486"
      ],
      [
        "Education, training, and library",
        "23.485",
        "53.03",
        "9.091"
      ],
      [
        "Building and grounds cleaning and maintenance",
        "12.5",
        "68.333",
        "11.667"
      ],
      [
        "Personal care and service",
        "18.939",
        "49.747",
        "18.434"
      ],
      [
        "Healthcare practitioners and technical",
        "22.674",
        "51.744",
        "15.116"
      ],
      [
        "Production",
        "14.331",
        "51.199",
        "18.245"
      ],
      [
        "Computer and mathematical",
        "4.167",
        "66.146",
        "14.062"
      ],
      [
        "Construction and extraction",
        "8.578",
        "61.887",
        "17.525"
      ],
      [
        "Protective service",
        "8.631",
        "65.179",
        "12.5"
      ],
      [
        "Food preparation and serving related",
        "21.078",
        "58.333",
        "17.647"
      ],
      [
        "Total",
        "11.76",
        "58.93",
        "15.939"
      ]
    ],
    "id": "0988097c-eeaa-4876-91cc-424a0e4d7f65",
    "claim": "What we have found is that Google Translate does not always translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, as evidenced by the data in Table 6.",
    "label": "refutes",
    "table_id": "2e483d7f-201a-4e26-b179-5216c375183e"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "59876715-8c94-4df5-8027-281cc74e8292",
    "claim": "Among all the baselines, GDPL obtains the most preference against PPO.",
    "label": "supports",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "ea4f2d3a-dae7-4eab-9d17-132bc404395a",
    "claim": "we build two agents for disentanglement learning: a generative model-based conversation model (GP-MBCM) and an end-to-end variant of the Adversarial Learning based Dialogue Model (ALDM).",
    "label": "not enough info",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "79301cb3-91f0-42b8-abe9-90fe05d52e87",
    "claim": "due to the monotonic nature of the 5-action generation problem, the greedy algorithm and fixed threshold based policies achieve close to perfect action selections, although the top-k sampling strategy is still able to generate more diverse and natural responses on the test set with augmented data",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. “Pref%”: in how many percentage of documents a system receives the higher human rating.",
    "table_column_names": [
      "Reward",
      "R-1",
      "R-2",
      "R-L",
      "Human",
      "Pref%"
    ],
    "table_content_values": [
      [
        "R-L (original)",
        "40.9",
        "17.8",
        "38.5",
        "1.75",
        "15"
      ],
      [
        "Learned (ours)",
        "39.2",
        "17.4",
        "37.5",
        "[BOLD] 2.20",
        "[BOLD] 75"
      ]
    ],
    "id": "0633fe26-997d-4980-b1c3-69077f797d1e",
    "claim": "It is clear from Table 5 that using the learned reward does not help the RL-based system generate summaries with significantly higher human ratings.",
    "label": "refutes",
    "table_id": "f5343270-2393-4ff2-85d1-a445b36215ea"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "8a19218e-4f09-4024-993b-dba930690f7b",
    "claim": "Lastly, BERT-large models do not fine-tune well (as opposed to RoBERTa).",
    "label": "not enough info",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result Lample et al. (2016).",
    "table_column_names": [
      "Model",
      "#Params",
      "NER"
    ],
    "table_content_values": [
      [
        "LSTM*",
        "-",
        "90.94"
      ],
      [
        "LSTM",
        "245K",
        "[BOLD] 89.61"
      ],
      [
        "GRU",
        "192K",
        "89.35"
      ],
      [
        "ATR",
        "87K",
        "88.46"
      ],
      [
        "SRU",
        "161K",
        "88.89"
      ],
      [
        "LRN",
        "129K",
        "88.56"
      ]
    ],
    "id": "98be01ed-6aba-4c93-9b7b-d19e39a83138",
    "claim": "As shown in Table 6, the performance of LRN is significantly lower than that of LSTM and GRU (-1.05 and -0.79).",
    "label": "refutes",
    "table_id": "11c65341-21bb-418e-b809-9cbf7eaecf52"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).",
    "table_column_names": [
      "Model",
      "#Params",
      "Base",
      "+Elmo"
    ],
    "table_content_values": [
      [
        "rnet*",
        "-",
        "71.1/79.5",
        "-/-"
      ],
      [
        "LSTM",
        "2.67M",
        "[BOLD] 70.46/78.98",
        "75.17/82.79"
      ],
      [
        "GRU",
        "2.31M",
        "70.41/ [BOLD] 79.15",
        "75.81/83.12"
      ],
      [
        "ATR",
        "1.59M",
        "69.73/78.70",
        "75.06/82.76"
      ],
      [
        "SRU",
        "2.44M",
        "69.27/78.41",
        "74.56/82.50"
      ],
      [
        "LRN",
        "2.14M",
        "70.11/78.83",
        "[BOLD] 76.14/ [BOLD] 83.83"
      ]
    ],
    "id": "043020a0-d7f8-48bf-b2be-50dbed48a648",
    "claim": "Table 4 lists the EM/F1 score of different models.",
    "label": "supports",
    "table_id": "e8718aba-7d2a-43da-ab9f-fe42d951b94a"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "e09138ac-6dc6-45f0-a0e3-2fc276e7b93a",
    "claim": "Excluding the direction aggregation module does not lead to a performance drop to 24.6 BLEU points.",
    "label": "refutes",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "What do Deep Networks Like to Read?",
    "paper_id": "1909.04547v1",
    "table_caption": "Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
    "table_column_names": [
      "[EMPTY]",
      "<bold>RNN</bold>",
      "<bold>CNN</bold>",
      "<bold>DAN</bold>"
    ],
    "table_content_values": [
      [
        "Positive",
        "+9.7",
        "+4.3",
        "+<bold>23.6</bold>"
      ],
      [
        "Negative",
        "+6.9",
        "+5.5",
        "+<bold>16.1</bold>"
      ],
      [
        "Flipped to Positive",
        "+20.2",
        "+24.9",
        "+27.4"
      ],
      [
        "Flipped to Negative",
        "+31.5",
        "+28.6",
        "+19.3"
      ]
    ],
    "id": "239b62aa-ebc4-4f36-aa92-d44876612730",
    "claim": "We see a varying increase in sentiment value across all three models after finetuning, indicating that the framework is not always able to pick up on words that are indicative of sentiment.",
    "label": "refutes",
    "table_id": "fe569d5a-7fe9-4318-80c3-68d5d6108755"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Inference",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training",
      "Throughput (instances/s) Training"
    ],
    "table_content_values": [
      [
        "Batch size",
        "Iter",
        "Recur",
        "Fold",
        "Iter",
        "Recur",
        "Fold"
      ],
      [
        "1",
        "19.2",
        "81.4",
        "16.5",
        "2.5",
        "4.8",
        "9.0"
      ],
      [
        "10",
        "49.3",
        "217.9",
        "52.2",
        "4.0",
        "4.2",
        "37.5"
      ],
      [
        "25",
        "72.1",
        "269.9",
        "61.6",
        "5.5",
        "3.6",
        "54.7"
      ]
    ],
    "id": "531171f1-fe4b-4849-81ec-36b06b6eb36f",
    "claim": "The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput.",
    "label": "supports",
    "table_id": "82989071-ae58-4870-9a3b-7e5f7a1ea4e7"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "edca66cb-3a89-4544-9002-165bea2531a9",
    "claim": "the lowest mean turns per successful conversation from the human test user (HUS) was achieved by GDPL (mean HUS turns 20.8)",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "fdbb1a73-528d-4505-afaf-f182c3127253",
    "claim": "the neural user simulator trained by GDPL outperforms the other models, in terms of both more turns per successful session (i.e., human-like turns, GDPL = 19.7) and success rate, as shown in Table 6.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 4: Cue classification on the test set.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] F-Score  [BOLD] Baseline",
      "[BOLD] F-Score  [BOLD] Proposed",
      "[BOLD] Support"
    ],
    "table_content_values": [
      [
        "False cues",
        "0.61",
        "0.68",
        "47"
      ],
      [
        "Actual cues",
        "0.97",
        "0.98",
        "557"
      ]
    ],
    "id": "c94dd088-6c09-435e-b57b-9a983da5a990",
    "claim": "This is mainly because the LSTM uses contextual information such as preceding cue words and preceding reactions, but the false cues are often individual words rather than phrases.",
    "label": "not enough info",
    "table_id": "08a8fe8c-92a2-41dc-a6e0-f97b95380cae"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "6cc40bb2-3a34-433d-9612-b5a5df7ae632",
    "claim": "in contrast, the proposed method obtains better and more robust performances than both baselines, with a 0.06 higher PCS and a 0.27 increase in in-scope recall",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "a0b9120d-320d-4547-967f-d8b3eb9529f2",
    "claim": "The average number of tokens per tweet is 22.3, per sentence is 13.6 and average scope length is 2.9.",
    "label": "supports",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "c0e45fc9-0434-4421-bdaf-7b40f7afac29",
    "claim": "In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information.",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "d94636ce-6e6c-4c0a-8766-510fbd289527",
    "claim": "to quantify the contribution of each model component on this task, we vary the model’s architecture by progressively adding context and the dependency feature by applying parameter sharing or via a pretrained neural classifier, obtaining comparable results.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "91a29994-676f-4490-8b85-b37396c20d2b",
    "claim": "That ambiguity can be reflected in the length of negation scope.We found that most negation scopes only involve one or two tokens",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "6592737a-49c3-4723-b433-e554703165cd",
    "claim": "[CONTINUE] G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 4: Accuracy of transferring between domains. Models with † use labeled data from source domains and unlabeled data from the target domain. Models with ‡ use human rationales on the target task.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer look + Beer aroma + Beer palate",
        "Hotel location",
        "78.65",
        "79.09",
        "79.28",
        "80.42",
        "82.10",
        "[BOLD] 84.52",
        "85.43"
      ],
      [
        "Beer look + Beer aroma + Beer palate",
        "Hotel cleanliness",
        "86.44",
        "86.68",
        "89.01",
        "86.95",
        "87.15",
        "[BOLD] 90.66",
        "92.09"
      ],
      [
        "Beer look + Beer aroma + Beer palate",
        "Hotel service",
        "85.34",
        "86.61",
        "87.91",
        "87.37",
        "86.40",
        "[BOLD] 89.93",
        "92.42"
      ]
    ],
    "id": "499719d6-acd1-40d1-8962-3bd945f7691c",
    "claim": "The error reduction over the best baseline is only 5.09% on average.",
    "label": "refutes",
    "table_id": "6258d803-d62d-43e0-9e7c-29fb98cf0ad8"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "452f09ad-4e54-4e45-b65a-f07fc4c3055e",
    "claim": "For slot values, the performance is poor when actions are absent, where it is only possible to generate a few fixed templates.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "What do Deep Networks Like to Read?",
    "paper_id": "1909.04547v1",
    "table_caption": "Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.",
    "table_column_names": [
      "Orig",
      "<u> turns in a <u> screenplay that <u> at the edges ; it ’s so clever you want to hate it ."
    ],
    "table_content_values": [
      [
        "DAN",
        "<u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate"
      ],
      [
        "CNN",
        "she turns on a on ( ( in in the the the edges ’s so clever “ want to hate it ”"
      ],
      [
        "RNN",
        "<u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it ."
      ]
    ],
    "id": "56344d9a-7b9f-4606-ac3b-ef1134c5db28",
    "claim": "In contrast, DAN masks out punctuation and determiners using words indicative of the class label (i.e.",
    "label": "supports",
    "table_id": "a2709d0e-c174-42ae-8a8b-4610a47e1991"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "62f2b650-5f5f-46e6-8570-bd6ff0013ea0",
    "claim": "TF has the best values of recall and f-measure for all corpora except the English version of TED Talks, where DF has the best value of recall and HClust has the best value of f-measure.",
    "label": "refutes",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "c40d9860-c431-4f1b-8575-cd9e463b4967",
    "claim": "the DA-RL method beats the DA-SLU methods on most criteria",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "fd27b181-4ffc-4033-89c1-cae259504aad",
    "claim": "For the 10-action experiments, the improvements are by 0.33 and 1.39 respectively.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Present",
      "[BOLD] Not Present"
    ],
    "table_content_values": [
      [
        "Emoji",
        "4805 (76.6%)",
        "23952 (68.0%)"
      ],
      [
        "Hashtags",
        "2122 (70.5%)",
        "26635 (69.4%)"
      ]
    ],
    "id": "f3feea83-901a-4779-ab3b-bbd37a7b2ad8",
    "claim": "[CONTINUE] Tweets containing emoji seem to be easier for the model to classify than those without.",
    "label": "supports",
    "table_id": "76b1831c-65dd-45d3-bdc5-b490b3a9cee2"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "8e8f27d4-b0f5-43ef-b7ea-55a8d400fc5e",
    "claim": "[CONTINUE] The relative lower BLEU score of our DAMD model compared to other models with different system action forms suggests that it does not outperform them in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), it is not enough to make up for the lower BLEU score, [CONTINUE] Moreover, even if a model has access to ground truth system action, the model does not necessarily improve its task performance.",
    "label": "refutes",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "8da915c7-59a0-473f-9ae4-dc07094a27f0",
    "claim": "Table 3 shows the impact of coverage for decreasing generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL.",
    "label": "refutes",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "ff46875b-b5a0-4a1d-81cf-3120d135efad",
    "claim": "Overall, ECA gains an average improvement of 10.5% over BLEU and 5.2% over METEOR.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "edebc118-50fc-421c-a113-85111fe97188",
    "claim": "we can also see that our method lags somewhat behind the state of the art on ROUGE, it achieves comparable ROUGE scores in comparison with RL-based systems on the CNN-DM dataset.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large",
        "B-COPA",
        "70.5 (± 2.5)",
        "72.6 (± 2.3)",
        "[BOLD] 69.1 (± 2.7)"
      ],
      [
        "BERT-large",
        "B-COPA (50%)",
        "69.9 (± 1.9)",
        "71.2 (± 1.3)",
        "69.0 (± 3.5)"
      ],
      [
        "BERT-large",
        "COPA",
        "[BOLD] 71.7 (± 0.5)",
        "[BOLD] 80.5 (± 0.4)",
        "66.3 (± 0.8)"
      ],
      [
        "RoBERTa-large",
        "B-COPA",
        "[BOLD] 76.7 (± 0.8)",
        "73.3 (± 1.5)",
        "[BOLD] 78.8 (± 2.0)"
      ],
      [
        "RoBERTa-large",
        "B-COPA (50%)",
        "72.4 (± 2.0)",
        "72.1 (± 1.7)",
        "72.6 (± 2.1)"
      ],
      [
        "RoBERTa-large",
        "COPA",
        "76.4 (± 0.7)",
        "[BOLD] 79.6 (± 1.0)",
        "74.4 (± 1.1)"
      ],
      [
        "BERT-base-NSP",
        "None",
        "[BOLD] 66.4",
        "66.2",
        "[BOLD] 66.7"
      ],
      [
        "BERT-large-NSP",
        "None",
        "65.0",
        "[BOLD] 66.9",
        "62.1"
      ]
    ],
    "id": "1ad45c40-5107-4f03-9741-d43fe4bf9bb5",
    "claim": "The relatively high accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are already well-equipped to perform this task \"out-of-the-box\".",
    "label": "supports",
    "table_id": "c55336a9-88a3-4f6b-8768-db03e1b246fb"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "d1e8aea0-544c-4fb8-bd06-c0c06214a07f",
    "claim": "comparing with the standard MD model, our models can generate more diverse responses (DAMD: 3.12 vs 3.65, HDSA: 2.14 vs 2.67), and our DAMD model (with external data)  can also generate more appropriate responses (2.50 vs 2.53).",
    "label": "not enough info",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
    "table_column_names": [
      "Cue",
      "[ITALIC] SCOPA",
      "[ITALIC] SB_COPA",
      "Diff.",
      "Prod."
    ],
    "table_content_values": [
      [
        "woman",
        "7.98",
        "4.84",
        "-3.14",
        "0.25"
      ],
      [
        "mother",
        "5.16",
        "3.95",
        "-1.21",
        "0.75"
      ],
      [
        "went",
        "6.00",
        "5.15",
        "-0.85",
        "0.73"
      ],
      [
        "down",
        "5.52",
        "4.93",
        "-0.58",
        "0.71"
      ],
      [
        "into",
        "4.07",
        "3.51",
        "-0.56",
        "0.40"
      ]
    ],
    "id": "d59d4e8e-7158-4781-8497-005678f26dbd",
    "claim": "This is corroborated by the negative difference of associated product scores.",
    "label": "not enough info",
    "table_id": "6609f902-7f48-422d-b5e9-6596405c71dd"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "31045e85-aa4a-47a2-97ba-f540814dee11",
    "claim": "capsule net improves the performance significantly by removing this residual connection, which is also confirmed in Table 4 where there is a slight increase in AUC when replacing capsule net with pure max-pooling operation in graph encoder",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
    "paper_id": "1909.02622v2",
    "table_caption": "Table 4: Pearson correlation with system-level human judgments on MSCOCO dataset. ’M’ and ’P’ are short names.",
    "table_column_names": [
      "Setting",
      "Metric",
      "M1",
      "M2"
    ],
    "table_content_values": [
      [
        "Baselines",
        "LEIC(*)",
        "<bold>0.939</bold>",
        "<bold>0.949</bold>"
      ],
      [
        "Baselines",
        "METEOR",
        "0.606",
        "0.594"
      ],
      [
        "Baselines",
        "SPICE",
        "0.759",
        "0.750"
      ],
      [
        "Baselines",
        "BERTScore-Recall",
        "0.809",
        "0.749"
      ],
      [
        "Sent-Mover",
        "SMD + W2V",
        "0.683",
        "0.668"
      ],
      [
        "Sent-Mover",
        "SMD + ELMO + P",
        "0.709",
        "0.712"
      ],
      [
        "Sent-Mover",
        "SMD + BERT + P",
        "0.723",
        "0.747"
      ],
      [
        "Sent-Mover",
        "SMD + BERT + M + P",
        "0.789",
        "0.784"
      ],
      [
        "Word-Mover",
        "Wmd-1 + W2V",
        "0.728",
        "0.764"
      ],
      [
        "Word-Mover",
        "Wmd-1 + ELMO + P",
        "0.753",
        "0.775"
      ],
      [
        "Word-Mover",
        "Wmd-1 + BERT + P",
        "0.780",
        "0.790"
      ],
      [
        "Word-Mover",
        "Wmd-1 + BERT + M + P",
        "<bold>0.813</bold>",
        "<bold>0.810</bold>"
      ],
      [
        "Word-Mover",
        "Wmd-2 + BERT + M + P",
        "0.812",
        "0.808"
      ]
    ],
    "id": "754e6967-568c-467b-8192-79e841cef788",
    "claim": "Table 4: Word mover metrics outperform all baselines except for the supervised metric LEIC, which uses more information by considering both images and texts.",
    "label": "supports",
    "table_id": "a96972a3-a95b-4918-b463-52961b2e16c9"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
    "table_column_names": [
      "[BOLD] Representation",
      "[BOLD] Hyper parameters Filter size",
      "[BOLD] Hyper parameters Num. Feature maps",
      "[BOLD] Hyper parameters Activation func.",
      "[BOLD] Hyper parameters L2 Reg.",
      "[BOLD] Hyper parameters Learning rate",
      "[BOLD] Hyper parameters Dropout Prob.",
      "[BOLD] F1.(avg. in 5-fold) with default values",
      "[BOLD] F1.(avg. in 5-fold) with optimal values"
    ],
    "table_content_values": [
      [
        "CoNLL08",
        "4-5",
        "1000",
        "Softplus",
        "1.15e+01",
        "1.13e-03",
        "1",
        "73.34",
        "74.49"
      ],
      [
        "SB",
        "4-5",
        "806",
        "Sigmoid",
        "8.13e-02",
        "1.79e-03",
        "0.87",
        "72.83",
        "[BOLD] 75.05"
      ],
      [
        "UD v1.3",
        "5",
        "716",
        "Softplus",
        "1.66e+00",
        "9.63E-04",
        "1",
        "68.93",
        "69.57"
      ]
    ],
    "id": "7a5a63e4-676b-42ae-8599-591b3465f476",
    "claim": "We see that the optimized parameter settings are consistent across the different representations, showing that tuning is not necessary for these types of comparisons.",
    "label": "refutes",
    "table_id": "602023a1-3ab5-4788-8101-59ae8f0c6ce1"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "69ed231d-8e01-4f0f-8360-c65a8d37d64c",
    "claim": "Although the punctuation-based heuristic works reasonably well, it is prone to error in the face of tokens not separated by punctuation, particularly in complex sentences such as example number 1",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches",
    "paper_id": "1904.01172v3",
    "table_caption": "Table 2: Comparison of exact-match accuracy achieved on selected benchmarks by a random or majority-choice baseline, various neural contextual embedding models, and humans. ELMo refers to the highest-performing listed approach using ELMo embeddings. Best system performance on each benchmark in bold. Information extracted from leaderboards (linked to in the first column) at time of writing (October 2019), and original papers for benchmarks introduced in Section 2.",
    "table_column_names": [
      "[BOLD] Benchmark",
      "[BOLD]  Simple Baseline ",
      "[BOLD] ELMo",
      "[BOLD] GPT",
      "[BOLD] BERT",
      "[BOLD] MT-DNN",
      "[BOLD] XLNet",
      "[BOLD] RoBERTa",
      "[BOLD] ALBERT",
      "[BOLD] Human"
    ],
    "table_content_values": [
      [
        "[BOLD] CLOTH",
        "25.0",
        "70.7",
        "–",
        "[BOLD] 86.0",
        "–",
        "–",
        "–",
        "–",
        "85.9"
      ],
      [
        "[BOLD] Cosmos QA",
        "–",
        "–",
        "54.5",
        "67.1",
        "–",
        "–",
        "–",
        "–",
        "94.0"
      ],
      [
        "[BOLD] DREAM",
        "33.4",
        "59.5",
        "55.5",
        "66.8",
        "–",
        "[BOLD] 72.0",
        "–",
        "–",
        "95.5"
      ],
      [
        "[BOLD] GLUE",
        "–",
        "70.0",
        "–",
        "80.5",
        "87.6",
        "88.4",
        "88.5",
        "[BOLD] 89.4",
        "87.1"
      ],
      [
        "[BOLD] HellaSWAG",
        "25.0",
        "33.3",
        "41.7",
        "47.3",
        "–",
        "–",
        "[BOLD] 85.2",
        "[EMPTY]",
        "95.6"
      ],
      [
        "[BOLD] MC-TACO",
        "17.4",
        "26.4",
        "–",
        "42.7",
        "–",
        "–",
        "[BOLD] 43.6",
        "–",
        "75.8"
      ],
      [
        "[BOLD] RACE",
        "24.9",
        "–",
        "59.0",
        "72.0",
        "–",
        "81.8",
        "83.2",
        "[BOLD] 89.4",
        "94.5"
      ],
      [
        "[BOLD] SciTail",
        "60.3",
        "–",
        "88.3",
        "–",
        "94.1",
        "–",
        "–",
        "–",
        "–"
      ],
      [
        "[BOLD] SQuAD 1.1",
        "1.3",
        "81.0",
        "–",
        "87.4",
        "–",
        "[BOLD] 89.9",
        "–",
        "–",
        "82.3"
      ],
      [
        "[BOLD] SQuAD 2.0",
        "48.9",
        "63.4",
        "–",
        "80.8",
        "–",
        "86.3",
        "86.8",
        "[BOLD] 89.7",
        "86.9"
      ],
      [
        "[BOLD] SuperGLUE",
        "47.1",
        "–",
        "–",
        "69.0",
        "–",
        "–",
        "[BOLD] 84.6",
        "–",
        "89.8"
      ],
      [
        "[BOLD] SWAG",
        "25.0",
        "59.1",
        "78.0",
        "86.3",
        "87.1",
        "–",
        "[BOLD] 89.9",
        "–",
        "88.0"
      ]
    ],
    "id": "c2398213-ef73-4862-8f2d-48b607c14e26",
    "claim": "The most representative models are only BERT and its variants.",
    "label": "refutes",
    "table_id": "03f667dc-57c4-41c7-a8fa-00978b4ca84f"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "e752385e-f0a0-4fa9-b573-8198a4d3bf24",
    "claim": "The relative improvement averaged over all tasks is 8%.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "8b062e9b-8f83-4dd6-b370-1a476c743858",
    "claim": "The HAN models outperform MEAD in terms of sentence prediction.",
    "label": "supports",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "5e5f4e5e-fc27-408f-92a5-1d9b04571613",
    "claim": "the question attention mechanism performs very poorly on out-of-domain questions, and shows no relative improvement when enhanced with attention over the span representations.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "782d2043-8a43-4dc8-9989-3d1e544a66c8",
    "claim": "Despite joint training, our hybrid model does not learn to pick up the best features from CBOW and CMOW simultaneously.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "[BOLD] 71.2"
      ]
    ],
    "id": "dc098fdc-48d1-455f-933a-9d30abd36dfe",
    "claim": "our model is able to reduce the gap between the within-document and cross-document entity coreference metrics on the ECB+",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Towards Scalable and Reliable Capsule Networksfor Challenging NLP Applications",
    "paper_id": "1906.02829v1",
    "table_caption": "Table 2: Comparisons of our NLP-Cap approach and baselines on two text classification benchmarks, where ’-’ denotes methods that failed to scale due to memory issues.",
    "table_column_names": [
      "<bold>Datasets</bold>",
      "<bold>Metrics</bold>",
      "<bold>FastXML</bold>",
      "<bold>PD-Sparse</bold>",
      "<bold>FastText</bold>",
      "<bold>Bow-CNN</bold>",
      "<bold>CNN-Kim</bold>",
      "<bold>XML-CNN</bold>",
      "<bold>Cap-Zhao</bold>",
      "<bold>NLP-Cap</bold>",
      "<bold>Impv</bold>"
    ],
    "table_content_values": [
      [
        "RCV1",
        "PREC@1",
        "94.62",
        "95.16",
        "95.40",
        "96.40",
        "93.54",
        "96.86",
        "96.63",
        "<bold>97.05</bold>",
        "+0.20%"
      ],
      [
        "RCV1",
        "PREC@3",
        "78.40",
        "79.46",
        "79.96",
        "81.17",
        "76.15",
        "81.11",
        "81.02",
        "<bold>81.27</bold>",
        "+0.20%"
      ],
      [
        "RCV1",
        "PREC@5",
        "54.82",
        "55.61",
        "55.64",
        "<bold>56.74</bold>",
        "52.94",
        "56.07",
        "56.12",
        "56.33",
        "-0.72%"
      ],
      [
        "[EMPTY]",
        "NDCG@1",
        "94.62",
        "95.16",
        "95.40",
        "96.40",
        "93.54",
        "96.88",
        "96.63",
        "<bold>97.05</bold>",
        "+0.20%"
      ],
      [
        "[EMPTY]",
        "NDCG@3",
        "89.21",
        "90.29",
        "90.95",
        "92.04",
        "87.26",
        "92.22",
        "92.31",
        "<bold>92.47</bold>",
        "+0.17%"
      ],
      [
        "[EMPTY]",
        "NDCG@5",
        "90.27",
        "91.29",
        "91.68",
        "92.89",
        "88.20",
        "92.63",
        "92.75",
        "<bold>93.11</bold>",
        "+0.52%"
      ],
      [
        "EUR-Lex",
        "PREC@1",
        "68.12",
        "72.10",
        "71.51",
        "64.99",
        "68.35",
        "75.65",
        "-",
        "<bold>80.20</bold>",
        "+6.01%"
      ],
      [
        "EUR-Lex",
        "PREC@3",
        "57.93",
        "57.74",
        "60.37",
        "51.68",
        "54.45",
        "61.81",
        "-",
        "<bold>65.48</bold>",
        "+5.93%"
      ],
      [
        "EUR-Lex",
        "PREC@5",
        "48.97",
        "47.48",
        "50.41",
        "42.32",
        "44.07",
        "50.90",
        "-",
        "<bold>52.83</bold>",
        "+3.79%"
      ],
      [
        "[EMPTY]",
        "NDCG@1",
        "68.12",
        "72.10",
        "71.51",
        "64.99",
        "68.35",
        "75.65",
        "-",
        "<bold>80.20</bold>",
        "+6.01%"
      ],
      [
        "[EMPTY]",
        "NDCG@3",
        "60.66",
        "61.33",
        "63.32",
        "55.03",
        "59.81",
        "66.71",
        "-",
        "<bold>71.11</bold>",
        "+6.59%"
      ],
      [
        "[EMPTY]",
        "NDCG@5",
        "56.42",
        "55.93",
        "58.56",
        "49.92",
        "57.99",
        "64.45",
        "-",
        "<bold>68.80</bold>",
        "+6.75%"
      ]
    ],
    "id": "f161e252-7353-46ef-97f7-408e240403cf",
    "claim": "In Table 2, we can see that our capsule-based approach does not bring a noticeable margin over the strong baselines on EUR-Lex, and only competitive results on RCV1.",
    "label": "refutes",
    "table_id": "59a5a6f8-3968-434e-a516-646a136fde69"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "a2dc6dbd-cd04-49eb-b388-2103d2295958",
    "claim": "And it lacks the performance when compared with the more sophisticated RL algorithms.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 2: Experiment 1",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.001",
        "0.003",
        "-20.818",
        "***",
        "0.505"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.083",
        "0.048",
        "101.636",
        "***",
        "1.724"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.001",
        "0.001",
        "0.035",
        "[EMPTY]",
        "1.001"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.023",
        "0.012",
        "64.418",
        "***",
        "1.993"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.002",
        "0.001",
        "4.047",
        "***",
        "1.120"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.049",
        "0.019",
        "120.986",
        "***",
        "2.573"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.173",
        "0.065",
        "243.285",
        "***",
        "2.653"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.032",
        "0.023",
        "39.483",
        "***",
        "1.396"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.111",
        "0.061",
        "122.707",
        "***",
        "1.812"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.178",
        "0.080",
        "211.319",
        "***",
        "2.239"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.028",
        "0.015",
        "63.131",
        "***",
        "1.854"
      ]
    ],
    "id": "6d1edb41-733b-4b4f-9eee-201b52781e80",
    "claim": "In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes.",
    "label": "supports",
    "table_id": "661ddd6e-51d9-446c-add1-3e80f3b78109"
  },
  {
    "paper": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection",
    "paper_id": "1904.04388v1",
    "table_caption": "Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Model",
      "[BOLD] dev mean",
      "[BOLD] dev best",
      "[BOLD] test mean",
      "[BOLD] test best",
      "[ITALIC] α"
    ],
    "table_content_values": [
      [
        "single",
        "text",
        "86.54",
        "86.80",
        "86.47",
        "86.96",
        "–"
      ],
      [
        "single",
        "raw",
        "35.00",
        "37.33",
        "35.78",
        "37.70",
        "–"
      ],
      [
        "single",
        "innovations",
        "80.86",
        "81.51",
        "80.28",
        "82.15",
        "–"
      ],
      [
        "early",
        "text + raw",
        "86.46",
        "86.65",
        "86.24",
        "86.53",
        "–"
      ],
      [
        "early",
        "text + innovations",
        "86.53",
        "86.77",
        "86.54",
        "87.00",
        "–"
      ],
      [
        "early",
        "text + raw + innovations",
        "86.35",
        "86.69",
        "86.55",
        "86.44",
        "–"
      ],
      [
        "late",
        "text + raw",
        "86.71",
        "87.05",
        "86.35",
        "86.71",
        "0.2"
      ],
      [
        "late",
        "text + innovations",
        "[BOLD] 86.98",
        "[BOLD] 87.48",
        "[BOLD] 86.68",
        "[BOLD] 87.02",
        "0.5"
      ],
      [
        "late",
        "text + raw + innovations",
        "86.95",
        "87.30",
        "86.60",
        "86.87",
        "0.5"
      ]
    ],
    "id": "3b3489c6-cb2f-4a1a-b462-ec4844bcfbf2",
    "claim": "The interpolation weight α for the late fusion experiments is low when innovations are used, which further indicates that innovation features are not useful in overall prediction.",
    "label": "refutes",
    "table_id": "f0441bd9-731a-41df-8a83-1d084b332e89"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "8240b880-6e23-4e88-b9ab-e8183400245c",
    "claim": "we see that the average ROUGE-L of the RL systems is similar to the supervised models (e.g., Zhang et\\xa0al.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Original",
      "Italian Debiased",
      "Italian English",
      "Italian Reduction",
      "German Original",
      "German Debiased",
      "German English",
      "German Reduction"
    ],
    "table_content_values": [
      [
        "Same Gender",
        "0.442",
        "0.434",
        "0.424",
        "–",
        "0.491",
        "0.478",
        "0.446",
        "–"
      ],
      [
        "Different Gender",
        "0.385",
        "0.421",
        "0.415",
        "–",
        "0.415",
        "0.435",
        "0.403",
        "–"
      ],
      [
        "difference",
        "0.057",
        "0.013",
        "0.009",
        "[BOLD] 91.67%",
        "0.076",
        "0.043",
        "0.043",
        "[BOLD] 100%"
      ]
    ],
    "id": "c3215ba9-f925-4d0e-9090-74e533d5c180",
    "claim": "As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much higher.",
    "label": "refutes",
    "table_id": "a3f7a8f4-e69f-4e32-beb4-281137d9a4a5"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "a5f84823-7cbf-4c56-9c4c-a5c245145720",
    "claim": "On the same dataset, our results are not as competitive as Damonte and Cohen (2019).",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Deriving Machine Attention from Human Rationales",
    "paper_id": "1808.09367v1",
    "table_caption": "Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.",
    "table_column_names": [
      "Source",
      "Target",
      "Svm",
      "Ra-Svm‡",
      "Ra-Cnn‡",
      "Trans†",
      "Ra-Trans‡†",
      "Ours‡†",
      "Oracle†"
    ],
    "table_content_values": [
      [
        "Beer aroma+palate",
        "Beer look",
        "74.41",
        "74.83",
        "74.94",
        "72.75",
        "76.41",
        "[BOLD] 79.53",
        "80.29"
      ],
      [
        "Beer look+palate",
        "Beer aroma",
        "68.57",
        "69.23",
        "67.55",
        "69.92",
        "76.45",
        "[BOLD] 77.94",
        "78.11"
      ],
      [
        "Beer look+aroma",
        "Beer palate",
        "63.88",
        "67.82",
        "65.72",
        "74.66",
        "73.40",
        "[BOLD] 75.24",
        "75.50"
      ]
    ],
    "id": "ce51a0ff-e0da-4d8c-84db-e4cabdbaac82",
    "claim": "It does not match the performance of ORACLE, with a difference of up to 6.29% absolute difference.",
    "label": "refutes",
    "table_id": "461a04b0-4d80-4f54-a0ed-42b74709e562"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "eb9d6e8f-d389-49e1-a146-61202625fda6",
    "claim": "The results in the table suggest that cleaning the missing slots did not provide more complex training examples.",
    "label": "refutes",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "2eaca02a-6756-46bf-99d2-d597218b717d",
    "claim": "This indicates that GINs cannot be employed in tasks where the distribution of node degrees has a long tail.",
    "label": "refutes",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "bff39859-b6c2-4366-92af-299cc4df3f54",
    "claim": "despite such simplification, our system consistently outperforms the two baselines by a wide margin across all three evaluation criteria.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "fb295289-5470-4bd0-99a4-18c93946d800",
    "claim": "The coverage mechanism is also effective in our models.",
    "label": "supports",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
    "table_column_names": [
      "Cue",
      "[ITALIC] SCOPA",
      "[ITALIC] SB_COPA",
      "Diff.",
      "Prod."
    ],
    "table_content_values": [
      [
        "woman",
        "7.98",
        "4.84",
        "-3.14",
        "0.25"
      ],
      [
        "mother",
        "5.16",
        "3.95",
        "-1.21",
        "0.75"
      ],
      [
        "went",
        "6.00",
        "5.15",
        "-0.85",
        "0.73"
      ],
      [
        "down",
        "5.52",
        "4.93",
        "-0.58",
        "0.71"
      ],
      [
        "into",
        "4.07",
        "3.51",
        "-0.56",
        "0.40"
      ]
    ],
    "id": "1818df5d-04f4-44b0-8e69-fd87724f010c",
    "claim": "We observe that BERT trained on Balanced COPA is less sensitive to a few highly productive superficial cues than BERT trained on original COPA.",
    "label": "supports",
    "table_id": "6609f902-7f48-422d-b5e9-6596405c71dd"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "d588b47d-e4c7-49e6-8f76-0c5678b232ea",
    "claim": "Moreover, training on B-COPA improves performance on the Hard subset, both when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%).",
    "label": "supports",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VIII: Precision scores for the Semantic Analogy Test",
    "table_column_names": [
      "Questions Subset",
      "# of Questions Seen",
      "GloVe",
      "Word2Vec",
      "Proposed"
    ],
    "table_content_values": [
      [
        "All",
        "8783",
        "78.94",
        "81.03",
        "79.96"
      ],
      [
        "At least one",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "concept word",
        "1635",
        "67.58",
        "70.89",
        "67.89"
      ],
      [
        "All concept words",
        "110",
        "77.27",
        "89.09",
        "83.64"
      ]
    ],
    "id": "a2014e4b-6a45-4619-8130-4052f27e2307",
    "claim": "However, the greatest performance increase is not seen for the last scenario, which suggests that the semantic features captured by embeddings cannot be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived.",
    "label": "refutes",
    "table_id": "3c7cb0a4-b617-452e-8ad2-47464d1402be"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "LR-All Features – Original Data",
        "80.5",
        "78.0",
        "0.873"
      ],
      [
        "Dist. Supervision + Pooling",
        "77.2",
        "75.7",
        "0.853"
      ],
      [
        "Dist. Supervision + EasyAdapt",
        "[BOLD] 81.2",
        "[BOLD] 79.0",
        "[BOLD] 0.885"
      ]
    ],
    "id": "8b27ad92-0b0c-4813-94fc-d817b8f9837d",
    "claim": "[CONTINUE] However, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1.",
    "label": "supports",
    "table_id": "6dbcb51c-9427-4e68-be86-351890ad3d0a"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] # pairs",
      "[BOLD] # words (doc)",
      "[BOLD] # sents (docs)",
      "[BOLD] # words (summary)",
      "[BOLD] # sents (summary)",
      "[BOLD] vocab size"
    ],
    "table_content_values": [
      [
        "Multi-News",
        "44,972/5,622/5,622",
        "2,103.49",
        "82.73",
        "263.66",
        "9.97",
        "666,515"
      ],
      [
        "DUC03+04",
        "320",
        "4,636.24",
        "173.15",
        "109.58",
        "2.88",
        "19,734"
      ],
      [
        "TAC 2011",
        "176",
        "4,695.70",
        "188.43",
        "99.70",
        "1.00",
        "24,672"
      ],
      [
        "CNNDM",
        "287,227/13,368/11,490",
        "810.57",
        "39.78",
        "56.20",
        "3.68",
        "717,951"
      ]
    ],
    "id": "6580de06-3999-4498-95f2-ca6c5250638a",
    "claim": "The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected.",
    "label": "supports",
    "table_id": "9fda9198-5e75-4f79-b42c-82b7dc3ee31f"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "74ffa35f-85bf-4c65-9d6a-082216a84d24",
    "claim": "When increasing the number of terms to 10,000, the DocSub models using TED Talks corpora performed better than when using Europarl corpora.",
    "label": "refutes",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "Aligning Vector-spaces with Noisy Supervised Lexicons",
    "paper_id": "1903.10238v1",
    "table_caption": "Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
    "table_column_names": [
      "Method",
      "En→It best",
      "En→It avg",
      "En→It iters",
      "En→De best",
      "En→De avg",
      "En→De iters",
      "En→Fi best",
      "En→Fi avg",
      "En→Fi iters",
      "En→Es best",
      "En→Es avg",
      "En→Es iters"
    ],
    "table_content_values": [
      [
        "Artetxe et al., 2018b",
        "[BOLD] 48.53",
        "48.13",
        "573",
        "48.47",
        "48.19",
        "773",
        "33.50",
        "32.63",
        "988",
        "37.60",
        "37.33",
        "808"
      ],
      [
        "Noise-aware Alignment",
        "[BOLD] 48.53",
        "[BOLD] 48.20",
        "471",
        "[BOLD] 49.67",
        "[BOLD] 48.89",
        "568",
        "[BOLD] 33.98",
        "[BOLD] 33.68",
        "502",
        "[BOLD] 38.40",
        "[BOLD] 37.79",
        "551"
      ]
    ],
    "id": "76767541-0820-44a2-9a66-24bea826ecca",
    "claim": "Our model improves the results in the translation tasks.",
    "label": "supports",
    "table_id": "279e3d12-df99-48f8-83ea-d3e42b8bbfcc"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "2d765333-cfa9-4b2f-89be-59b5d90a8066",
    "claim": "we observe that our system’s summaries are preferred by humans more than the competitor systems’ in terms of readability and the coherence between passages, indicating the superiority of our system",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "02552450-1723-479d-b04b-eace6056bf93",
    "claim": "analogy can be well dealt with and we obtain a precisio score higher than all methods except for Word2Vec",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "41907607-9691-409a-9a0a-517d74061500",
    "claim": "The full model does not give the best performance on the AMR15 dev set.",
    "label": "refutes",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "050de577-add7-44d3-9d2c-7892c25a8464",
    "claim": "[CONTINUE] Although the PRKGC model do not receive supervision about human-generated NLDs, paths with the maximum score match human-generated NLDs to some extent.",
    "label": "supports",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.",
    "table_column_names": [
      "AMR Anno.",
      "BLEU"
    ],
    "table_content_values": [
      [
        "Automatic",
        "16.8"
      ],
      [
        "Gold",
        "[BOLD] *17.5*"
      ]
    ],
    "id": "e37522d9-013e-410c-bf5b-57c5a6ad3d60",
    "claim": "The improvement from automatic AMR to gold AMR (+0.7 BLEU) is not significant, which shows that the translation quality of our model cannot be further improved with an increase of AMR parsing accuracy.",
    "label": "refutes",
    "table_id": "d3d1985b-cf33-4981-9332-d03be820d004"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "de8ef478-5e2b-4df6-90f3-1e34c08c069a",
    "claim": "In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections.",
    "label": "supports",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "Language Independent Sequence Labelling for Opinion Target Extraction",
    "paper_id": "1901.09755v1",
    "table_caption": "Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.",
    "table_column_names": [
      "Language",
      "System",
      "F1"
    ],
    "table_content_values": [
      [
        "es",
        "GTI",
        "68.51"
      ],
      [
        "es",
        "L +  [BOLD] CW600 + W2VW300",
        "[BOLD] 69.92"
      ],
      [
        "es",
        "Baseline",
        "51.91"
      ],
      [
        "fr",
        "IIT-T",
        "66.67"
      ],
      [
        "fr",
        "L +  [BOLD] CW100",
        "[BOLD] 69.50"
      ],
      [
        "fr",
        "Baseline",
        "45.45"
      ],
      [
        "nl",
        "IIT-T",
        "56.99"
      ],
      [
        "nl",
        "L +  [BOLD] W2VW400",
        "[BOLD] 66.39"
      ],
      [
        "nl",
        "Baseline",
        "50.64"
      ],
      [
        "ru",
        "Danii.",
        "33.47"
      ],
      [
        "ru",
        "L +  [BOLD] CW500",
        "[BOLD] 65.53"
      ],
      [
        "ru",
        "Baseline",
        "49.31"
      ],
      [
        "tr",
        "L +  [BOLD] BW",
        "[BOLD] 60.22"
      ],
      [
        "tr",
        "Baseline",
        "41.86"
      ]
    ],
    "id": "67898573-d093-4de1-90ce-1a972b3794a9",
    "claim": "Table 6 shows that our system does not outperform the best previous approaches across the five languages.",
    "label": "refutes",
    "table_id": "ee6e6baf-f2d4-4eed-88b7-bd2c39612e5e"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "991cd154-0a60-4089-995d-b01bf6f79cd8",
    "claim": "The performance gap between our HDSA and DAMD grows as the number of actions increases due to the repetitive action problem in DAMD, which is also supported by the performance of our HDSA with a sampled threshold",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "f1e358e6-d02a-42cc-ab2d-acab9c445b37",
    "claim": "the proposed RL approach allows to create rewards without a summary-level information, thus achieving summary evaluation metrics with greater consistency.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "2d186228-c158-4640-a457-4d3b8f8ee6b0",
    "claim": "in summary, GDPL can learn useful dialogue skills from internal and external data, and the learned dialogue policy outperforms baseline methods in all three criteria.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "6b4c6197-f12f-490f-a908-1dd581078e9f",
    "claim": "our BERT-Cosine metric is the most effective at ranking “good” summaries",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer",
    "paper_id": "1810.11878v2",
    "table_caption": "Table 4: Manual evaluation results (%) using models from Table 2 (i.e., with roughly fixed Acc). > means “better than”. ΔSim=Sim(A)−Sim(B), and ΔPP=PP(A)−PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.",
    "table_column_names": [
      "Dataset",
      "Models A",
      "Models B",
      "Transfer quality A>B",
      "Transfer quality B>A",
      "Transfer quality Tie",
      "Semantic preservation A>B",
      "Semantic preservation B>A",
      "Semantic preservation Tie",
      "Semantic preservation ΔSim",
      "Fluency A>B",
      "Fluency B>A",
      "Fluency Tie",
      "Fluency ΔPP"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "M0",
        "M2",
        "9.0",
        "6.0",
        "85.1",
        "1.5",
        "[BOLD] 25.4",
        "73.1",
        "-0.05",
        "10.4",
        "[BOLD] 23.9",
        "65.7",
        "0.9"
      ],
      [
        "Yelp",
        "M0",
        "M7",
        "9.6",
        "14.7",
        "75.8",
        "2.5",
        "[BOLD] 54.5",
        "42.9",
        "-0.09",
        "4.6",
        "[BOLD] 39.4",
        "56.1",
        "8.3"
      ],
      [
        "Yelp",
        "M6",
        "M7",
        "13.7",
        "11.6",
        "74.7",
        "16.0",
        "16.7",
        "67.4",
        "0.01",
        "10.3",
        "20.0",
        "69.7",
        "14.3"
      ],
      [
        "[EMPTY]",
        "M2",
        "M7",
        "5.8",
        "9.3",
        "84.9",
        "8.1",
        "[BOLD] 25.6",
        "66.3",
        "-0.04",
        "14.0",
        "[BOLD] 26.7",
        "59.3",
        "7.4"
      ],
      [
        "Literature",
        "M2",
        "M6",
        "4.2",
        "6.7",
        "89.2",
        "16.7",
        "20.8",
        "62.5",
        "0.01",
        "[BOLD] 40.8",
        "13.3",
        "45.8",
        "-13.3"
      ],
      [
        "Literature",
        "M6",
        "M7",
        "15.8",
        "13.3",
        "70.8",
        "[BOLD] 25.0",
        "9.2",
        "65.8",
        "0.03",
        "14.2",
        "20.8",
        "65.0",
        "14.2"
      ]
    ],
    "id": "0945c766-61d1-41b1-96ea-8577746e2651",
    "claim": "For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments and very similar Sim scores.",
    "label": "supports",
    "table_id": "d3f0a504-9761-4763-bbb8-2d7bfe87ec88"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.",
    "table_column_names": [
      "Corpus",
      "Metric",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "Europarl",
        "TotalTerms:",
        "980",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "996",
        "1,000"
      ],
      [
        "Europarl",
        "TotalRoots:",
        "79",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "NumberRels:",
        "1,527",
        "1,031",
        "1,049",
        "1,185",
        "1,093",
        "1,644",
        "999"
      ],
      [
        "Europarl",
        "MaxDepth:",
        "19",
        "902",
        "894",
        "784",
        "849",
        "6",
        "10"
      ],
      [
        "Europarl",
        "MinDepth:",
        "1",
        "902",
        "894",
        "784",
        "849",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgDepth:",
        "9.43",
        "902",
        "894",
        "784",
        "849",
        "2.73",
        "4.29"
      ],
      [
        "Europarl",
        "DepthCohesion:",
        "2.02",
        "1",
        "1",
        "1",
        "1",
        "2.19",
        "2.33"
      ],
      [
        "Europarl",
        "MaxWidth:",
        "27",
        "3",
        "3",
        "4",
        "3",
        "201",
        "58"
      ],
      [
        "Europarl",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgWidth:",
        "1.98",
        "1.03",
        "1.05",
        "1.19",
        "1.09",
        "6.25",
        "2.55"
      ],
      [
        "TED Talks",
        "TotalTerms:",
        "296",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000"
      ],
      [
        "TED Talks",
        "TotalRoots:",
        "101",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "NumberRels:",
        "291",
        "1,045",
        "1,229",
        "3,637",
        "4,284",
        "2,875",
        "999"
      ],
      [
        "TED Talks",
        "MaxDepth:",
        "10",
        "860",
        "727",
        "388",
        "354",
        "252",
        "17"
      ],
      [
        "TED Talks",
        "MinDepth:",
        "1",
        "860",
        "727",
        "388",
        "354",
        "249",
        "1"
      ],
      [
        "TED Talks",
        "AvgDepth:",
        "3.94",
        "860",
        "727",
        "388",
        "354",
        "250.43",
        "6.16"
      ],
      [
        "TED Talks",
        "DepthCohesion:",
        "2.54",
        "1",
        "1",
        "1",
        "1",
        "1.01",
        "2.76"
      ],
      [
        "TED Talks",
        "MaxWidth:",
        "37",
        "3",
        "79",
        "18",
        "13",
        "9",
        "41"
      ],
      [
        "TED Talks",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "AvgWidth:",
        "1.79",
        "1.05",
        "1.23",
        "3.64",
        "4.29",
        "2.94",
        "2.37"
      ]
    ],
    "id": "e500eb41-5d94-4380-b1a6-2df603c661c0",
    "claim": "The results for the Portuguese corpora are quite different from the ones generated by the English corpora, with terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating shallow taxonomies, disproving the characteristics of each method.",
    "label": "refutes",
    "table_id": "886ce51d-34a7-471b-bb4d-9e97051486de"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "9a9e8bed-916f-41b2-b6c7-a9163e27d1ac",
    "claim": "We observe that the redundancy removal step is not necessary for the HAN models to achieve outstanding results.",
    "label": "refutes",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "1c6f6376-967e-4d24-a135-f6d6d35ab53a",
    "claim": "systems with hand-crafted (R-1/2/L) or automatic (R-1,2,L) rewards always have higher ROUGE scores compared with our system, which improves their performance in limited scenarios.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "86dee411-f4cc-4ea5-b221-e4b7a4877884",
    "claim": "It should also be noted that scores obtained by SPINE are relatively low on some tests, but still acceptable, indicating that it has achieved its interpretability performance without sacrificing its semantic functions.",
    "label": "refutes",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Adversarial Removal of Demographic Attributes from Text Data",
    "paper_id": "1808.06640v2",
    "table_caption": "Table 6: Accuracies of the protected attribute with different encoders.",
    "table_column_names": [
      "[EMPTY]",
      "[EMPTY]",
      "Embedding Leaky",
      "Embedding Guarded"
    ],
    "table_content_values": [
      [
        "RNN",
        "Leaky",
        "64.5",
        "67.8"
      ],
      [
        "RNN",
        "Guarded",
        "59.3",
        "54.8"
      ]
    ],
    "id": "6fa0512b-21f4-4bd1-86eb-c649baf8805f",
    "claim": "[CONTINUE] Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix.",
    "label": "supports",
    "table_id": "257ac8bf-91fb-4400-bd5a-b88442e5f57f"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 8: Performance of models in Macro F1 on tweets from each domain.",
    "table_column_names": [
      "[BOLD] Domain",
      "[BOLD] In-Domain",
      "[BOLD] Pooling",
      "[BOLD] EasyAdapt"
    ],
    "table_content_values": [
      [
        "Food & Beverage",
        "63.9",
        "60.9",
        "[BOLD] 83.1"
      ],
      [
        "Apparel",
        "[BOLD] 76.2",
        "71.1",
        "72.5"
      ],
      [
        "Retail",
        "58.8",
        "[BOLD] 79.7",
        "[BOLD] 79.7"
      ],
      [
        "Cars",
        "41.5",
        "77.8",
        "[BOLD] 80.9"
      ],
      [
        "Services",
        "65.2",
        "75.9",
        "[BOLD] 76.7"
      ],
      [
        "Software",
        "61.3",
        "73.4",
        "[BOLD] 78.7"
      ],
      [
        "Transport",
        "56.4",
        "[BOLD] 73.4",
        "69.8"
      ],
      [
        "Electronics",
        "66.2",
        "73.0",
        "[BOLD] 76.2"
      ],
      [
        "Other",
        "42.4",
        "[BOLD] 82.8",
        "[BOLD] 82.8"
      ]
    ],
    "id": "a1404ba9-ad11-4c99-803f-71fc990d1c07",
    "claim": "Overall, predictive performance is high across all domains, with the exception of transport.",
    "label": "supports",
    "table_id": "c89b018d-1487-4970-8e22-437de259e040"
  },
  {
    "paper": "Evaluation of Greek Word Embeddings",
    "paper_id": "1904.04032v3",
    "table_caption": "Table 4: Word similarity.",
    "table_column_names": [
      "Model",
      "Pearson",
      "p-value",
      "Pairs (unknown)"
    ],
    "table_content_values": [
      [
        "gr_def",
        "[BOLD] 0.6042",
        "3.1E-35",
        "2.3%"
      ],
      [
        "gr_neg10",
        "0.5973",
        "2.9E-34",
        "2.3%"
      ],
      [
        "cc.el.300",
        "0.5311",
        "1.7E-25",
        "4.9%"
      ],
      [
        "wiki.el",
        "0.5812",
        "2.2E-31",
        "4.5%"
      ],
      [
        "gr_cbow_def",
        "0.5232",
        "2.7E-25",
        "2.3%"
      ],
      [
        "gr_d300_nosub",
        "0.5889",
        "3.8E-33",
        "2.3%"
      ],
      [
        "gr_w2v_sg_n5",
        "0.5879",
        "4.4E-33",
        "2.3%"
      ]
    ],
    "id": "905c2475-f7a6-4b0f-aeca-1ba199c9fd50",
    "claim": "According to Pearson correlation, gr cbow def model had the highest correlation with human ratings of similarity.",
    "label": "refutes",
    "table_id": "4b23f4f7-87c1-4499-8938-2a758af522d8"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "5a1d0c5b-836f-4eef-85b7-ff4f0e532907",
    "claim": "Also, the average human rating for Refresh is significantly higher (p (cid:28) 0.01) than ExtAbsRL,",
    "label": "supports",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
    "table_column_names": [
      "Schema",
      "AntePre(Test)",
      "AntePre(Train)"
    ],
    "table_content_values": [
      [
        "Type 1",
        "76.67",
        "86.79"
      ],
      [
        "Type 2",
        "79.55",
        "88.86"
      ],
      [
        "Type 1 (Cat1)",
        "90.26",
        "93.64"
      ],
      [
        "Type 2 (Cat2)",
        "83.38",
        "92.49"
      ]
    ],
    "id": "25fb709f-c1cf-4153-b8f6-672f1f494623",
    "claim": "These results do not use the best performing KnowComb system.",
    "label": "refutes",
    "table_id": "18072983-1b23-4da0-8e08-1b6a92fbc124"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "768f70ec-749a-408f-a097-279e7b07e70f",
    "claim": "We observe that PCNN+ATT (1) exhibits the best performances.",
    "label": "refutes",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "paper_id": "1909.03546v2",
    "table_caption": "Table 3: F1 scores on Relation.",
    "table_column_names": [
      "[EMPTY]",
      "ACE05",
      "SciERC",
      "WLPC"
    ],
    "table_content_values": [
      [
        "BERT + LSTM",
        "60.6",
        "40.3",
        "65.1"
      ],
      [
        "+RelProp",
        "61.9",
        "41.1",
        "65.3"
      ],
      [
        "+CorefProp",
        "59.7",
        "42.6",
        "-"
      ],
      [
        "BERT FineTune",
        "[BOLD] 62.1",
        "44.3",
        "65.4"
      ],
      [
        "+RelProp",
        "62.0",
        "43.0",
        "[BOLD] 65.5"
      ],
      [
        "+CorefProp",
        "60.0",
        "[BOLD] 45.3",
        "-"
      ]
    ],
    "id": "7df38698-8364-4d29-b26e-8e8631b92458",
    "claim": "[CONTINUE] Relation propagation (RelProp) improves relation extraction performance over pretrained BERT, but does not improve fine-tuned BERT.",
    "label": "supports",
    "table_id": "3fc48613-1f39-4968-a1bc-84c2cae46001"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "4cc9d2c1-a18e-47de-aa39-4909ec936ba9",
    "claim": "[CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly bad compared to TF-IDF",
    "label": "supports",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "c0ce7d66-9d67-4ba9-831c-145cdbef0f49",
    "claim": "[CONTINUE] The relative lower BLEU score [CONTINUE] Our DAMD model significantly outperforms other models with different system action forms in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), [CONTINUE] Moreover, if a model has access to ground truth system action, the model further improves its task performance.",
    "label": "supports",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
    "table_column_names": [
      "Model",
      "Val. Accuracy",
      "Loss",
      "Val. Loss",
      "Pretraining Time",
      "Finetuning Time"
    ],
    "table_content_values": [
      [
        "Siamese Networks",
        "77.42%",
        "0.5601",
        "0.5329",
        "[EMPTY]",
        "4m per epoch"
      ],
      [
        "BERT",
        "87.47%",
        "0.4655",
        "0.4419",
        "66 hours",
        "2m per epoch"
      ],
      [
        "GPT-2",
        "90.99%",
        "0.2172",
        "0.1826",
        "78 hours",
        "4m per epoch"
      ],
      [
        "ULMFiT",
        "91.59%",
        "0.3750",
        "0.1972",
        "11 hours",
        "2m per epoch"
      ],
      [
        "ULMFiT (no LM Finetuning)",
        "78.11%",
        "0.5512",
        "0.5409",
        "11 hours",
        "2m per epoch"
      ],
      [
        "BERT + Multitasking",
        "91.20%",
        "0.3155",
        "0.3023",
        "66 hours",
        "4m per epoch"
      ],
      [
        "GPT-2 + Multitasking",
        "96.28%",
        "0.2609",
        "0.2197",
        "78 hours",
        "5m per epoch"
      ]
    ],
    "id": "78a8bdad-2657-45d7-88ef-60e249c81ea8",
    "claim": "GPT-2, on the other hand, finetuned to a final accuracy of 91.20%, only a 0.61% improvement over the performance of ULMFiT.",
    "label": "refutes",
    "table_id": "8ef20dc7-9293-47b5-91f4-23b850e09731"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "9de1dc72-cd2d-4713-8dc5-57373c5607e1",
    "claim": "imparting named entities and events certainly yields considerable improvement in a word intrusion test.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "0d745e8a-02cf-46dc-b3c2-a9dad265bded",
    "claim": "we can see that our proposed technique outperforms all other approaches including the attention model for sentiment classification task",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section 3).",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] Part",
      "[BOLD] MRs",
      "[BOLD] Refs",
      "[BOLD] SER(%)"
    ],
    "table_content_values": [
      [
        "Original",
        "Train",
        "4,862",
        "42,061",
        "17.69"
      ],
      [
        "Original",
        "Dev",
        "547",
        "4,672",
        "11.42"
      ],
      [
        "Original",
        "Test",
        "630",
        "4,693",
        "11.49"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Train",
        "8,362",
        "33,525",
        "(0.00)"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Dev",
        "1,132",
        "4,299",
        "(0.00)"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Test",
        "1,358",
        "4,693",
        "(0.00)"
      ]
    ],
    "id": "05d0d281-1e18-4f77-932d-b89d635f6ca2",
    "claim": "This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs.",
    "label": "supports",
    "table_id": "82c0eea1-ccb0-4495-a12a-2ecf9bed5f45"
  },
  {
    "paper": "Predicting Discourse Structure using Distant Supervision from Sentiment",
    "paper_id": "1910.14176v1",
    "table_caption": "Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined",
    "table_column_names": [
      "Approach",
      "RST-DTtest",
      "Instr-DTtest"
    ],
    "table_content_values": [
      [
        "Right Branching",
        "54.64",
        "58.47"
      ],
      [
        "Left Branching",
        "53.73",
        "48.15"
      ],
      [
        "Hier. Right Branch.",
        "[BOLD] 70.82",
        "[BOLD] 67.86"
      ],
      [
        "Hier. Left Branch.",
        "70.58",
        "63.49"
      ],
      [
        "[BOLD] Intra-Domain Evaluation",
        "[BOLD] Intra-Domain Evaluation",
        "[BOLD] Intra-Domain Evaluation"
      ],
      [
        "HILDAHernault et al. ( 2010 )",
        "83.00",
        "—"
      ],
      [
        "DPLPJi and Eisenstein ( 2014 )",
        "82.08",
        "—"
      ],
      [
        "CODRAJoty et al. ( 2015 )",
        "83.84",
        "[BOLD] 82.88"
      ],
      [
        "Two-StageWang et al. ( 2017 )",
        "[BOLD] 86.00",
        "77.28"
      ],
      [
        "[BOLD] Inter-Domain Evaluation",
        "[BOLD] Inter-Domain Evaluation",
        "[BOLD] Inter-Domain Evaluation"
      ],
      [
        "Two-StageRST-DT",
        "×",
        "73.65"
      ],
      [
        "Two-StageInstr-DT",
        "74.48",
        "×"
      ],
      [
        "Two-StageOurs(avg)",
        "76.42",
        "[BOLD] 74.22"
      ],
      [
        "Two-StageOurs(max)",
        "[BOLD] 77.24",
        "73.12"
      ],
      [
        "Human Morey et al. ( 2017 )",
        "88.30",
        "—"
      ]
    ],
    "id": "4a080275-4fe7-4c3b-b13a-c21e16ec6d34",
    "claim": "The first set of results in Table 3 shows that the hierarchical right/left branching baselines dominate the completely right/left branching ones.",
    "label": "supports",
    "table_id": "00d328ef-0661-439e-bcf1-d6ba0a8e1c50"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.",
    "table_column_names": [
      "Model",
      "Accuracy"
    ],
    "table_content_values": [
      [
        "BigramPMI Goodwin et al. ( 2012 )",
        "63.4"
      ],
      [
        "PMI Gordon et al. ( 2011 )",
        "65.4"
      ],
      [
        "PMI+Connectives Luo et al. ( 2016 )",
        "70.2"
      ],
      [
        "PMI+Con.+Phrase Sasaki et al. ( 2017 )",
        "71.4"
      ],
      [
        "BERT-large Wang et al. ( 2019 )",
        "70.5"
      ],
      [
        "BERT-large Sap et al. ( 2019 )",
        "75.0"
      ],
      [
        "BERT-large Li et al. ( 2019 )",
        "75.4"
      ],
      [
        "RoBERTa-large (finetuned)",
        "90.6"
      ],
      [
        "BERT-large (finetuned)*",
        "76.5 ± 2.7"
      ],
      [
        "RoBERTa-large (finetuned)*",
        "87.7 ± 0.9"
      ]
    ],
    "id": "3512c69e-13aa-47de-8bd5-07e449d021a2",
    "claim": "this result shows the effectiveness of the causality-centric training objective and evaluation metrics in the COPA task; RoBERTa-large (finetuned) achieves substantial improvements (90.6%) over the previous state-of-the-art methods.",
    "label": "not enough info",
    "table_id": "7fee409c-d3c7-42e0-bf75-fce5f2f3559b"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Parameters",
      "[BOLD] Validation AUC@0.05",
      "[BOLD] Test AUC@0.05"
    ],
    "table_content_values": [
      [
        "Base",
        "8.0M",
        "[BOLD] 0.871",
        "0.816"
      ],
      [
        "4L SRU → 2L LSTM",
        "7.3M",
        "0.864",
        "[BOLD] 0.829"
      ],
      [
        "4L SRU → 2L SRU",
        "7.8M",
        "0.856",
        "[BOLD] 0.829"
      ],
      [
        "Flat → hierarchical",
        "12.4M",
        "0.825",
        "0.559"
      ],
      [
        "Cross entropy → hinge loss",
        "8.0M",
        "0.765",
        "0.693"
      ],
      [
        "6.6M → 1M examples",
        "8.0M",
        "0.835",
        "0.694"
      ],
      [
        "6.6M → 100K examples",
        "8.0M",
        "0.565",
        "0.417"
      ],
      [
        "200 → 100 negatives",
        "8.0M",
        "0.864",
        "0.647"
      ],
      [
        "200 → 10 negatives",
        "8.0M",
        "0.720",
        "0.412"
      ]
    ],
    "id": "c1b4b874-ea14-4243-a9d8-1f7ce0e5c941",
    "claim": "The model performs significantly better when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function.",
    "label": "refutes",
    "table_id": "5efab962-70b6-488d-9601-cbd5b4852a40"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
    "table_column_names": [
      "[BOLD] Emoji alias",
      "[BOLD] N",
      "[BOLD] emoji #",
      "[BOLD] emoji %",
      "[BOLD] no-emoji #",
      "[BOLD] no-emoji %",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "mask",
        "163",
        "154",
        "94.48",
        "134",
        "82.21",
        "- 12.27"
      ],
      [
        "two_hearts",
        "87",
        "81",
        "93.10",
        "77",
        "88.51",
        "- 4.59"
      ],
      [
        "heart_eyes",
        "122",
        "109",
        "89.34",
        "103",
        "84.43",
        "- 4.91"
      ],
      [
        "heart",
        "267",
        "237",
        "88.76",
        "235",
        "88.01",
        "- 0.75"
      ],
      [
        "rage",
        "92",
        "78",
        "84.78",
        "66",
        "71.74",
        "- 13.04"
      ],
      [
        "cry",
        "116",
        "97",
        "83.62",
        "83",
        "71.55",
        "- 12.07"
      ],
      [
        "sob",
        "490",
        "363",
        "74.08",
        "345",
        "70.41",
        "- 3.67"
      ],
      [
        "unamused",
        "167",
        "121",
        "72.46",
        "116",
        "69.46",
        "- 3.00"
      ],
      [
        "weary",
        "204",
        "140",
        "68.63",
        "139",
        "68.14",
        "- 0.49"
      ],
      [
        "joy",
        "978",
        "649",
        "66.36",
        "629",
        "64.31",
        "- 2.05"
      ],
      [
        "sweat_smile",
        "111",
        "73",
        "65.77",
        "75",
        "67.57",
        "1.80"
      ],
      [
        "confused",
        "77",
        "46",
        "59.74",
        "48",
        "62.34",
        "2.60"
      ]
    ],
    "id": "2c6bda23-8b25-4510-a555-9234e71214ac",
    "claim": "[CONTINUE] Finally, not all emoji are beneficial for this task.",
    "label": "supports",
    "table_id": "2410b1c3-4d48-49e9-9279-dc23daa879d1"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "4ee4134b-7e4a-41fb-9eb4-ef3dd0fef13f",
    "claim": "The results in Table 7 show that the method is comparable to state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction.",
    "label": "supports",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "55e824e9-c2e1-4cb6-8431-63df8a604668",
    "claim": "DAMD shows the effectiveness of capturing large-scale action patterns",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "27cbf083-1fc5-4938-9b87-bcce547cdec5",
    "claim": "As can be seen in the results presented in Table 3, the models using softmax and sparsemax in the output attention layer outperform the models using TVMAX.",
    "label": "refutes",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>External</bold>",
      "<bold>BLEU</bold>"
    ],
    "table_content_values": [
      [
        "Konstas et al. (2017)",
        "200K",
        "27.40"
      ],
      [
        "Song et al. (2018)",
        "200K",
        "28.20"
      ],
      [
        "Guo et al. (2019)",
        "200K",
        "31.60"
      ],
      [
        "G2S-GGNN",
        "200K",
        "<bold>32.23</bold>"
      ]
    ],
    "id": "c0d4f7e7-545a-4108-a282-9c2355965cb2",
    "claim": "G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3.",
    "label": "supports",
    "table_id": "103c9a5f-ec19-41cf-bc63-cbd81b53f4b1"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 2: Experiment 1",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.001",
        "0.003",
        "-20.818",
        "***",
        "0.505"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.083",
        "0.048",
        "101.636",
        "***",
        "1.724"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.001",
        "0.001",
        "0.035",
        "[EMPTY]",
        "1.001"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.023",
        "0.012",
        "64.418",
        "***",
        "1.993"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.002",
        "0.001",
        "4.047",
        "***",
        "1.120"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.049",
        "0.019",
        "120.986",
        "***",
        "2.573"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.173",
        "0.065",
        "243.285",
        "***",
        "2.653"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.032",
        "0.023",
        "39.483",
        "***",
        "1.396"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.111",
        "0.061",
        "122.707",
        "***",
        "1.812"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.178",
        "0.080",
        "211.319",
        "***",
        "2.239"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.028",
        "0.015",
        "63.131",
        "***",
        "1.854"
      ]
    ],
    "id": "1a8e67b2-88be-49d5-bb39-f4d96cdb3495",
    "claim": "For Waseem (2016) we see that there is no significant difference in the estimated rates at which tweets are clas [CONTINUE] sified as racist across groups, although the rates remain low.",
    "label": "supports",
    "table_id": "661ddd6e-51d9-446c-add1-3e80f3b78109"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "c256c279-dab2-4c45-b0e9-b49660868f5f",
    "claim": "Increasing the window size to 10 reduces the F1 score marginally (A3−A4).",
    "label": "supports",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
    "table_column_names": [
      "[BOLD] Language pair",
      "[BOLD] Model type",
      "[BOLD] Oracle model",
      "[BOLD] Decoder configuration  [BOLD] Uniform",
      "[BOLD] Decoder configuration  [BOLD] BI + IS"
    ],
    "table_content_values": [
      [
        "es-en",
        "Unadapted",
        "36.4",
        "34.7",
        "36.6"
      ],
      [
        "es-en",
        "No-reg",
        "36.6",
        "34.8",
        "-"
      ],
      [
        "es-en",
        "EWC",
        "37.0",
        "36.3",
        "[BOLD] 37.2"
      ],
      [
        "en-de",
        "Unadapted",
        "36.4",
        "26.8",
        "38.8"
      ],
      [
        "en-de",
        "No-reg",
        "41.7",
        "31.8",
        "-"
      ],
      [
        "en-de",
        "EWC",
        "42.1",
        "38.6",
        "[BOLD] 42.0"
      ]
    ],
    "id": "df4ef506-a5a6-4e77-8a07-2b8a0f630696",
    "claim": "[CONTINUE] BI+IS decoding with single-domain trained models achieves gains over both the naive uniform approach and over oracle single-domain models.",
    "label": "supports",
    "table_id": "a02493f5-adad-4462-a582-8a2cfe6f431d"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "440cc321-d9fd-4f0c-8f3f-21bcaee949e5",
    "claim": "These results indicate that dense connections do not play a significant role in our model.",
    "label": "refutes",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "0dacc112-f2b4-4fcb-9e05-98368e576c3e",
    "claim": "Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are not able to learn the task to a high degree.",
    "label": "refutes",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "7f929dc9-7327-40e8-9352-1767a83b1a2f",
    "claim": "[CONTINUE] however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU).",
    "label": "supports",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
    "table_column_names": [
      "[EMPTY]",
      "WN-N P",
      "WN-N R",
      "WN-N F",
      "WN-V P",
      "WN-V R",
      "WN-V F",
      "VN P",
      "VN R",
      "VN F"
    ],
    "table_content_values": [
      [
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2"
      ],
      [
        "type",
        ".700",
        ".654",
        ".676",
        ".535",
        ".474",
        ".503",
        ".327",
        ".309",
        ".318"
      ],
      [
        "x+POS",
        ".699",
        ".651",
        ".674",
        ".544",
        ".472",
        ".505",
        ".339",
        ".312",
        ".325"
      ],
      [
        "lemma",
        ".706",
        ".660",
        ".682",
        ".576",
        ".520",
        ".547",
        ".384",
        ".360",
        ".371"
      ],
      [
        "x+POS",
        "<bold>.710</bold>",
        "<bold>.662</bold>",
        "<bold>.685</bold>",
        "<bold>.589</bold>",
        "<bold>.529</bold>",
        "<bold>.557</bold>",
        "<bold>.410</bold>",
        "<bold>.389</bold>",
        "<bold>.399</bold>"
      ],
      [
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep"
      ],
      [
        "type",
        ".712",
        ".661",
        ".686",
        ".545",
        ".457",
        ".497",
        ".324",
        ".296",
        ".310"
      ],
      [
        "x+POS",
        ".715",
        ".659",
        ".686",
        ".560",
        ".464",
        ".508",
        ".349",
        ".320",
        ".334"
      ],
      [
        "lemma",
        "<bold>.725</bold>",
        "<bold>.668</bold>",
        "<bold>.696</bold>",
        ".591",
        ".512",
        ".548",
        ".408",
        ".371",
        ".388"
      ],
      [
        "x+POS",
        ".722",
        ".666",
        ".693",
        "<bold>.609</bold>",
        "<bold>.527</bold>",
        "<bold>.565</bold>",
        "<bold>.412</bold>",
        "<bold>.381</bold>",
        "<bold>.396</bold>"
      ]
    ],
    "id": "59fe28ba-af08-4f7c-aa03-c61c11dbe6f2",
    "claim": "Lemma-based targets with POS disambiguation perform best on WN-N when dependency-based contexts are used; the difference to lemmatized targets without disambiguation is statistically significant (p < .1).",
    "label": "refutes",
    "table_id": "051fe422-03d4-44d5-836c-7f982b328555"
  },
  {
    "paper": "Modulated Self-attention Convolutional Network for VQA",
    "paper_id": "1910.03343v2",
    "table_caption": "Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
    "table_column_names": [
      "[BOLD] ResNet-34",
      "[BOLD] Eval set %",
      "[BOLD] #param"
    ],
    "table_content_values": [
      [
        "Baseline (No SA)Anderson et al. ( 2018 )",
        "55.00",
        "0M"
      ],
      [
        "SA (S: 1,2,3 - B: 1)",
        "55.11",
        "} 0.107M"
      ],
      [
        "SA (S: 1,2,3 - B: 2)",
        "55.17",
        "} 0.107M"
      ],
      [
        "[BOLD] SA (S: 1,2,3 - B: 3)",
        "[BOLD] 55.27",
        "} 0.107M"
      ]
    ],
    "id": "b4829db1-041f-4a7e-9371-9042d2584441",
    "claim": "[CONTINUE] We empirically found that self-attention was the most efficient in the 3rd stage.",
    "label": "supports",
    "table_id": "05b02f41-4f87-4857-b570-81187fef92ee"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The “+” indicates that the true response is added to the whitelist.",
    "table_column_names": [
      "[BOLD] Whitelist",
      "[BOLD] R@1",
      "[BOLD] R@3",
      "[BOLD] R@5",
      "[BOLD] R@10",
      "[BOLD] BLEU"
    ],
    "table_content_values": [
      [
        "Random 10K+",
        "0.252",
        "0.400",
        "0.472",
        "0.560",
        "37.71"
      ],
      [
        "Frequency 10K+",
        "0.257",
        "0.389",
        "0.455",
        "0.544",
        "41.34"
      ],
      [
        "Clustering 10K+",
        "0.230",
        "0.376",
        "0.447",
        "0.541",
        "37.59"
      ],
      [
        "Random 1K+",
        "0.496",
        "0.663",
        "0.728",
        "0.805",
        "59.28"
      ],
      [
        "Frequency 1K+",
        "0.513",
        "0.666",
        "0.726",
        "0.794",
        "67.05"
      ],
      [
        "Clustering 1K+",
        "0.481",
        "0.667",
        "0.745",
        "0.835",
        "61.88"
      ],
      [
        "Frequency 10K",
        "0.136",
        "0.261",
        "0.327",
        "0.420",
        "30.46"
      ],
      [
        "Clustering 10K",
        "0.164",
        "0.292",
        "0.360",
        "0.457",
        "31.47"
      ],
      [
        "Frequency 1K",
        "0.273",
        "0.465",
        "0.550",
        "0.658",
        "47.13"
      ],
      [
        "Clustering 1K",
        "0.331",
        "0.542",
        "0.650",
        "0.782",
        "49.26"
      ]
    ],
    "id": "7be9b83d-f973-4655-9a2c-39eb8160b687",
    "claim": "The results in Table 5 show that the frequency whitelists perform better than the random and clustering whitelists when the true response is added.",
    "label": "refutes",
    "table_id": "8cf7b802-0e8b-4a89-98f5-9dcaddd2a600"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
    "table_column_names": [
      "Schema",
      "AntePre(Test)",
      "AntePre(Train)"
    ],
    "table_content_values": [
      [
        "Type 1",
        "76.67",
        "86.79"
      ],
      [
        "Type 2",
        "79.55",
        "88.86"
      ],
      [
        "Type 1 (Cat1)",
        "90.26",
        "93.64"
      ],
      [
        "Type 2 (Cat2)",
        "83.38",
        "92.49"
      ]
    ],
    "id": "68d6065c-868c-40ac-b3a9-14218014c2c1",
    "claim": "These results use the best performing KnowComb system.",
    "label": "supports",
    "table_id": "18072983-1b23-4da0-8e08-1b6a92fbc124"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
    "table_column_names": [
      "Model",
      "Val. Accuracy",
      "Loss",
      "Val. Loss",
      "Pretraining Time",
      "Finetuning Time"
    ],
    "table_content_values": [
      [
        "Siamese Networks",
        "77.42%",
        "0.5601",
        "0.5329",
        "[EMPTY]",
        "4m per epoch"
      ],
      [
        "BERT",
        "87.47%",
        "0.4655",
        "0.4419",
        "66 hours",
        "2m per epoch"
      ],
      [
        "GPT-2",
        "90.99%",
        "0.2172",
        "0.1826",
        "78 hours",
        "4m per epoch"
      ],
      [
        "ULMFiT",
        "91.59%",
        "0.3750",
        "0.1972",
        "11 hours",
        "2m per epoch"
      ],
      [
        "ULMFiT (no LM Finetuning)",
        "78.11%",
        "0.5512",
        "0.5409",
        "11 hours",
        "2m per epoch"
      ],
      [
        "BERT + Multitasking",
        "91.20%",
        "0.3155",
        "0.3023",
        "66 hours",
        "4m per epoch"
      ],
      [
        "GPT-2 + Multitasking",
        "96.28%",
        "0.2609",
        "0.2197",
        "78 hours",
        "5m per epoch"
      ]
    ],
    "id": "dee030ab-36f2-4c8f-a9f2-297aac021d8f",
    "claim": "BERT achieved a final accuracy of 91.20%, now marginally comparable to ULMFiT's full performance.",
    "label": "supports",
    "table_id": "8ef20dc7-9293-47b5-91f4-23b850e09731"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. † denotes the previous best state-of-the-art model.",
    "table_column_names": [
      "Model",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "CNN zeng2014relation",
        "0.413",
        "0.591",
        "0.486",
        "0.444",
        "0.625",
        "0.519"
      ],
      [
        "PCNN zeng2015distant",
        "0.380",
        "[BOLD] 0.642",
        "0.477",
        "0.446",
        "0.679",
        "0.538†"
      ],
      [
        "EA huang2016attention",
        "0.443",
        "0.638",
        "0.523†",
        "0.419",
        "0.677",
        "0.517"
      ],
      [
        "BGWA jat2018attention",
        "0.364",
        "0.632",
        "0.462",
        "0.417",
        "[BOLD] 0.692",
        "0.521"
      ],
      [
        "BiLSTM-CNN",
        "0.490",
        "0.507",
        "0.498",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "Our model",
        "[BOLD] 0.541",
        "0.595",
        "[BOLD] 0.566*",
        "[BOLD] 0.507",
        "0.652",
        "[BOLD] 0.571*"
      ]
    ],
    "id": "a50811fb-3024-4844-b141-56da2fa21184",
    "claim": "Our model does not outperform the previous state-of-the-art models on both datasets in terms of F1 score.",
    "label": "refutes",
    "table_id": "c6b2cd01-ccb9-4fdd-90ab-a8ed5ae0d132"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "5ba4624b-1fd1-4ae1-bb8d-1bf53456f96e",
    "claim": "This indicates that PMeans can better detect informative sentences, and PMeans-RNN can better find informative words in extracted sentence.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "a44d9bc1-bdf5-497b-8274-551f0c88fa9e",
    "claim": "Our results indicate that neither beam search nor diversity-enhancing decoding schemes can generate multiple actions well.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
    "table_column_names": [
      "Model",
      "Val. Accuracy",
      "Loss",
      "Val. Loss",
      "Pretraining Time",
      "Finetuning Time"
    ],
    "table_content_values": [
      [
        "Siamese Networks",
        "77.42%",
        "0.5601",
        "0.5329",
        "[EMPTY]",
        "4m per epoch"
      ],
      [
        "BERT",
        "87.47%",
        "0.4655",
        "0.4419",
        "66 hours",
        "2m per epoch"
      ],
      [
        "GPT-2",
        "90.99%",
        "0.2172",
        "0.1826",
        "78 hours",
        "4m per epoch"
      ],
      [
        "ULMFiT",
        "91.59%",
        "0.3750",
        "0.1972",
        "11 hours",
        "2m per epoch"
      ],
      [
        "ULMFiT (no LM Finetuning)",
        "78.11%",
        "0.5512",
        "0.5409",
        "11 hours",
        "2m per epoch"
      ],
      [
        "BERT + Multitasking",
        "91.20%",
        "0.3155",
        "0.3023",
        "66 hours",
        "4m per epoch"
      ],
      [
        "GPT-2 + Multitasking",
        "96.28%",
        "0.2609",
        "0.2197",
        "78 hours",
        "5m per epoch"
      ]
    ],
    "id": "fad1f689-0127-4c3a-bf86-258d0de6f364",
    "claim": "GPT-2, on the other hand, finetuned to a final accuracy of 96.28%, a full 4.69% improvement over the performance of ULMFiT.",
    "label": "supports",
    "table_id": "8ef20dc7-9293-47b5-91f4-23b850e09731"
  },
  {
    "paper": "Neural End-to-End Learning for Computational Argumentation Mining",
    "paper_id": "1704.06104v2",
    "table_caption": "Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.",
    "table_column_names": [
      "[EMPTY]",
      "C-F1 100%",
      "C-F1 50%",
      "R-F1 100%",
      "R-F1 50%",
      "F1 100%",
      "F1 50%"
    ],
    "table_content_values": [
      [
        "Y-3",
        "49.59",
        "65.37",
        "26.28",
        "37.00",
        "34.35",
        "47.25"
      ],
      [
        "Y-3:Y<italic>C</italic>-1",
        "54.71",
        "66.84",
        "28.44",
        "37.35",
        "37.40",
        "47.92"
      ],
      [
        "Y-3:Y<italic>R</italic>-1",
        "51.32",
        "66.49",
        "26.92",
        "37.18",
        "35.31",
        "47.69"
      ],
      [
        "Y-3:Y<italic>C</italic>-3",
        "<bold>54.58</bold>",
        "67.66",
        "<bold>30.22</bold>",
        "<bold>40.30</bold>",
        "<bold>38.90</bold>",
        "<bold>50.51</bold>"
      ],
      [
        "Y-3:Y<italic>R</italic>-3",
        "53.31",
        "66.71",
        "26.65",
        "35.86",
        "35.53",
        "46.64"
      ],
      [
        "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
        "52.95",
        "<bold>67.84</bold>",
        "27.90",
        "39.71",
        "36.54",
        "50.09"
      ],
      [
        "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
        "54.55",
        "67.60",
        "28.30",
        "38.26",
        "37.26",
        "48.86"
      ]
    ],
    "id": "53cdc8d3-bd81-4d5d-8572-79b80c41480d",
    "claim": "We find that when we train STagBL with only its main task—with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance increases typically by a few percentage points.",
    "label": "supports",
    "table_id": "d0039005-f056-4745-9652-8796a2c2b307"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "a0df0012-c351-4b59-b594-86522f72ffda",
    "claim": "the blue marker represents the ratio of the \"Full\" score, and the orange marker denotes the ratio of “Other”.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "0ed421ff-4061-441d-bbdd-6f1cdc44ca0b",
    "claim": "When we increase the DCGCN blocks from 1 to 4, the model performance does not necessarily increase on AMR15 development set.",
    "label": "refutes",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
    "table_column_names": [
      "[BOLD] Decoder configuration",
      "[BOLD] es-en  [BOLD] Health",
      "[BOLD] es-en  [BOLD] Bio",
      "[BOLD] en-de  [BOLD] News",
      "[BOLD] en-de  [BOLD] TED",
      "[BOLD] en-de  [BOLD] IT"
    ],
    "table_content_values": [
      [
        "Oracle model",
        "35.9",
        "36.1",
        "37.8",
        "24.1",
        "39.6"
      ],
      [
        "Uniform",
        "33.1",
        "36.4",
        "21.9",
        "18.4",
        "38.9"
      ],
      [
        "Identity-BI",
        "35.0",
        "36.6",
        "32.7",
        "25.3",
        "42.6"
      ],
      [
        "BI",
        "35.9",
        "36.5",
        "38.0",
        "26.1",
        "[BOLD] 44.7"
      ],
      [
        "IS",
        "[BOLD] 36.0",
        "36.8",
        "37.5",
        "25.6",
        "43.3"
      ],
      [
        "BI + IS",
        "[BOLD] 36.0",
        "[BOLD] 36.9",
        "[BOLD] 38.4",
        "[BOLD] 26.4",
        "[BOLD] 44.7"
      ]
    ],
    "id": "af3ebe5e-4d9c-48e9-83d1-6039796679bf",
    "claim": "Table 5 shows improvements on data without domain labelling using our adaptive decoding schemes with unadapted models trained only on one domain [CONTINUE] Uniform ensembling under-performs all oracle models except es-en Bio, especially on general domains.",
    "label": "supports",
    "table_id": "f27c1026-3ef4-450e-97a8-368ddf0d0848"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "ffc551ba-ac96-4d34-ab5f-fec1d0e20a57",
    "claim": "It can be observed that the learned reward function does not have good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise.",
    "label": "refutes",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 4: Experiment 2, t= “b*tch”",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.010",
        "0.010",
        "-0.632",
        "[EMPTY]",
        "0.978"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.963",
        "0.944",
        "20.064",
        "***",
        "1.020"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.011",
        "0.011",
        "-1.254",
        "[EMPTY]",
        "0.955"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.349",
        "0.290",
        "28.803",
        "***",
        "1.203"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.012",
        "0.012",
        "-0.162",
        "[EMPTY]",
        "0.995"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.017",
        "0.015",
        "4.698",
        "***",
        "1.152"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.988",
        "0.991",
        "-6.289",
        "***",
        "0.997"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.099",
        "0.091",
        "6.273",
        "***",
        "1.091"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.074",
        "0.027",
        "46.054",
        "***",
        "2.728"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.925",
        "0.968",
        "-41.396",
        "***",
        "0.956"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.010",
        "0.010",
        "0.000",
        "[EMPTY]",
        "1.000"
      ]
    ],
    "id": "26382fa1-036e-4dcf-bf81-08d8763ef416",
    "claim": "The Waseem and Hovy (2016) classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned [CONTINUE] tweets predicted to belong to this class.",
    "label": "supports",
    "table_id": "def49c38-6913-4cba-9692-08a2b37b5640"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
    "table_column_names": [
      "ID LSTM-800",
      "5-fold CV 70.56",
      "Δ 0.66",
      "Single model 67.54",
      "Δ 0.78",
      "Ensemble 67.65",
      "Δ 0.30"
    ],
    "table_content_values": [
      [
        "LSTM-400",
        "70.50",
        "0.60",
        "[BOLD] 67.59",
        "0.83",
        "[BOLD] 68.00",
        "0.65"
      ],
      [
        "IN-TITLE",
        "70.11",
        "0.21",
        "[EMPTY]",
        "[EMPTY]",
        "67.52",
        "0.17"
      ],
      [
        "[BOLD] SUBMISSION",
        "69.90",
        "–",
        "66.76",
        "–",
        "67.35",
        "–"
      ],
      [
        "NO-HIGHWAY",
        "69.72",
        "−0.18",
        "66.42",
        "−0.34",
        "66.64",
        "−0.71"
      ],
      [
        "NO-OVERLAPS",
        "69.46",
        "−0.44",
        "65.07",
        "−1.69",
        "66.47",
        "−0.88"
      ],
      [
        "LSTM-400-DROPOUT",
        "69.45",
        "−0.45",
        "65.53",
        "−1.23",
        "67.28",
        "−0.07"
      ],
      [
        "NO-TRANSLATIONS",
        "69.42",
        "−0.48",
        "65.92",
        "−0.84",
        "67.23",
        "−0.12"
      ],
      [
        "NO-ELMO-FINETUNING",
        "67.71",
        "−2.19",
        "65.16",
        "−1.60",
        "65.42",
        "−1.93"
      ]
    ],
    "id": "335c8128-859a-4bb2-808d-c48b428dd5d0",
    "claim": "[CONTINUE] The 'alternating' LSTM layout we chose for our submission actually outperformed the 'traditional' one in terms of both single model and ensemble performance.",
    "label": "refutes",
    "table_id": "0980bab3-642c-413e-8b15-6abd868956a8"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "7e1aa17c-ab4a-421b-a158-486615a41865",
    "claim": "Firstly, we use a simple rule-based classifier where each word after punctuation marks that are not parenthesis, brackets, or quotes, is predicted to be in the scope of a negation.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "1d4b85e2-79a4-463d-873b-f6bdcef4fa2f",
    "claim": "[CONTINUE] Lin-SVM outperforms other classifiers in extracting most relations.",
    "label": "supports",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "3df48964-f174-4875-97f2-dee5dfb515c5",
    "claim": "[CONTINUE] Though ALDM obtains a higher inform F1 and match rate than PPO, it does not get a significant improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3.",
    "label": "refutes",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 2: Test error (ERR) on document classification task. “#Params”: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "AmaPolar ERR",
      "AmaPolar Time",
      "Yahoo ERR",
      "Yahoo Time",
      "AmaFull ERR",
      "AmaFull Time",
      "YelpPolar ERR",
      "YelpPolar Time"
    ],
    "table_content_values": [
      [
        "Zhang et al. ( 2015 )",
        "Zhang et al. ( 2015 )",
        "-",
        "6.10",
        "-",
        "29.16",
        "-",
        "40.57",
        "-",
        "5.26",
        "-"
      ],
      [
        "This",
        "LSTM",
        "227K",
        "[BOLD] 4.37",
        "0.947",
        "[BOLD] 24.62",
        "1.332",
        "37.22",
        "1.003",
        "3.58",
        "1.362"
      ],
      [
        "This",
        "GRU",
        "176K",
        "4.39",
        "0.948",
        "24.68",
        "1.242",
        "[BOLD] 37.20",
        "0.982",
        "[BOLD] 3.47",
        "1.230"
      ],
      [
        "This",
        "ATR",
        "74K",
        "4.78",
        "0.867",
        "25.33",
        "1.117",
        "38.54",
        "0.836",
        "4.00",
        "1.124"
      ],
      [
        "Work",
        "SRU",
        "194K",
        "4.95",
        "0.919",
        "24.78",
        "1.394",
        "38.23",
        "0.907",
        "3.99",
        "1.310"
      ],
      [
        "[EMPTY]",
        "LRN",
        "151K",
        "4.98",
        "[BOLD] 0.731",
        "25.07",
        "[BOLD] 1.038",
        "38.42",
        "[BOLD] 0.788",
        "3.98",
        "[BOLD] 1.022"
      ]
    ],
    "id": "07dfd438-80f3-41d8-ba09-1f769f983131",
    "claim": "LRN does not accelerate the training over LSTM and SRU by about 20%.",
    "label": "refutes",
    "table_id": "68b50739-9265-4c71-b076-3e790e4552ed"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "1d89289c-47a5-4052-8b51-a516aead51a8",
    "claim": "[CONTINUE] Across unigrams, part-of-speech patterns and word clusters, we see a distinctive pattern emerging around pronoun usage.",
    "label": "supports",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "paper_id": "1905.12516v1",
    "table_caption": "Table 4: Experiment 2, t= “b*tch”",
    "table_column_names": [
      "Dataset",
      "Class",
      "ˆ [ITALIC] piblack",
      "ˆ [ITALIC] piwhite",
      "[ITALIC] t",
      "[ITALIC] p",
      "ˆ [ITALIC] piblackˆ [ITALIC] piwhite"
    ],
    "table_content_values": [
      [
        "[ITALIC] Waseem and Hovy",
        "Racism",
        "0.010",
        "0.010",
        "-0.632",
        "[EMPTY]",
        "0.978"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.963",
        "0.944",
        "20.064",
        "***",
        "1.020"
      ],
      [
        "[ITALIC] Waseem",
        "Racism",
        "0.011",
        "0.011",
        "-1.254",
        "[EMPTY]",
        "0.955"
      ],
      [
        "[EMPTY]",
        "Sexism",
        "0.349",
        "0.290",
        "28.803",
        "***",
        "1.203"
      ],
      [
        "[EMPTY]",
        "Racism and sexism",
        "0.012",
        "0.012",
        "-0.162",
        "[EMPTY]",
        "0.995"
      ],
      [
        "[ITALIC] Davidson et al.",
        "Hate",
        "0.017",
        "0.015",
        "4.698",
        "***",
        "1.152"
      ],
      [
        "[EMPTY]",
        "Offensive",
        "0.988",
        "0.991",
        "-6.289",
        "***",
        "0.997"
      ],
      [
        "[ITALIC] Golbeck et al.",
        "Harassment",
        "0.099",
        "0.091",
        "6.273",
        "***",
        "1.091"
      ],
      [
        "[ITALIC] Founta et al.",
        "Hate",
        "0.074",
        "0.027",
        "46.054",
        "***",
        "2.728"
      ],
      [
        "[EMPTY]",
        "Abusive",
        "0.925",
        "0.968",
        "-41.396",
        "***",
        "0.956"
      ],
      [
        "[EMPTY]",
        "Spam",
        "0.010",
        "0.010",
        "0.000",
        "[EMPTY]",
        "1.000"
      ]
    ],
    "id": "4366020a-5cdf-4758-aa6a-8185b337656e",
    "claim": "The Waseem and Hovy (2016) classifier is not particularly sensitive to the word \"b*tch\" with only 1% of black-aligned and 1% of white-aligned tweets predicted to belong to this class.",
    "label": "refutes",
    "table_id": "def49c38-6913-4cba-9692-08a2b37b5640"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] # pairs",
      "[BOLD] # words (doc)",
      "[BOLD] # sents (docs)",
      "[BOLD] # words (summary)",
      "[BOLD] # sents (summary)",
      "[BOLD] vocab size"
    ],
    "table_content_values": [
      [
        "Multi-News",
        "44,972/5,622/5,622",
        "2,103.49",
        "82.73",
        "263.66",
        "9.97",
        "666,515"
      ],
      [
        "DUC03+04",
        "320",
        "4,636.24",
        "173.15",
        "109.58",
        "2.88",
        "19,734"
      ],
      [
        "TAC 2011",
        "176",
        "4,695.70",
        "188.43",
        "99.70",
        "1.00",
        "24,672"
      ],
      [
        "CNNDM",
        "287,227/13,368/11,490",
        "810.57",
        "39.78",
        "56.20",
        "3.68",
        "717,951"
      ]
    ],
    "id": "5e7dfbf4-9542-4f80-a83b-44c4f4753db7",
    "claim": "The total number of words in the concatenated inputs is longer than other MDS datasets, as those consist of 10 input documents, but shorter than SDS datasets, as expected.",
    "label": "refutes",
    "table_id": "9fda9198-5e75-4f79-b42c-82b7dc3ee31f"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 4: Results of the ablation study on the LDC2017T10 development set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>",
      "<bold>Size</bold>"
    ],
    "table_content_values": [
      [
        "biLSTM",
        "22.50",
        "30.42",
        "57.6M"
      ],
      [
        "<italic>GEt</italic> + biLSTM",
        "26.33",
        "32.62",
        "59.6M"
      ],
      [
        "<italic>GEb</italic> + biLSTM",
        "26.12",
        "32.49",
        "59.6M"
      ],
      [
        "<italic>GEt</italic> + <italic>GEb</italic> + biLSTM",
        "27.37",
        "33.30",
        "61.7M"
      ]
    ],
    "id": "5d749e06-1775-42bd-b28f-d106eab9163f",
    "claim": "The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M).",
    "label": "supports",
    "table_id": "5dbdd29a-b64a-47ff-824a-6b8486b9ad4a"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
    "table_column_names": [
      "[BOLD] Representation",
      "[BOLD] Hyper parameters Filter size",
      "[BOLD] Hyper parameters Num. Feature maps",
      "[BOLD] Hyper parameters Activation func.",
      "[BOLD] Hyper parameters L2 Reg.",
      "[BOLD] Hyper parameters Learning rate",
      "[BOLD] Hyper parameters Dropout Prob.",
      "[BOLD] F1.(avg. in 5-fold) with default values",
      "[BOLD] F1.(avg. in 5-fold) with optimal values"
    ],
    "table_content_values": [
      [
        "CoNLL08",
        "4-5",
        "1000",
        "Softplus",
        "1.15e+01",
        "1.13e-03",
        "1",
        "73.34",
        "74.49"
      ],
      [
        "SB",
        "4-5",
        "806",
        "Sigmoid",
        "8.13e-02",
        "1.79e-03",
        "0.87",
        "72.83",
        "[BOLD] 75.05"
      ],
      [
        "UD v1.3",
        "5",
        "716",
        "Softplus",
        "1.66e+00",
        "9.63E-04",
        "1",
        "68.93",
        "69.57"
      ]
    ],
    "id": "c6b5330f-3246-4e62-a3c8-78aeae05a8f5",
    "claim": "The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation.",
    "label": "supports",
    "table_id": "602023a1-3ab5-4788-8101-59ae8f0c6ce1"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 3: Performance comparison of our model with different values of m on the two datasets.",
    "table_column_names": [
      "[ITALIC] m",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "1",
        "0.541",
        "0.595",
        "[BOLD] 0.566",
        "0.495",
        "0.621",
        "0.551"
      ],
      [
        "2",
        "0.521",
        "0.597",
        "0.556",
        "0.482",
        "0.656",
        "0.555"
      ],
      [
        "3",
        "0.490",
        "0.617",
        "0.547",
        "0.509",
        "0.633",
        "0.564"
      ],
      [
        "4",
        "0.449",
        "0.623",
        "0.522",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "5",
        "0.467",
        "0.609",
        "0.529",
        "0.488",
        "0.677",
        "0.567"
      ]
    ],
    "id": "0a6b428c-3dd6-45e7-82d2-a04db14738d7",
    "claim": "On the NYT11 dataset, m = 4 gives the best performance.",
    "label": "supports",
    "table_id": "fbd16514-f1a6-4567-8557-22b8d673c5b6"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "40b4cebb-43c9-4eb1-b17e-5416de928391",
    "claim": "Since only 20% of the tweets are used as negative training samples, we cannot use all negative tweets for development phase.",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.",
    "table_column_names": [
      "[ITALIC] Block",
      "[ITALIC] n",
      "[ITALIC] m",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "1",
        "1",
        "1",
        "17.6",
        "48.3"
      ],
      [
        "1",
        "1",
        "2",
        "19.2",
        "50.3"
      ],
      [
        "1",
        "2",
        "1",
        "18.4",
        "49.1"
      ],
      [
        "1",
        "1",
        "3",
        "19.6",
        "49.4"
      ],
      [
        "1",
        "3",
        "1",
        "20.0",
        "50.5"
      ],
      [
        "1",
        "3",
        "3",
        "21.4",
        "51.0"
      ],
      [
        "1",
        "3",
        "6",
        "21.8",
        "51.7"
      ],
      [
        "1",
        "6",
        "3",
        "21.7",
        "51.5"
      ],
      [
        "1",
        "6",
        "6",
        "22.0",
        "52.1"
      ],
      [
        "2",
        "3",
        "6",
        "[BOLD] 23.5",
        "53.3"
      ],
      [
        "2",
        "6",
        "3",
        "23.3",
        "[BOLD] 53.4"
      ],
      [
        "2",
        "6",
        "6",
        "22.0",
        "52.1"
      ]
    ],
    "id": "0ee1d441-a931-45f7-ae8d-11c809364782",
    "claim": "We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give similar results for both 1 DCGCN block and 2 DCGCN blocks.",
    "label": "supports",
    "table_id": "9ac477a4-1794-4d29-8be4-94cfe8c3b70a"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "90528389-92f2-4f21-8903-6700b00bcec4",
    "claim": "In some cases it seems to make no difference in results, e.g., Europarl in Portuguese which did not increase the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in no increase of f-measure from F=0.5555 in DF to F=0.6403 in TF.",
    "label": "refutes",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations",
    "paper_id": "1808.08672v2",
    "table_caption": "Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
    "table_column_names": [
      "[BOLD] Emoji alias",
      "[BOLD] N",
      "[BOLD] emoji #",
      "[BOLD] emoji %",
      "[BOLD] no-emoji #",
      "[BOLD] no-emoji %",
      "[BOLD] Δ%"
    ],
    "table_content_values": [
      [
        "mask",
        "163",
        "154",
        "94.48",
        "134",
        "82.21",
        "- 12.27"
      ],
      [
        "two_hearts",
        "87",
        "81",
        "93.10",
        "77",
        "88.51",
        "- 4.59"
      ],
      [
        "heart_eyes",
        "122",
        "109",
        "89.34",
        "103",
        "84.43",
        "- 4.91"
      ],
      [
        "heart",
        "267",
        "237",
        "88.76",
        "235",
        "88.01",
        "- 0.75"
      ],
      [
        "rage",
        "92",
        "78",
        "84.78",
        "66",
        "71.74",
        "- 13.04"
      ],
      [
        "cry",
        "116",
        "97",
        "83.62",
        "83",
        "71.55",
        "- 12.07"
      ],
      [
        "sob",
        "490",
        "363",
        "74.08",
        "345",
        "70.41",
        "- 3.67"
      ],
      [
        "unamused",
        "167",
        "121",
        "72.46",
        "116",
        "69.46",
        "- 3.00"
      ],
      [
        "weary",
        "204",
        "140",
        "68.63",
        "139",
        "68.14",
        "- 0.49"
      ],
      [
        "joy",
        "978",
        "649",
        "66.36",
        "629",
        "64.31",
        "- 2.05"
      ],
      [
        "sweat_smile",
        "111",
        "73",
        "65.77",
        "75",
        "67.57",
        "1.80"
      ],
      [
        "confused",
        "77",
        "46",
        "59.74",
        "48",
        "62.34",
        "2.60"
      ]
    ],
    "id": "96697120-9b1f-4d1a-b6f8-dcbc3571a596",
    "claim": "[CONTINUE] The most interesting ones are mask, rage, and cry, which significantly decrease accuracy.",
    "label": "refutes",
    "table_id": "2410b1c3-4d48-49e9-9279-dc23daa879d1"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "9435110a-5463-4110-9d72-57ce814067cf",
    "claim": "Finally, Table 5 shows the F1 score of the (in-scope, out-of-scope) negation scopes using Punctuation, our Proposed model and BiLSTM classifier.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "181b9881-e4d1-4e77-af0f-77df20be3b57",
    "claim": "the average length of “good” summaries is higher than that of other summaries.",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "e34b8982-4346-4879-b0ef-05f9a01e1536",
    "claim": "We observe that the B3 metric is harsher than the other two and is most suitable when a very high precision of entity identification is desired.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "f27a4cd3-518d-4311-b4e9-c12bfdeb6b12",
    "claim": "Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are able to learn the task to a high degree.",
    "label": "supports",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "ea497540-0fb6-42d3-971a-c539b056ba98",
    "claim": "Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings (Table 1).",
    "label": "supports",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
    "table_column_names": [
      "Uni",
      "POS",
      "0 87.9",
      "1 92.0",
      "2 91.7",
      "3 91.8",
      "4 91.9"
    ],
    "table_content_values": [
      [
        "Uni",
        "SEM",
        "81.8",
        "87.8",
        "87.4",
        "87.6",
        "88.2"
      ],
      [
        "Bi",
        "POS",
        "87.9",
        "93.3",
        "92.9",
        "93.2",
        "92.8"
      ],
      [
        "Bi",
        "SEM",
        "81.9",
        "91.3",
        "90.8",
        "91.9",
        "91.9"
      ],
      [
        "Res",
        "POS",
        "87.9",
        "92.5",
        "91.9",
        "92.0",
        "92.4"
      ],
      [
        "Res",
        "SEM",
        "81.9",
        "88.2",
        "87.5",
        "87.6",
        "88.5"
      ]
    ],
    "id": "afb8c8d5-9e11-40ab-92fb-f60a4ab66649",
    "claim": "Comparing POS and SEM tagging (Table 5), we note that higher layer representations improve SEM tagging, while POS tagging peaks at layer 1. we noticed small but consistent improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5).",
    "label": "supports",
    "table_id": "56a61e48-903a-4dc4-8cbf-2048d2c8ee3c"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
    "table_column_names": [
      "Corpus",
      "Metric",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "Europarl",
        "TotalTerms:",
        "957",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "836",
        "1,000"
      ],
      [
        "Europarl",
        "TotalRoots:",
        "44",
        "1",
        "1",
        "1",
        "1",
        "43",
        "1"
      ],
      [
        "Europarl",
        "NumberRels:",
        "1,588",
        "1,025",
        "1,028",
        "1,185",
        "1,103",
        "1,184",
        "999"
      ],
      [
        "Europarl",
        "MaxDepth:",
        "21",
        "921",
        "901",
        "788",
        "835",
        "8",
        "15"
      ],
      [
        "Europarl",
        "MinDepth:",
        "1",
        "921",
        "901",
        "788",
        "835",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgDepth:",
        "11.82",
        "921",
        "901",
        "788",
        "835",
        "3.05",
        "8.46"
      ],
      [
        "Europarl",
        "DepthCohesion:",
        "1.78",
        "1",
        "1",
        "1",
        "1",
        "2.62",
        "1.77"
      ],
      [
        "Europarl",
        "MaxWidth:",
        "20",
        "2",
        "3",
        "4",
        "3",
        "88",
        "41"
      ],
      [
        "Europarl",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgWidth:",
        "1.99",
        "1.03",
        "1.03",
        "1.19",
        "1.10",
        "4.20",
        "2.38"
      ],
      [
        "TED Talks",
        "TotalTerms:",
        "476",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000"
      ],
      [
        "TED Talks",
        "TotalRoots:",
        "164",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "NumberRels:",
        "521",
        "1,029",
        "1,331",
        "3,025",
        "3,438",
        "3,802",
        "1,009"
      ],
      [
        "TED Talks",
        "MaxDepth:",
        "16",
        "915",
        "658",
        "454",
        "395",
        "118",
        "12"
      ],
      [
        "TED Talks",
        "MinDepth:",
        "1",
        "913",
        "658",
        "454",
        "395",
        "110",
        "1"
      ],
      [
        "TED Talks",
        "AvgDepth:",
        "5.82",
        "914",
        "658",
        "454",
        "395",
        "112.24",
        "5.95"
      ],
      [
        "TED Talks",
        "DepthCohesion:",
        "2.75",
        "1",
        "1",
        "1",
        "1",
        "1.05",
        "2.02"
      ],
      [
        "TED Talks",
        "MaxWidth:",
        "25",
        "2",
        "77",
        "13",
        "12",
        "66",
        "98"
      ],
      [
        "TED Talks",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "AvgWidth:",
        "1.83",
        "1.03",
        "1.36",
        "3.03",
        "3.44",
        "6.64",
        "2.35"
      ]
    ],
    "id": "2373a5b5-05cc-45ca-9e6c-5323513811b8",
    "claim": "[CONTINUE] As we can observe in Table 6, limiting the number of terms to 1,000, Patt and DocSub do not to generate relations for all terms.",
    "label": "supports",
    "table_id": "7fd32b5d-95c3-40d7-8e3d-16391cad7f14"
  },
  {
    "paper": "What do Deep Networks Like to Read?",
    "paper_id": "1909.04547v1",
    "table_caption": "Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
    "table_column_names": [
      "[EMPTY]",
      "<bold>RNN</bold>",
      "<bold>CNN</bold>",
      "<bold>DAN</bold>"
    ],
    "table_content_values": [
      [
        "Positive",
        "+9.7",
        "+4.3",
        "+<bold>23.6</bold>"
      ],
      [
        "Negative",
        "+6.9",
        "+5.5",
        "+<bold>16.1</bold>"
      ],
      [
        "Flipped to Positive",
        "+20.2",
        "+24.9",
        "+27.4"
      ],
      [
        "Flipped to Negative",
        "+31.5",
        "+28.6",
        "+19.3"
      ]
    ],
    "id": "db25a77b-8a01-4cdf-9925-4697cd1d307f",
    "claim": "This is especially true in the case of DAN where we see a decrease as the decoder repeatedly predicts words having low sentiment value.",
    "label": "refutes",
    "table_id": "fe569d5a-7fe9-4318-80c3-68d5d6108755"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "a7269b76-9a15-4b21-8159-01234702d223",
    "claim": "It achieves competitive results using only the title and body text, in comparison to the R-1,2,L reward systems that integrate multi-task models (Narayan et\\xa0al.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Training scheme",
      "[BOLD] News",
      "[BOLD] TED",
      "[BOLD] IT"
    ],
    "table_content_values": [
      [
        "1",
        "News",
        "37.8",
        "25.3",
        "35.3"
      ],
      [
        "2",
        "TED",
        "23.7",
        "24.1",
        "14.4"
      ],
      [
        "3",
        "IT",
        "1.6",
        "1.8",
        "39.6"
      ],
      [
        "4",
        "News and TED",
        "38.2",
        "25.5",
        "35.4"
      ],
      [
        "5",
        "1 then TED, No-reg",
        "30.6",
        "[BOLD] 27.0",
        "22.1"
      ],
      [
        "6",
        "1 then TED, L2",
        "37.9",
        "26.7",
        "31.8"
      ],
      [
        "7",
        "1 then TED, EWC",
        "[BOLD] 38.3",
        "[BOLD] 27.0",
        "33.1"
      ],
      [
        "8",
        "5 then IT, No-reg",
        "8.0",
        "6.9",
        "56.3"
      ],
      [
        "9",
        "6 then IT, L2",
        "32.3",
        "22.6",
        "56.9"
      ],
      [
        "10",
        "7 then IT, EWC",
        "35.8",
        "24.6",
        "[BOLD] 57.0"
      ]
    ],
    "id": "b5d5b9ea-69ce-4b98-99de-831145e49e2b",
    "claim": "However, EWC outperforms no-reg and L2 on News, not only reducing forgetting but giving 0.5 BLEU improvement over the baseline News model.",
    "label": "supports",
    "table_id": "2c8215aa-6c63-49db-ae71-e1c029b6e82c"
  },
  {
    "paper": "Suggestion Mining from Online Reviews using ULMFiT",
    "paper_id": "1904.09076v1",
    "table_caption": "Table 3: Performance of different models on the provided train and test dataset for Sub Task A.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] F1 (train)",
      "[BOLD] F1 (test)"
    ],
    "table_content_values": [
      [
        "[BOLD] Multinomial Naive Bayes (using Count Vectorizer)",
        "0.641",
        "0.517"
      ],
      [
        "[BOLD] Logistic Regression (using Count Vectorizer)",
        "0.679",
        "0.572"
      ],
      [
        "[BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer)",
        "0.695",
        "0.576"
      ],
      [
        "[BOLD] LSTM (128 LSTM Units)",
        "0.731",
        "0.591"
      ],
      [
        "[BOLD] Provided Baseline",
        "0.720",
        "0.267"
      ],
      [
        "[BOLD] ULMFit*",
        "0.861",
        "0.701"
      ]
    ],
    "id": "d85b8a88-a37f-4803-a198-3a7032d6e695",
    "claim": "[CONTINUE] The Logistic Regression model achieved the best results with a F1-score of 0.679 on the training dataset and a F1-score of 0.572 on the test dataset.",
    "label": "refutes",
    "table_id": "27322e32-7392-4d54-a916-ac0742e29c2e"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "53819988-8969-430d-93e2-594f7fb8d007",
    "claim": "These result reveal that there exist trade-offs between the different metrics and that a DLM-based algorithm is better suited to solve the user simulation problem than reinforcement learning",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "23c4f4c9-27e5-4284-8229-470a13efae02",
    "claim": "Furthermore, the scope length of negative instances is at the range of 0-8 tokens, with an average scope length of 2.9 tokens.",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.",
    "table_column_names": [
      "[EMPTY]",
      "dev perp ↓",
      "dev acc ↑",
      "dev wer ↓",
      "test perp ↓",
      "test acc ↑",
      "test wer ↓"
    ],
    "table_content_values": [
      [
        "Spanish-only-LM",
        "329.68",
        "26.6",
        "30.47",
        "322.26",
        "25.1",
        "29.62"
      ],
      [
        "English-only-LM",
        "320.92",
        "29.3",
        "32.02",
        "314.04",
        "30.3",
        "32.51"
      ],
      [
        "All:CS-last-LM",
        "76.64",
        "47.8",
        "14.56",
        "76.97",
        "49.2",
        "14.13"
      ],
      [
        "All:Shuffled-LM",
        "68.00",
        "51.8",
        "13.64",
        "68.72",
        "51.4",
        "13.89"
      ],
      [
        "CS-only-LM",
        "43.20",
        "60.7",
        "12.60",
        "43.42",
        "57.9",
        "12.18"
      ],
      [
        "CS-only+vocab-LM",
        "45.61",
        "61.0",
        "12.56",
        "45.79",
        "58.8",
        "12.49"
      ],
      [
        "Fine-Tuned-LM",
        "39.76",
        "66.9",
        "10.71",
        "40.11",
        "65.4",
        "10.17"
      ],
      [
        "CS-only-disc",
        "–",
        "72.0",
        "6.35",
        "–",
        "70.5",
        "6.70"
      ],
      [
        "Fine-Tuned-disc",
        "–",
        "[BOLD] 74.2",
        "[BOLD] 5.85",
        "–",
        "[BOLD] 75.5",
        "[BOLD] 5.59"
      ]
    ],
    "id": "773f8b3d-676b-44d0-b830-93b964c3976c",
    "claim": "We gain further improvement by adding monolingual data and get an even higher accuracy of 75.5%, which is 10.1 points higher than the best language model.",
    "label": "supports",
    "table_id": "d4c8c3ca-899e-4e8c-8efe-81b447163aa5"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 4: Precisions on the Wikidata dataset with different choice of d.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC",
      "Time"
    ],
    "table_content_values": [
      [
        "[ITALIC] d=1",
        "0.602",
        "0.487",
        "0.403",
        "0.367",
        "4h"
      ],
      [
        "[ITALIC] d=32",
        "0.645",
        "0.501",
        "0.393",
        "0.370",
        "-"
      ],
      [
        "[ITALIC] d=16",
        "0.655",
        "0.518",
        "0.413",
        "0.413",
        "20h"
      ],
      [
        "[ITALIC] d=8",
        "0.650",
        "0.519",
        "0.422",
        "0.405",
        "8h"
      ]
    ],
    "id": "10c6f16f-2759-48a2-b4eb-f82c9ecce72e",
    "claim": "Thus, after taking the depth in KG into consideration, the precision increases to 19.47%, which increases the AUC score to 0.413.",
    "label": "not enough info",
    "table_id": "b3925d14-8042-410e-841f-73dce02fc49b"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "[BOLD] Baselines",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen ( 2015a )",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. ( 2018 )",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "[BOLD] Model Variants",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "5e806a3c-f8ff-469f-9d27-e6fc37a34a3d",
    "claim": "Our joint model outperforms all the base [CONTINUE] lines with a gap of 10.5 CoNLL F1 points from the last published results (KCP), while surpassing our strong lemma baseline by 3 points.",
    "label": "supports",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 7: Results of the human evaluation of the responses produced by our model. A response is acceptable if it is either good or great. Note: Numbers may not add up to 100% due to rounding.",
    "table_column_names": [
      "[BOLD] Whitelist",
      "[BOLD] Great",
      "[BOLD] Good",
      "[BOLD] Bad",
      "[BOLD] Accept"
    ],
    "table_content_values": [
      [
        "Freq. 1K",
        "54%",
        "26%",
        "20%",
        "80%"
      ],
      [
        "Cluster. 1K",
        "55%",
        "21%",
        "23%",
        "77%"
      ],
      [
        "Freq. 10K",
        "56%",
        "24%",
        "21%",
        "80%"
      ],
      [
        "Cluster. 10K",
        "57%",
        "23%",
        "20%",
        "80%"
      ],
      [
        "Real response",
        "60%",
        "24%",
        "16%",
        "84%"
      ]
    ],
    "id": "cf78ecf8-180e-4067-9f5f-5091c236de7d",
    "claim": "Interestingly, the size and type of whitelist have a significant effect on performance, indicating that all the whitelists do not contain responses appropriate to a variety of conversational contexts.",
    "label": "refutes",
    "table_id": "28834fcb-affd-4e2d-9c79-c4298b5936bd"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 3: Cue and token distribution in the conversational negation corpus.",
    "table_column_names": [
      "Total negation cues",
      "2921"
    ],
    "table_content_values": [
      [
        "True negation cues",
        "2674"
      ],
      [
        "False negation cues",
        "247"
      ],
      [
        "Average scope length",
        "2.9"
      ],
      [
        "Average sentence length",
        "13.6"
      ],
      [
        "Average tweet length",
        "22.3"
      ]
    ],
    "id": "a5ca51e3-76e6-4054-a5b3-f85f9c2987dc",
    "claim": "Most of the false negation cues correspond to contracted negations (e.g., “haven’t”).",
    "label": "not enough info",
    "table_id": "eb1db734-3cd4-4bbe-bb91-18652492df66"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 1: Effect of using the shortest dependency path on each relation type.",
    "table_column_names": [
      "[BOLD] Relation",
      "[BOLD] best F1 (in 5-fold) without sdp",
      "[BOLD] best F1 (in 5-fold) with sdp",
      "[BOLD] Diff."
    ],
    "table_content_values": [
      [
        "USAGE",
        "60.34",
        "80.24",
        "+ 19.90"
      ],
      [
        "MODEL-FEATURE",
        "48.89",
        "70.00",
        "+ 21.11"
      ],
      [
        "PART_WHOLE",
        "29.51",
        "70.27",
        "+40.76"
      ],
      [
        "TOPIC",
        "45.80",
        "91.26",
        "+45.46"
      ],
      [
        "RESULT",
        "54.35",
        "81.58",
        "+27.23"
      ],
      [
        "COMPARE",
        "20.00",
        "61.82",
        "+ 41.82"
      ],
      [
        "macro-averaged",
        "50.10",
        "76.10",
        "+26.00"
      ]
    ],
    "id": "3f8320d5-deeb-473d-ba46-1fead5ed6bde",
    "claim": "We find that the effect of syntactic structure is consistent across the different relation types.",
    "label": "refutes",
    "table_id": "1eca16bf-a63a-45a1-a4a3-84f12cf74c95"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "9b196ff7-6836-42f0-9f3e-a6a56543acbd",
    "claim": "Coverage helps the model improve its EM by 1.5 and its F1 by 0.5.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "601420ef-2d1c-4385-a5a1-9fda20185823",
    "claim": "We first use order-based feature which is relative to PPO to show our improvement.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "a5751137-2fe5-4016-8932-c418dc82cae4",
    "claim": "[CONTINUE] In addition, the presence of verbs in past participle (VBN) is the most distinctive part-of-speech pattern of complaints.",
    "label": "supports",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "ae63ad57-2a1f-45be-84c1-7468ed337a30",
    "claim": "for example, for BERT, the error rates for all the runs are negative with at most 0.05% accuracy loss and at most 0.12% accuracy gain",
    "label": "not enough info",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "Keyphrase Generation for Scientific Articles using GANs",
    "paper_id": "1909.12229v1",
    "table_caption": "Table 2: α-nDCG@5 metrics",
    "table_column_names": [
      "Model",
      "Inspec",
      "Krapivin",
      "NUS",
      "KP20k"
    ],
    "table_content_values": [
      [
        "Catseq",
        "0.87803",
        "0.781",
        "0.82118",
        "0.804"
      ],
      [
        "Catseq-RL",
        "0.8602",
        "[BOLD] 0.786",
        "0.83",
        "0.809"
      ],
      [
        "GAN",
        "[BOLD] 0.891",
        "0.771",
        "[BOLD] 0.853",
        "[BOLD] 0.85"
      ]
    ],
    "id": "813352e2-948e-4c73-b239-4420a1634970",
    "claim": "The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is only marginally better than both the other baseline models.",
    "label": "refutes",
    "table_id": "db0cb6ad-ade2-4926-8a07-f92a9d74c055"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 5: Textual similarity scores (asymmetric, Multi30k).",
    "table_column_names": [
      "[EMPTY]",
      "EN → DE R@1",
      "EN → DE R@5",
      "EN → DE R@10",
      "DE → EN R@1",
      "DE → EN R@5",
      "DE → EN R@10"
    ],
    "table_content_values": [
      [
        "FME",
        "51.4",
        "76.4",
        "84.5",
        "46.9",
        "71.2",
        "79.1"
      ],
      [
        "AME",
        "[BOLD] 51.7",
        "[BOLD] 76.7",
        "[BOLD] 85.1",
        "[BOLD] 49.1",
        "[BOLD] 72.6",
        "[BOLD] 80.5"
      ]
    ],
    "id": "d2534ed4-c340-4211-8610-924f9fb9c445",
    "claim": "FME outperforms the AME model, confirming the importance of word embeddings adaptation.",
    "label": "refutes",
    "table_id": "7a510b9a-eb7e-4be5-8a97-0694604a8f80"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
    "table_column_names": [
      "Corpus",
      "Metric",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "Europarl",
        "TotalTerms:",
        "957",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "836",
        "1,000"
      ],
      [
        "Europarl",
        "TotalRoots:",
        "44",
        "1",
        "1",
        "1",
        "1",
        "43",
        "1"
      ],
      [
        "Europarl",
        "NumberRels:",
        "1,588",
        "1,025",
        "1,028",
        "1,185",
        "1,103",
        "1,184",
        "999"
      ],
      [
        "Europarl",
        "MaxDepth:",
        "21",
        "921",
        "901",
        "788",
        "835",
        "8",
        "15"
      ],
      [
        "Europarl",
        "MinDepth:",
        "1",
        "921",
        "901",
        "788",
        "835",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgDepth:",
        "11.82",
        "921",
        "901",
        "788",
        "835",
        "3.05",
        "8.46"
      ],
      [
        "Europarl",
        "DepthCohesion:",
        "1.78",
        "1",
        "1",
        "1",
        "1",
        "2.62",
        "1.77"
      ],
      [
        "Europarl",
        "MaxWidth:",
        "20",
        "2",
        "3",
        "4",
        "3",
        "88",
        "41"
      ],
      [
        "Europarl",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgWidth:",
        "1.99",
        "1.03",
        "1.03",
        "1.19",
        "1.10",
        "4.20",
        "2.38"
      ],
      [
        "TED Talks",
        "TotalTerms:",
        "476",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000"
      ],
      [
        "TED Talks",
        "TotalRoots:",
        "164",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "NumberRels:",
        "521",
        "1,029",
        "1,331",
        "3,025",
        "3,438",
        "3,802",
        "1,009"
      ],
      [
        "TED Talks",
        "MaxDepth:",
        "16",
        "915",
        "658",
        "454",
        "395",
        "118",
        "12"
      ],
      [
        "TED Talks",
        "MinDepth:",
        "1",
        "913",
        "658",
        "454",
        "395",
        "110",
        "1"
      ],
      [
        "TED Talks",
        "AvgDepth:",
        "5.82",
        "914",
        "658",
        "454",
        "395",
        "112.24",
        "5.95"
      ],
      [
        "TED Talks",
        "DepthCohesion:",
        "2.75",
        "1",
        "1",
        "1",
        "1",
        "1.05",
        "2.02"
      ],
      [
        "TED Talks",
        "MaxWidth:",
        "25",
        "2",
        "77",
        "13",
        "12",
        "66",
        "98"
      ],
      [
        "TED Talks",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "AvgWidth:",
        "1.83",
        "1.03",
        "1.36",
        "3.03",
        "3.44",
        "6.64",
        "2.35"
      ]
    ],
    "id": "bf52da4c-3af3-4cc3-9e6d-19e0744ef2fe",
    "claim": "Patt model could not generate relations for all terms because terms must to be in a pattern in order to have their taxonomic relation identified.",
    "label": "supports",
    "table_id": "7fd32b5d-95c3-40d7-8e3d-16391cad7f14"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "96c086ca-dbf6-4f2f-b5ca-e65b06ea3b23",
    "claim": "However, the slightly increased invalid response percentage for the DAMD (+) model compared to the HDSA (+) model suggests that data augmentation may not be the most effective approach. We also observe that HDSA (+) outperforms DAMD in both diversity and appropriateness scores.",
    "label": "refutes",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. “Pref%”: in how many percentage of documents a system receives the higher human rating.",
    "table_column_names": [
      "Reward",
      "R-1",
      "R-2",
      "R-L",
      "Human",
      "Pref%"
    ],
    "table_content_values": [
      [
        "R-L (original)",
        "40.9",
        "17.8",
        "38.5",
        "1.75",
        "15"
      ],
      [
        "Learned (ours)",
        "39.2",
        "17.4",
        "37.5",
        "[BOLD] 2.20",
        "[BOLD] 75"
      ]
    ],
    "id": "1354fd83-f529-48f9-9a42-981bb82374b2",
    "claim": "When using our learned reward, the generated summaries have significantly higher average human ratings than when using ROUGE as rewards.",
    "label": "refutes",
    "table_id": "f5343270-2393-4ff2-85d1-a445b36215ea"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Feature",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Feature",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams",
        "[BOLD] Unigrams"
      ],
      [
        "not",
        ".154",
        "[URL]",
        ".150"
      ],
      [
        "my",
        ".131",
        "!",
        ".082"
      ],
      [
        "working",
        ".124",
        "he",
        ".069"
      ],
      [
        "still",
        ".123",
        "thank",
        ".067"
      ],
      [
        "on",
        ".119",
        ",",
        ".064"
      ],
      [
        "can’t",
        ".113",
        "love",
        ".064"
      ],
      [
        "service",
        ".112",
        "lol",
        ".061"
      ],
      [
        "customer",
        ".109",
        "you",
        ".060"
      ],
      [
        "why",
        ".108",
        "great",
        ".058"
      ],
      [
        "website",
        ".107",
        "win",
        ".058"
      ],
      [
        "no",
        ".104",
        "’",
        ".058"
      ],
      [
        "?",
        ".098",
        "she",
        ".054"
      ],
      [
        "fix",
        ".093",
        ":",
        ".053"
      ],
      [
        "won’t",
        ".092",
        "that",
        ".053"
      ],
      [
        "been",
        ".090",
        "more",
        ".052"
      ],
      [
        "issue",
        ".089",
        "it",
        ".052"
      ],
      [
        "days",
        ".088",
        "would",
        ".051"
      ],
      [
        "error",
        ".087",
        "him",
        ".047"
      ],
      [
        "is",
        ".084",
        "life",
        ".046"
      ],
      [
        "charged",
        ".083",
        "good",
        ".046"
      ],
      [
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)",
        "[BOLD] POS (Unigrams and Bigrams)"
      ],
      [
        "VBN",
        ".141",
        "UH",
        ".104"
      ],
      [
        "$",
        ".118",
        "NNP",
        ".098"
      ],
      [
        "VBZ",
        ".114",
        "PRP",
        ".076"
      ],
      [
        "NN_VBZ",
        ".114",
        "HT",
        ".076"
      ],
      [
        "PRP$",
        ".107",
        "PRP_.",
        ".076"
      ],
      [
        "PRP$_NN",
        ".105",
        "PRP_RB",
        ".067"
      ],
      [
        "VBG",
        ".093",
        "NNP_NNP",
        ".062"
      ],
      [
        "CD",
        ".092",
        "VBP_PRP",
        ".054"
      ],
      [
        "WRB_VBZ",
        ".084",
        "JJ",
        ".053"
      ],
      [
        "VBZ_VBN",
        ".084",
        "DT_JJ",
        ".051"
      ]
    ],
    "id": "cbb555a9-2b08-4e5d-a9fe-216378072ded",
    "claim": "[CONTINUE] Mentions of time are specific of complaints (been, still, on, days, Temporal References cluster).",
    "label": "supports",
    "table_id": "40492e01-0267-4ccb-8a18-4b2936e6bbb0"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] # pairs",
      "[BOLD] # words (doc)",
      "[BOLD] # sents (docs)",
      "[BOLD] # words (summary)",
      "[BOLD] # sents (summary)",
      "[BOLD] vocab size"
    ],
    "table_content_values": [
      [
        "Multi-News",
        "44,972/5,622/5,622",
        "2,103.49",
        "82.73",
        "263.66",
        "9.97",
        "666,515"
      ],
      [
        "DUC03+04",
        "320",
        "4,636.24",
        "173.15",
        "109.58",
        "2.88",
        "19,734"
      ],
      [
        "TAC 2011",
        "176",
        "4,695.70",
        "188.43",
        "99.70",
        "1.00",
        "24,672"
      ],
      [
        "CNNDM",
        "287,227/13,368/11,490",
        "810.57",
        "39.78",
        "56.20",
        "3.68",
        "717,951"
      ]
    ],
    "id": "1ff5fc91-911a-4368-876e-b26811139368",
    "claim": "Our summaries are notably shorter than in other works, about 260 words on average.",
    "label": "refutes",
    "table_id": "9fda9198-5e75-4f79-b42c-82b7dc3ee31f"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.",
    "table_column_names": [
      "[EMPTY]",
      "dev perp ↓",
      "dev acc ↑",
      "dev wer ↓",
      "test perp ↓",
      "test acc ↑",
      "test wer ↓"
    ],
    "table_content_values": [
      [
        "Spanish-only-LM",
        "329.68",
        "26.6",
        "30.47",
        "322.26",
        "25.1",
        "29.62"
      ],
      [
        "English-only-LM",
        "320.92",
        "29.3",
        "32.02",
        "314.04",
        "30.3",
        "32.51"
      ],
      [
        "All:CS-last-LM",
        "76.64",
        "47.8",
        "14.56",
        "76.97",
        "49.2",
        "14.13"
      ],
      [
        "All:Shuffled-LM",
        "68.00",
        "51.8",
        "13.64",
        "68.72",
        "51.4",
        "13.89"
      ],
      [
        "CS-only-LM",
        "43.20",
        "60.7",
        "12.60",
        "43.42",
        "57.9",
        "12.18"
      ],
      [
        "CS-only+vocab-LM",
        "45.61",
        "61.0",
        "12.56",
        "45.79",
        "58.8",
        "12.49"
      ],
      [
        "Fine-Tuned-LM",
        "39.76",
        "66.9",
        "10.71",
        "40.11",
        "65.4",
        "10.17"
      ],
      [
        "CS-only-disc",
        "–",
        "72.0",
        "6.35",
        "–",
        "70.5",
        "6.70"
      ],
      [
        "Fine-Tuned-disc",
        "–",
        "[BOLD] 74.2",
        "[BOLD] 5.85",
        "–",
        "[BOLD] 75.5",
        "[BOLD] 5.59"
      ]
    ],
    "id": "d6d08181-0eff-4e32-8f7a-d1d0134e99c2",
    "claim": "Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 74.2%, 0.3 points less than the accuracy of the FINE-TUNED-LM model.",
    "label": "refutes",
    "table_id": "d4c8c3ca-899e-4e8c-8efe-81b447163aa5"
  },
  {
    "paper": "On the difficulty of a distributional semantics of spoken language",
    "paper_id": "1803.08869v2",
    "table_caption": "Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
    "table_column_names": [
      "[EMPTY]",
      "Recall@10 (%)",
      "Median rank",
      "RSAimage"
    ],
    "table_content_values": [
      [
        "VGS",
        "27",
        "6",
        "0.4"
      ],
      [
        "SegMatch",
        "[BOLD] 10",
        "[BOLD] 37",
        "[BOLD] 0.5"
      ],
      [
        "Audio2vec-U",
        "5",
        "105",
        "0.0"
      ],
      [
        "Audio2vec-C",
        "2",
        "647",
        "0.0"
      ],
      [
        "Mean MFCC",
        "1",
        "1,414",
        "0.0"
      ],
      [
        "Chance",
        "0",
        "3,955",
        "0.0"
      ]
    ],
    "id": "4a849ae1-eaa2-49ab-b46d-23bba1169582",
    "claim": "It does not come close to VGS on paraphrase retrieval, and it does not correlate with the visual modality better.",
    "label": "refutes",
    "table_id": "087b26ab-9679-4d8d-b96c-57140c1a8b7b"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "9cd68def-c73f-452d-add9-53415403de26",
    "claim": "ACER and PPO do not obtain high performance in inform F1 and match rate.",
    "label": "refutes",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "bead6cbe-98f0-4891-a5fc-d3de82369621",
    "claim": "In general, the performance of the model does not drop substantially as we remove more dense connections.",
    "label": "refutes",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "Encoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Linear Combination",
        "23.7",
        "53.2"
      ],
      [
        "-Global Node",
        "24.2",
        "54.6"
      ],
      [
        "-Direction Aggregation",
        "24.6",
        "54.6"
      ],
      [
        "-Graph Attention",
        "24.9",
        "54.7"
      ],
      [
        "-Global Node&Linear Combination",
        "22.9",
        "52.4"
      ],
      [
        "Decoder Modules",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "-Coverage Mechanism",
        "23.8",
        "53.0"
      ]
    ],
    "id": "4c507350-fae9-4a57-8c67-cfddd0d800b6",
    "claim": "After removing the graph attention module, our model gives 22.9 BLEU points.",
    "label": "refutes",
    "table_id": "1f2195b6-bddb-4be8-aeba-6d471c1e537a"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section 3).",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] Part",
      "[BOLD] MRs",
      "[BOLD] Refs",
      "[BOLD] SER(%)"
    ],
    "table_content_values": [
      [
        "Original",
        "Train",
        "4,862",
        "42,061",
        "17.69"
      ],
      [
        "Original",
        "Dev",
        "547",
        "4,672",
        "11.42"
      ],
      [
        "Original",
        "Test",
        "630",
        "4,693",
        "11.49"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Train",
        "8,362",
        "33,525",
        "(0.00)"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Dev",
        "1,132",
        "4,299",
        "(0.00)"
      ],
      [
        "[0.5pt/2pt] Cleaned",
        "Test",
        "1,358",
        "4,693",
        "(0.00)"
      ]
    ],
    "id": "00cd7c4e-19f7-4e98-876a-c7c38277a86d",
    "claim": "On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes.",
    "label": "supports",
    "table_id": "82c0eea1-ccb0-4495-a12a-2ecf9bed5f45"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "Logistic Regression",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Sentiment – MPQA",
        "64.2",
        "39.1",
        "0.499"
      ],
      [
        "Sentiment – NRC",
        "63.9",
        "42.2",
        "0.599"
      ],
      [
        "Sentiment – V&B",
        "68.9",
        "60.0",
        "0.696"
      ],
      [
        "Sentiment – VADER",
        "66.0",
        "54.2",
        "0.654"
      ],
      [
        "Sentiment – Stanford",
        "68.0",
        "55.6",
        "0.696"
      ],
      [
        "Complaint Specific (all)",
        "65.7",
        "55.2",
        "0.634"
      ],
      [
        "Request",
        "64.2",
        "39.1",
        "0.583"
      ],
      [
        "Intensifiers",
        "64.5",
        "47.3",
        "0.639"
      ],
      [
        "Downgraders",
        "65.4",
        "49.8",
        "0.615"
      ],
      [
        "Temporal References",
        "64.2",
        "43.7",
        "0.535"
      ],
      [
        "Pronoun Types",
        "64.1",
        "39.1",
        "0.545"
      ],
      [
        "POS Bigrams",
        "72.2",
        "66.8",
        "0.756"
      ],
      [
        "LIWC",
        "71.6",
        "65.8",
        "0.784"
      ],
      [
        "Word2Vec Clusters",
        "67.7",
        "58.3",
        "0.738"
      ],
      [
        "Bag-of-Words",
        "79.8",
        "77.5",
        "0.866"
      ],
      [
        "All Features",
        "[BOLD] 80.5",
        "[BOLD] 78.0",
        "[BOLD] 0.873"
      ],
      [
        "Neural Networks",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "MLP",
        "78.3",
        "76.2",
        "0.845"
      ],
      [
        "LSTM",
        "80.2",
        "77.0",
        "0.864"
      ]
    ],
    "id": "60340fc9-b2cd-46fc-b451-0981d1000f34",
    "claim": "The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics do not perform as well as the part of speech tags.",
    "label": "refutes",
    "table_id": "caac7a4b-bc0e-4b38-bc24-9d0ad39f2fa7"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "1ff499d0-7570-4df6-9550-0be662ce31b5",
    "claim": "In fact, DocSub had worse results in precision when using both Europarl and Ted Talks corpora in English, where DF reached best values of precision and f-measure.",
    "label": "refutes",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "4d93d55b-d069-4e2d-b720-69b1ff950af3",
    "claim": "However, our model generates shorter sentences than human arguments, with about 15 words per sentence compared to 22 words per sentence for human arguments.",
    "label": "refutes",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] External",
      "B"
    ],
    "table_content_values": [
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "-",
        "22.0"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "-",
        "23.3"
      ],
      [
        "GCNSEQ (Damonte and Cohen,  2019 )",
        "-",
        "24.4"
      ],
      [
        "DCGCN(single)",
        "-",
        "25.9"
      ],
      [
        "DCGCN(ensemble)",
        "-",
        "[BOLD] 28.2"
      ],
      [
        "TSP (Song et al.,  2016 )",
        "ALL",
        "22.4"
      ],
      [
        "PBMT (Pourdamghani et al.,  2016 )",
        "ALL",
        "26.9"
      ],
      [
        "Tree2Str (Flanigan et al.,  2016 )",
        "ALL",
        "23.0"
      ],
      [
        "SNRG (Song et al.,  2017 )",
        "ALL",
        "25.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "0.2M",
        "27.4"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "0.2M",
        "28.2"
      ],
      [
        "DCGCN(single)",
        "0.1M",
        "29.0"
      ],
      [
        "DCGCN(single)",
        "0.2M",
        "[BOLD] 31.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "2M",
        "32.3"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "2M",
        "33.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "20M",
        "33.8"
      ],
      [
        "DCGCN(single)",
        "0.3M",
        "33.2"
      ],
      [
        "DCGCN(ensemble)",
        "0.3M",
        "[BOLD] 35.3"
      ]
    ],
    "id": "136c3899-0818-4fac-a86a-9914176d9a8e",
    "claim": "These results show that our model is not as effective in terms of using automatically generated AMR graphs.",
    "label": "refutes",
    "table_id": "f7b025d4-ffc5-4764-9949-312c3463da35"
  },
  {
    "paper": "Assessing Gender Bias in Machine Translation – A Case Study with Google Translate",
    "paper_id": "1809.02208v4",
    "table_caption": "Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
    "table_column_names": [
      "Category",
      "Female (%)",
      "Male (%)",
      "Neutral (%)"
    ],
    "table_content_values": [
      [
        "Office and administrative support",
        "11.015",
        "58.812",
        "16.954"
      ],
      [
        "Architecture and engineering",
        "2.299",
        "72.701",
        "10.92"
      ],
      [
        "Farming, fishing, and forestry",
        "12.179",
        "62.179",
        "14.744"
      ],
      [
        "Management",
        "11.232",
        "66.667",
        "12.681"
      ],
      [
        "Community and social service",
        "20.238",
        "62.5",
        "10.119"
      ],
      [
        "Healthcare support",
        "25.0",
        "43.75",
        "17.188"
      ],
      [
        "Sales and related",
        "8.929",
        "62.202",
        "16.964"
      ],
      [
        "Installation, maintenance, and repair",
        "5.22",
        "58.333",
        "17.125"
      ],
      [
        "Transportation and material moving",
        "8.81",
        "62.976",
        "17.5"
      ],
      [
        "Legal",
        "11.905",
        "72.619",
        "10.714"
      ],
      [
        "Business and financial operations",
        "7.065",
        "67.935",
        "15.58"
      ],
      [
        "Life, physical, and social science",
        "5.882",
        "73.284",
        "10.049"
      ],
      [
        "Arts, design, entertainment, sports, and media",
        "10.36",
        "67.342",
        "11.486"
      ],
      [
        "Education, training, and library",
        "23.485",
        "53.03",
        "9.091"
      ],
      [
        "Building and grounds cleaning and maintenance",
        "12.5",
        "68.333",
        "11.667"
      ],
      [
        "Personal care and service",
        "18.939",
        "49.747",
        "18.434"
      ],
      [
        "Healthcare practitioners and technical",
        "22.674",
        "51.744",
        "15.116"
      ],
      [
        "Production",
        "14.331",
        "51.199",
        "18.245"
      ],
      [
        "Computer and mathematical",
        "4.167",
        "66.146",
        "14.062"
      ],
      [
        "Construction and extraction",
        "8.578",
        "61.887",
        "17.525"
      ],
      [
        "Protective service",
        "8.631",
        "65.179",
        "12.5"
      ],
      [
        "Food preparation and serving related",
        "21.078",
        "58.333",
        "17.647"
      ],
      [
        "Total",
        "11.76",
        "58.93",
        "15.939"
      ]
    ],
    "id": "4a0cb1cb-cb53-4f2f-a292-d8f09739ae8f",
    "claim": "Furthermore, this bias is seemingly not aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics.",
    "label": "refutes",
    "table_id": "2e483d7f-201a-4e26-b179-5216c375183e"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "[BOLD] 71.2"
      ]
    ],
    "id": "f240b702-2fe4-4303-8584-e97282356a54",
    "claim": "Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score.",
    "label": "supports",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 5: Performance breakdown of the PRKGC+NS model. Derivation Precision denotes ROUGE-L F1 of generated NLDs.",
    "table_column_names": [
      "# gold NLD steps",
      "Answer Prec.",
      "Derivation Prec."
    ],
    "table_content_values": [
      [
        "1",
        "79.2",
        "38.4"
      ],
      [
        "2",
        "64.4",
        "48.6"
      ],
      [
        "3",
        "62.3",
        "41.3"
      ]
    ],
    "id": "0ea80dd7-1266-4389-a316-5dea81a6c8e7",
    "claim": "As shown in Table 5, as the required derivation step increases, the PRKGC+NS model suffers from predicting answer entities and generating correct NLDs.",
    "label": "supports",
    "table_id": "352681a1-edc8-4f6b-8f54-7dfb99336e21"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1038",
        "0.0170",
        "0.0490",
        "0.0641",
        "0.0641",
        "0.0613",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1282",
        "0.0291",
        "0.0410",
        "0.0270",
        "0.0270",
        "0.1154",
        "0.0661"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.6185",
        "0.3744",
        "0.4144",
        "0.4394",
        "0.4394",
        "[BOLD] 0.7553",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.6308",
        "0.4124",
        "0.4404",
        "0.4515",
        "0.4945",
        "[BOLD] 0.8609",
        "0.5295"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "[BOLD] 0.0021",
        "0.0004",
        "0.0011",
        "0.0014",
        "0.0014",
        "0.0013",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0011",
        "0.0008",
        "0.0011",
        "0.0008",
        "0.0008",
        "[BOLD] 0.0030",
        "0.0018"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0012",
        "0.0008",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0016",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0003",
        "0.0009",
        "0.0009",
        "0.0010",
        "0.0010",
        "[BOLD] 0.0017",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "[BOLD] 0.0041",
        "0.0007",
        "0.0021",
        "0.0027",
        "0.0027",
        "0.0026",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0022",
        "0.0016",
        "0.0022",
        "0.0015",
        "0.0015",
        "[BOLD] 0.0058",
        "0.0036"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0024",
        "0.0016",
        "0.0018",
        "0.0019",
        "0.0019",
        "[BOLD] 0.0031",
        "0.0023"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0005",
        "0.0018",
        "0.0018",
        "0.0020",
        "0.0021",
        "[BOLD] 0.0034",
        "0.0022"
      ]
    ],
    "id": "957b738e-152b-46ad-b45e-b4e422ebe50c",
    "claim": "As filtering out multiple hypernyms might remove also correct relations, the recall values for all corpora are very low.",
    "label": "supports",
    "table_id": "7ff90dc3-0887-4d7b-b7bf-bb5149801b4e"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "b63fe5ff-755a-4646-b243-780c4301ed03",
    "claim": "Our model achieves higher recall@0.2 and better area under the ROC curve.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "[BOLD] 71.2"
      ]
    ],
    "id": "2dd3dec1-0bbd-478e-9bd1-8db58b16cf2e",
    "claim": "coreference is thus a very challenging task with low precision and recall over the entire system",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "052c61e4-3626-4d15-b461-16728483b42f",
    "claim": "Table 4 shows that GDPL has the largest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves less like the human.",
    "label": "refutes",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 7: Results of the human evaluation of the responses produced by our model. A response is acceptable if it is either good or great. Note: Numbers may not add up to 100% due to rounding.",
    "table_column_names": [
      "[BOLD] Whitelist",
      "[BOLD] Great",
      "[BOLD] Good",
      "[BOLD] Bad",
      "[BOLD] Accept"
    ],
    "table_content_values": [
      [
        "Freq. 1K",
        "54%",
        "26%",
        "20%",
        "80%"
      ],
      [
        "Cluster. 1K",
        "55%",
        "21%",
        "23%",
        "77%"
      ],
      [
        "Freq. 10K",
        "56%",
        "24%",
        "21%",
        "80%"
      ],
      [
        "Cluster. 10K",
        "57%",
        "23%",
        "20%",
        "80%"
      ],
      [
        "Real response",
        "60%",
        "24%",
        "16%",
        "84%"
      ]
    ],
    "id": "94b2149c-0f67-46ca-823e-e03b57610d66",
    "claim": "Interestingly, the size and type of whitelist seem to have little effect on performance, indicating that all the whitelists contain responses appropriate to a variety of conversational contexts.",
    "label": "supports",
    "table_id": "28834fcb-affd-4e2d-9c79-c4298b5936bd"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "9eb45685-012a-4854-aa9a-db7991101942",
    "claim": "[CONTINUE] It is perceptible that GDPL has better performance than GDPL-sess on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL also outperforms GDPL-discr",
    "label": "supports",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "9324694a-7af5-4508-a05b-580beb45c78c",
    "claim": "Our vector representation is the state of the art, given a sufficient amount of training time.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "08d031b1-0289-4e40-ad4c-c1b6fdc4f9f3",
    "claim": "Third, the learned reward functions based on ROUGE scores worked well in most cases, especially in a direct regression model with CNN-RNN encoder.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "f7b605d0-750b-4154-88bb-3876be299a58",
    "claim": "More than 1000 participants are asked to evaluate 10 random dialog sessions generated by each model.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "dec09923-481d-4a17-8163-2541473a06cd",
    "claim": ", For Matching Fail and Success, the negative score in other rows implies that the two partitions cannot obtain any reward if the corresponding metric is not satisfied by all sessions in the partition, showing that satisfying Matching Fail, Matching Success, and Success are the most important, followed by Informativeness.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "99af5f47-1b08-4a45-8235-734b854449ad",
    "claim": "Our single model DCGCN(single) does not outperform all the single models, as it only achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively.",
    "label": "refutes",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 2: Test error (ERR) on document classification task. “#Params”: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.",
    "table_column_names": [
      "Model",
      "Model",
      "#Params",
      "AmaPolar ERR",
      "AmaPolar Time",
      "Yahoo ERR",
      "Yahoo Time",
      "AmaFull ERR",
      "AmaFull Time",
      "YelpPolar ERR",
      "YelpPolar Time"
    ],
    "table_content_values": [
      [
        "Zhang et al. ( 2015 )",
        "Zhang et al. ( 2015 )",
        "-",
        "6.10",
        "-",
        "29.16",
        "-",
        "40.57",
        "-",
        "5.26",
        "-"
      ],
      [
        "This",
        "LSTM",
        "227K",
        "[BOLD] 4.37",
        "0.947",
        "[BOLD] 24.62",
        "1.332",
        "37.22",
        "1.003",
        "3.58",
        "1.362"
      ],
      [
        "This",
        "GRU",
        "176K",
        "4.39",
        "0.948",
        "24.68",
        "1.242",
        "[BOLD] 37.20",
        "0.982",
        "[BOLD] 3.47",
        "1.230"
      ],
      [
        "This",
        "ATR",
        "74K",
        "4.78",
        "0.867",
        "25.33",
        "1.117",
        "38.54",
        "0.836",
        "4.00",
        "1.124"
      ],
      [
        "Work",
        "SRU",
        "194K",
        "4.95",
        "0.919",
        "24.78",
        "1.394",
        "38.23",
        "0.907",
        "3.99",
        "1.310"
      ],
      [
        "[EMPTY]",
        "LRN",
        "151K",
        "4.98",
        "[BOLD] 0.731",
        "25.07",
        "[BOLD] 1.038",
        "38.42",
        "[BOLD] 0.788",
        "3.98",
        "[BOLD] 1.022"
      ]
    ],
    "id": "3c4ed041-3a0d-436a-a068-dccdb94ff06c",
    "claim": "[CONTINUE] LRN accelerates the training over LSTM and SRU by about 20%,",
    "label": "supports",
    "table_id": "68b50739-9265-4c71-b076-3e790e4552ed"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "384e6200-e560-4998-ab9c-e93a8a58f751",
    "claim": "for example, DAMD with full supervision achieves the best performance (Combined Score), showing the importance of action supervision.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "b7f5e489-e99b-438c-95ff-17378aa3bb9f",
    "claim": "For other attributes such as sentiment distribution and sentiment reliability, the F1 metric based on positive sentiment is comparatively low, because instances of neutral sentiment are simply ignored in calculating the F1 score.",
    "label": "not enough info",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Zero-Shot Grounding of Objects from Natural Language Queries",
    "paper_id": "1908.07129v1",
    "table_caption": "Table 3: Category-wise performance with the default split of Flickr30k Entities.",
    "table_column_names": [
      "Method",
      "Overall",
      "people",
      "clothing",
      "bodyparts",
      "animals",
      "vehicles",
      "instruments",
      "scene",
      "other"
    ],
    "table_content_values": [
      [
        "QRC - VGG(det)",
        "60.21",
        "75.08",
        "55.9",
        "20.27",
        "73.36",
        "68.95",
        "45.68",
        "65.27",
        "38.8"
      ],
      [
        "CITE - VGG(det)",
        "61.89",
        "[BOLD] 75.95",
        "58.50",
        "30.78",
        "[BOLD] 77.03",
        "[BOLD] 79.25",
        "48.15",
        "58.78",
        "43.24"
      ],
      [
        "ZSGNet - VGG (cls)",
        "60.12",
        "72.52",
        "60.57",
        "38.51",
        "63.61",
        "64.47",
        "49.59",
        "64.66",
        "41.09"
      ],
      [
        "ZSGNet - Res50 (cls)",
        "[BOLD] 63.39",
        "73.87",
        "[BOLD] 66.18",
        "[BOLD] 45.27",
        "73.79",
        "71.38",
        "[BOLD] 58.54",
        "[BOLD] 66.49",
        "[BOLD] 45.53"
      ]
    ],
    "id": "1b72f46f-433f-431f-a9ed-6475f5455096",
    "claim": "However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet does not show much better performance.",
    "label": "refutes",
    "table_id": "347fd2b9-f84e-4288-a2e3-b793a47f6266"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "59a37071-0e16-4c45-a4d1-dbd161893407",
    "claim": "our extractive summarizer trained with reinforcement learning is rated higher by humans.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.",
    "table_column_names": [
      "[EMPTY]",
      "dev perp ↓",
      "dev acc ↑",
      "dev wer ↓",
      "test perp ↓",
      "test acc ↑",
      "test wer ↓"
    ],
    "table_content_values": [
      [
        "Spanish-only-LM",
        "329.68",
        "26.6",
        "30.47",
        "322.26",
        "25.1",
        "29.62"
      ],
      [
        "English-only-LM",
        "320.92",
        "29.3",
        "32.02",
        "314.04",
        "30.3",
        "32.51"
      ],
      [
        "All:CS-last-LM",
        "76.64",
        "47.8",
        "14.56",
        "76.97",
        "49.2",
        "14.13"
      ],
      [
        "All:Shuffled-LM",
        "68.00",
        "51.8",
        "13.64",
        "68.72",
        "51.4",
        "13.89"
      ],
      [
        "CS-only-LM",
        "43.20",
        "60.7",
        "12.60",
        "43.42",
        "57.9",
        "12.18"
      ],
      [
        "CS-only+vocab-LM",
        "45.61",
        "61.0",
        "12.56",
        "45.79",
        "58.8",
        "12.49"
      ],
      [
        "Fine-Tuned-LM",
        "39.76",
        "66.9",
        "10.71",
        "40.11",
        "65.4",
        "10.17"
      ],
      [
        "CS-only-disc",
        "–",
        "72.0",
        "6.35",
        "–",
        "70.5",
        "6.70"
      ],
      [
        "Fine-Tuned-disc",
        "–",
        "[BOLD] 74.2",
        "[BOLD] 5.85",
        "–",
        "[BOLD] 75.5",
        "[BOLD] 5.59"
      ]
    ],
    "id": "47157bc0-08a1-4857-952c-75b652a9ec42",
    "claim": "Similarly, when using discriminative trainthe FINE-TUNED-DISCRIMINATIVE model ing, outperforms the CS-ONLY-DISCRIMINATIVE model.",
    "label": "supports",
    "table_id": "d4c8c3ca-899e-4e8c-8efe-81b447163aa5"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.",
    "table_column_names": [
      "[ITALIC] k",
      "Ar",
      "Es",
      "Fr",
      "Ru",
      "Zh",
      "En"
    ],
    "table_content_values": [
      [
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy"
      ],
      [
        "0",
        "88.0",
        "87.9",
        "87.9",
        "87.8",
        "87.7",
        "87.4"
      ],
      [
        "1",
        "92.4",
        "91.9",
        "92.1",
        "92.1",
        "91.5",
        "89.4"
      ],
      [
        "2",
        "91.9",
        "91.8",
        "91.8",
        "91.8",
        "91.3",
        "88.3"
      ],
      [
        "3",
        "92.0",
        "92.3",
        "92.1",
        "91.6",
        "91.2",
        "87.9"
      ],
      [
        "4",
        "92.1",
        "92.4",
        "92.5",
        "92.0",
        "90.5",
        "86.9"
      ],
      [
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy"
      ],
      [
        "0",
        "81.9",
        "81.9",
        "81.8",
        "81.8",
        "81.8",
        "81.2"
      ],
      [
        "1",
        "87.9",
        "87.7",
        "87.8",
        "87.9",
        "87.7",
        "84.5"
      ],
      [
        "2",
        "87.4",
        "87.5",
        "87.4",
        "87.3",
        "87.2",
        "83.2"
      ],
      [
        "3",
        "87.8",
        "87.9",
        "87.9",
        "87.3",
        "87.3",
        "82.9"
      ],
      [
        "4",
        "88.3",
        "88.6",
        "88.4",
        "88.1",
        "87.7",
        "82.1"
      ],
      [
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU"
      ],
      [
        "[EMPTY]",
        "32.7",
        "49.1",
        "38.5",
        "34.2",
        "32.1",
        "96.6"
      ]
    ],
    "id": "ff3845c0-9328-4bc8-a651-072c91290d64",
    "claim": "[CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 boost the performance to around 87-88%, [CONTINUE] which is far above the UnsupEmb and MFT baselines.",
    "label": "supports",
    "table_id": "430d822d-f0b9-4d4b-b9a6-b995a9e11686"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "de6493c8-315b-467e-875c-32f51426215c",
    "claim": "the mean KL divergence decreases from 2.098 to 0.238 as we apply more model components to the user simulator, where DP-MBCM and GP-MBCM model the human dialog policy using the LSTM-DQN framework and ACER and PPO model the human dialog policy using the Actor-Critic framework, ALDM and GDPL model the human dialog policy using the Actor-Critic framework,",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "54b945d8-03c2-4cd1-8058-4bc1bcb9d223",
    "claim": "[CONTINUE] It is perceptible that GDPL-sess has better performance than GDPL on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL-discr also outperforms GDPL.",
    "label": "refutes",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "ec04f4f3-b4ce-453b-8d8b-55d0399d1bcf",
    "claim": "For example, the greedy agent says the magic words like “I want to book an experience” at the beginning of the conversation.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "2ca8cd4b-2fd2-4e9b-be58-dc16c5750fc9",
    "claim": "As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees, but the throughput of the linear dataset increases more significantly when the batch size increases from 1 to 25.",
    "label": "refutes",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "85489224-2e94-411e-b51c-ddeefb943583",
    "claim": "we also did try VADER (NLTK implementation), but we ended up with low performance since VADER is not trained for Spanish language and it is implemented for social media",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "30e73d0d-6f33-471a-87ca-8d240db19162",
    "claim": "Specifically, BERT+MLP+Pref does not significantly outperform (p < 0.05) all the other models that do not use BERT+MLP.",
    "label": "refutes",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation",
    "paper_id": "1907.12894v1",
    "table_caption": "Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
    "table_column_names": [
      "[EMPTY]",
      "DUC’01 <italic>R</italic>1",
      "DUC’01 <italic>R</italic>2",
      "DUC’02 <italic>R</italic>1",
      "DUC’02 <italic>R</italic>2",
      "DUC’04 <italic>R</italic>1",
      "DUC’04 <italic>R</italic>2"
    ],
    "table_content_values": [
      [
        "ICSI",
        "33.31",
        "7.33",
        "35.04",
        "8.51",
        "37.31",
        "9.36"
      ],
      [
        "PriorSum",
        "35.98",
        "7.89",
        "36.63",
        "8.97",
        "38.91",
        "10.07"
      ],
      [
        "TCSum",
        "<bold>36.45</bold>",
        "7.66",
        "36.90",
        "8.61",
        "38.27",
        "9.66"
      ],
      [
        "TCSum−",
        "33.45",
        "6.07",
        "34.02",
        "7.39",
        "35.66",
        "8.66"
      ],
      [
        "SRSum",
        "36.04",
        "8.44",
        "<bold>38.93</bold>",
        "<bold>10.29</bold>",
        "39.29",
        "10.70"
      ],
      [
        "DeepTD",
        "28.74",
        "5.95",
        "31.63",
        "7.09",
        "33.57",
        "7.96"
      ],
      [
        "REAPER",
        "32.43",
        "6.84",
        "35.03",
        "8.11",
        "37.22",
        "8.64"
      ],
      [
        "RELIS",
        "34.73",
        "<bold>8.66</bold>",
        "37.11",
        "9.12",
        "<bold>39.34</bold>",
        "<bold>10.73</bold>"
      ]
    ],
    "id": "29c9462b-ac10-4436-83e3-afab19d30849",
    "claim": "[CONTINUE] RELIS significantly outperforms the other RL-based systems.",
    "label": "supports",
    "table_id": "d462f28f-7f86-4229-a2f3-71f92c29bc51"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 3: Test BLEU for es-en adaptive training. EWC reduces forgetting compared to other fine-tuning methods, while offering the greatest improvement on the new domain.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Training scheme",
      "[BOLD] Health",
      "[BOLD] Bio"
    ],
    "table_content_values": [
      [
        "1",
        "Health",
        "[BOLD] 35.9",
        "33.1"
      ],
      [
        "2",
        "Bio",
        "29.6",
        "36.1"
      ],
      [
        "3",
        "Health and Bio",
        "35.8",
        "37.2"
      ],
      [
        "4",
        "1 then Bio, No-reg",
        "30.3",
        "36.6"
      ],
      [
        "5",
        "1 then Bio, L2",
        "35.1",
        "37.3"
      ],
      [
        "6",
        "1 then Bio, EWC",
        "35.2",
        "[BOLD] 37.8"
      ]
    ],
    "id": "e5e0d1cc-b6b3-4ce5-bd6b-3a00628c32ba",
    "claim": "We find EWC outperforms the L2 approach",
    "label": "supports",
    "table_id": "8f01f974-9c61-443a-9772-ef3b10d66d43"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
    "table_column_names": [
      "System",
      "All P",
      "All R",
      "All F1",
      "In  [ITALIC] E+ P",
      "In  [ITALIC] E+ R",
      "In  [ITALIC] E+ F1"
    ],
    "table_content_values": [
      [
        "Name matching",
        "15.03",
        "15.03",
        "15.03",
        "29.13",
        "29.13",
        "29.13"
      ],
      [
        "MIL (model 1)",
        "35.87",
        "35.87",
        "35.87 ±0.72",
        "69.38",
        "69.38",
        "69.38 ±1.29"
      ],
      [
        "MIL-ND (model 2)",
        "37.42",
        "[BOLD] 37.42",
        "37.42 ±0.35",
        "72.50",
        "[BOLD] 72.50",
        "[BOLD] 72.50 ±0.68"
      ],
      [
        "[ITALIC] τMIL-ND (model 2)",
        "[BOLD] 38.91",
        "36.73",
        "[BOLD] 37.78 ±0.26",
        "[BOLD] 73.19",
        "71.15",
        "72.16 ±0.48"
      ],
      [
        "Supervised learning",
        "42.90",
        "42.90",
        "42.90 ±0.59",
        "83.12",
        "83.12",
        "83.12 ±1.15"
      ]
    ],
    "id": "d13b3ebf-01ff-42c0-9b3e-999d9d28f9cd",
    "claim": "[CONTINUE] MIL-ND achieves higher precision, recall, and F1 than MIL, [CONTINUE] Using its confidence at test time (τ MIL-ND, 'All' setting) was also beneficial in terms of precision and F1 (it cannot possibly increase recall).",
    "label": "supports",
    "table_id": "ce21ef70-1b61-4a48-9c97-69426861922c"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "da02f66d-8a1d-4ea0-aea6-e404a1c7037a",
    "claim": "PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007).",
    "label": "supports",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "Low-supervision urgency detection and transfer in short crisis messages",
    "paper_id": "1907.06745v1",
    "table_caption": "TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
    "table_column_names": [
      "System",
      "Accuracy",
      "Precision",
      "Recall",
      "F-Measure"
    ],
    "table_content_values": [
      [
        "Local",
        "63.97%",
        "64.27%",
        "64.50%",
        "63.93%"
      ],
      [
        "Manual",
        "64.25%",
        "[BOLD] 70.84%∗∗",
        "48.50%",
        "57.11%"
      ],
      [
        "Wiki",
        "67.25%",
        "66.51%",
        "69.50%",
        "67.76%"
      ],
      [
        "Local-Manual",
        "65.75%",
        "67.96%",
        "59.50%",
        "62.96%"
      ],
      [
        "Wiki-Local",
        "67.40%",
        "65.54%",
        "68.50%",
        "66.80%"
      ],
      [
        "Wiki-Manual",
        "67.75%",
        "70.38%",
        "63.00%",
        "65.79%"
      ],
      [
        "[ITALIC] Our Approach",
        "[BOLD] 69.25%∗∗∗",
        "68.76%",
        "[BOLD] 70.50%∗∗",
        "[BOLD] 69.44%∗∗∗"
      ]
    ],
    "id": "cd32feb8-8dd5-43fa-8568-60aaaffe74d8",
    "claim": "Manual features reduce recall, but do not help the system to improve accuracy and precision.",
    "label": "refutes",
    "table_id": "678b9f33-65b1-4b0a-b4f6-96de47883169"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "[BOLD] 71.2"
      ]
    ],
    "id": "6a9321bb-e9c3-4a8c-9109-fc7268df8508",
    "claim": "The best results are shown in bold.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "5f1f4b6d-1767-41f9-9314-c9dcd0205077",
    "claim": "The highest values of precision are achieved by DSim model, and the highest recalls are obtained by HClust and Patt models.",
    "label": "refutes",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "[BOLD] Model",
      "R",
      "MUC P",
      "[ITALIC] F1",
      "R",
      "B3 P",
      "[ITALIC] F1",
      "R",
      "CEAF- [ITALIC] e P",
      "[ITALIC] F1",
      "CoNLL  [ITALIC] F1"
    ],
    "table_content_values": [
      [
        "[BOLD] Baselines",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen ( 2015a )",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. ( 2018 )",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "[BOLD] Model Variants",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "[BOLD] 79.5"
      ]
    ],
    "id": "d36ed967-4ea2-4de7-809e-04b35275ba96",
    "claim": "This suggests that lemma features enhance (cross-document) coreference performance more than simple cluster features.",
    "label": "not enough info",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "a5e07487-b4cc-41c2-bb42-5794acaaf94c",
    "claim": "the relation identification component yields better performance compared to Rank+ExATT.",
    "label": "not enough info",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 6: Results of the Human Rating on CWC.",
    "table_column_names": [
      "[EMPTY]",
      "Ours Better(%)",
      "No Prefer(%)",
      "Ours Worse(%)"
    ],
    "table_content_values": [
      [
        "Retrieval-Stgy ",
        "[BOLD] 62",
        "22",
        "16"
      ],
      [
        "PMI ",
        "[BOLD] 54",
        "32",
        "14"
      ],
      [
        "Neural ",
        "[BOLD] 60",
        "22",
        "18"
      ],
      [
        "Kernel ",
        "[BOLD] 62",
        "26",
        "12"
      ]
    ],
    "id": "736c33a1-48f2-4f67-939b-397fd82f51f2",
    "claim": "Our agent outperforms the comparison agents with a large margin.",
    "label": "supports",
    "table_id": "2f4a12c2-27e3-4c98-984e-2b17d287642e"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "40429ce5-1c82-41e5-8724-5f22da89362c",
    "claim": "GDPL-sess and GDPL-discr mark both pretraining strategies, while GDPL marks the ensemble model.",
    "label": "not enough info",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "910499d2-85dd-428e-a7f5-268b24bfa673",
    "claim": "[CONTINUE] Dual2seq is not significantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much smaller under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU).",
    "label": "refutes",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "<bold>Baselines</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Cluster+Lemma",
        "76.5",
        "79.9",
        "78.1",
        "71.7",
        "85",
        "77.8",
        "75.5",
        "71.7",
        "73.6",
        "76.5"
      ],
      [
        "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
        "71",
        "75",
        "73",
        "71",
        "78",
        "74",
        "-",
        "-",
        "64",
        "73"
      ],
      [
        "KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>)",
        "67",
        "71",
        "69",
        "71",
        "67",
        "69",
        "71",
        "67",
        "69",
        "69"
      ],
      [
        "Cluster+KCP",
        "68.4",
        "79.3",
        "73.4",
        "67.2",
        "87.2",
        "75.9",
        "77.4",
        "66.4",
        "71.5",
        "73.6"
      ],
      [
        "<bold>Model Variants</bold>",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Disjoint",
        "75.5",
        "83.6",
        "79.4",
        "75.4",
        "86",
        "80.4",
        "80.3",
        "71.9",
        "75.9",
        "78.5"
      ],
      [
        "Joint",
        "77.6",
        "84.5",
        "80.9",
        "76.1",
        "85.1",
        "80.3",
        "81",
        "73.8",
        "77.3",
        "<bold>79.5</bold>"
      ]
    ],
    "id": "5b739ed7-b554-4969-8cb2-5d048179aeb5",
    "claim": "[CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 10.5 CoNLL F1 points on events,",
    "label": "supports",
    "table_id": "6312793c-3597-4fae-92d2-86265199d8fb"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "b8a1ebb9-8375-438d-a7a3-af682c33ac69",
    "claim": "Our proposed method outperforms GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set.",
    "label": "supports",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "7cead150-e3a2-4135-aacf-47c24848a499",
    "claim": "Without using the dense connections in the last two blocks, the score drops to 23.8.",
    "label": "supports",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "e2aba604-ba6c-4e5b-a15a-91c898f2453a",
    "claim": "It should also be noted that scores obtained by SPINE is unacceptably low on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions.",
    "label": "supports",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. “Pref%”: in how many percentage of documents a system receives the higher human rating.",
    "table_column_names": [
      "Reward",
      "R-1",
      "R-2",
      "R-L",
      "Human",
      "Pref%"
    ],
    "table_content_values": [
      [
        "R-L (original)",
        "40.9",
        "17.8",
        "38.5",
        "1.75",
        "15"
      ],
      [
        "Learned (ours)",
        "39.2",
        "17.4",
        "37.5",
        "[BOLD] 2.20",
        "[BOLD] 75"
      ]
    ],
    "id": "6ba6dcd8-17b3-4498-8a29-4345b38b7aa1",
    "claim": "[CONTINUE] It is clear from Table 5 that using the learned reward helps the RL-based system generate summaries with significantly higher human ratings.",
    "label": "supports",
    "table_id": "f5343270-2393-4ff2-85d1-a445b36215ea"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 4: Precisions on the Wikidata dataset with different choice of d.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC",
      "Time"
    ],
    "table_content_values": [
      [
        "[ITALIC] d=1",
        "0.602",
        "0.487",
        "0.403",
        "0.367",
        "4h"
      ],
      [
        "[ITALIC] d=32",
        "0.645",
        "0.501",
        "0.393",
        "0.370",
        "-"
      ],
      [
        "[ITALIC] d=16",
        "0.655",
        "0.518",
        "0.413",
        "0.413",
        "20h"
      ],
      [
        "[ITALIC] d=8",
        "0.650",
        "0.519",
        "0.422",
        "0.405",
        "8h"
      ]
    ],
    "id": "c6f16d71-f550-42fc-87b9-f2796d26bd4c",
    "claim": "at a recall of 1, the d=32 setting already achieve a precision of over 0.8 with a significant gap of 0.2 when compared to the best performance of d=8, indicating that at this recall level, the d=32 model seems to be more effective at identifying mappings.",
    "label": "not enough info",
    "table_id": "b3925d14-8042-410e-841f-73dce02fc49b"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "4827721a-f264-4ddb-8171-fc8d60473bdd",
    "claim": "As an explanation for these differences, we believe that the mixtures of different task profiles allowed participants to learn more detailed topic-dependent aspects (better than a single vector model), particularly in relation to the use of language in subject-oriented communication.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 2: Averages of rankings of the words in same-gender pairs vs. different-gender pairs for Italian and German, along with their differences. Og stands for the original embeddings, Db for the debiased embeddings, and En for English. Each row presents the averages of pairs with the respective scores in SimLex-999 (0–4, 4–7, 7–10).",
    "table_column_names": [
      "[EMPTY]",
      "Italian Same-gender",
      "Italian Diff-Gender",
      "Italian difference",
      "German Same-gender",
      "German Diff-Gender",
      "German difference"
    ],
    "table_content_values": [
      [
        "7–10",
        "Og: 4884",
        "Og: 12947",
        "Og: 8063",
        "Og: 5925",
        "Og: 33604",
        "Og: 27679"
      ],
      [
        "7–10",
        "Db: 5523",
        "Db: 7312",
        "Db: 1789",
        "Db: 7653",
        "Db: 26071",
        "Db: 18418"
      ],
      [
        "7–10",
        "En: 6978",
        "En: 2467",
        "En: -4511",
        "En: 4517",
        "En: 8666",
        "En: 4149"
      ],
      [
        "4–7",
        "Og: 10954",
        "Og: 15838",
        "Og: 4884",
        "Og: 19271",
        "Og: 27256",
        "Og: 7985"
      ],
      [
        "4–7",
        "Db: 12037",
        "Db: 12564",
        "Db: 527",
        "Db: 24845",
        "Db: 22970",
        "Db: -1875"
      ],
      [
        "4–7",
        "En: 15891",
        "En: 17782",
        "En: 1891",
        "En: 13282",
        "En: 17649",
        "En: 4367"
      ],
      [
        "0–4",
        "Og: 23314",
        "Og: 35783",
        "Og: 12469",
        "Og: 50983",
        "Og: 85263",
        "Og: 34280"
      ],
      [
        "0–4",
        "Db: 26386",
        "Db: 28067",
        "Db: 1681",
        "Db: 60603",
        "Db: 79081",
        "Db: 18478"
      ],
      [
        "0–4",
        "En: 57278",
        "En: 53053",
        "En: -4225",
        "En: 41509",
        "En: 62929",
        "En: 21420"
      ]
    ],
    "id": "ea35a87b-5630-4eb2-b60a-28894e2b6299",
    "claim": "As expected, the average ranking of samegender pairs is significantly lower than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller.",
    "label": "supports",
    "table_id": "97f25636-ac5f-4737-8cbe-3b42364a43d3"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "dc8b32c8-ec58-46a5-85b9-4a7eb85ba965",
    "claim": "[CONTINUE] Another interesting fact in Table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset, as the batch size increases.",
    "label": "supports",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
    "table_column_names": [
      "Model",
      "Training data",
      "Overall",
      "Easy",
      "Hard"
    ],
    "table_content_values": [
      [
        "BERT-large-FT",
        "B-COPA",
        "74.5 (± 0.7)",
        "74.7 (± 0.4)",
        "[BOLD] 74.4 (± 0.9)"
      ],
      [
        "BERT-large-FT",
        "B-COPA (50%)",
        "74.3 (± 2.2)",
        "76.8 (± 1.9)",
        "72.8 (± 3.1)"
      ],
      [
        "BERT-large-FT",
        "COPA",
        "[BOLD] 76.5 (± 2.7)",
        "[BOLD] 83.9 (± 4.4)",
        "71.9 (± 2.5)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA",
        "[BOLD] 89.0 (± 0.3)",
        "88.9 (± 2.1)",
        "[BOLD] 89.0 (± 0.8)"
      ],
      [
        "RoBERTa-large-FT",
        "B-COPA (50%)",
        "86.1 (± 2.2)",
        "87.4 (± 1.1)",
        "85.4 (± 2.9)"
      ],
      [
        "RoBERTa-large-FT",
        "COPA",
        "87.7 (± 0.9)",
        "[BOLD] 91.6 (± 1.1)",
        "85.3 (± 2.0)"
      ]
    ],
    "id": "a487bf03-88aa-46db-bf2b-25eba56b5a37",
    "claim": "However, training on B-COPA does not necessarily improve performance on the Hard subset, even when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%).",
    "label": "refutes",
    "table_id": "dc204964-76db-4a89-999d-02096f0710f4"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "d907f6a2-fd3e-47ad-bfba-3223f86388a4",
    "claim": "word vectors generated using our proposed word embedding method using high dimensional, sparse vectors are shown to perform well when used in analogy completion tasks.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "3a52a91a-e771-4f88-91f4-1c3baebb5af8",
    "claim": "we can find that capsule can provide more quantitative performance for our triple prediction task.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "6000ee88-8d5d-4420-8496-2a6b49fc72ae",
    "claim": "for example, DAMD + multi-action data augmentation performs much better than all the other models, suggesting that it is critical to carefully model system actions.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Orig",
      "Italian Debias",
      "German Orig",
      "German Debias"
    ],
    "table_content_values": [
      [
        "SimLex",
        "0.280",
        "[BOLD] 0.288",
        "0.343",
        "[BOLD] 0.356"
      ],
      [
        "WordSim",
        "0.548",
        "[BOLD] 0.577",
        "0.547",
        "[BOLD] 0.553"
      ]
    ],
    "id": "211e093d-d629-48fa-bdba-ab688d36cc5b",
    "claim": "In both cases, the new embeddings perform better than the original ones.",
    "label": "supports",
    "table_id": "b897f892-83c1-4817-847f-fa52a995ccab"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "0fb25186-f020-4e5a-9e5f-e3d96653c844",
    "claim": "In some cases it seems to make difference in results, e.g., Europarl in Portuguese which increased the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in an increase of f-measure from F=0.5555 in DF to F=0.6403 in TF.",
    "label": "supports",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "a4e8cf0d-5a37-4d81-b804-b773d5b80be4",
    "claim": "Under the same setting, our model does not consistently outperform graph encoders based on recurrent neural networks or gating mechanisms.",
    "label": "refutes",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Neural End-to-End Learning for Computational Argumentation Mining",
    "paper_id": "1704.06104v2",
    "table_caption": "Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.",
    "table_column_names": [
      "[EMPTY]",
      "C-F1 100%",
      "C-F1 50%",
      "R-F1 100%",
      "R-F1 50%",
      "F1 100%",
      "F1 50%"
    ],
    "table_content_values": [
      [
        "Y-3",
        "49.59",
        "65.37",
        "26.28",
        "37.00",
        "34.35",
        "47.25"
      ],
      [
        "Y-3:Y<italic>C</italic>-1",
        "54.71",
        "66.84",
        "28.44",
        "37.35",
        "37.40",
        "47.92"
      ],
      [
        "Y-3:Y<italic>R</italic>-1",
        "51.32",
        "66.49",
        "26.92",
        "37.18",
        "35.31",
        "47.69"
      ],
      [
        "Y-3:Y<italic>C</italic>-3",
        "<bold>54.58</bold>",
        "67.66",
        "<bold>30.22</bold>",
        "<bold>40.30</bold>",
        "<bold>38.90</bold>",
        "<bold>50.51</bold>"
      ],
      [
        "Y-3:Y<italic>R</italic>-3",
        "53.31",
        "66.71",
        "26.65",
        "35.86",
        "35.53",
        "46.64"
      ],
      [
        "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
        "52.95",
        "<bold>67.84</bold>",
        "27.90",
        "39.71",
        "36.54",
        "50.09"
      ],
      [
        "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
        "54.55",
        "67.60",
        "28.30",
        "38.26",
        "37.26",
        "48.86"
      ]
    ],
    "id": "209c036c-49d4-4305-82fd-7e422df199d1",
    "claim": "We find that when we train STagBL with only its main task—with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance decreases typically by a few percentage points.",
    "label": "refutes",
    "table_id": "d0039005-f056-4745-9652-8796a2c2b307"
  },
  {
    "paper": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation",
    "paper_id": "1907.12894v1",
    "table_caption": "Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
    "table_column_names": [
      "[EMPTY]",
      "DUC’01 <italic>R</italic>1",
      "DUC’01 <italic>R</italic>2",
      "DUC’02 <italic>R</italic>1",
      "DUC’02 <italic>R</italic>2",
      "DUC’04 <italic>R</italic>1",
      "DUC’04 <italic>R</italic>2"
    ],
    "table_content_values": [
      [
        "ICSI",
        "33.31",
        "7.33",
        "35.04",
        "8.51",
        "37.31",
        "9.36"
      ],
      [
        "PriorSum",
        "35.98",
        "7.89",
        "36.63",
        "8.97",
        "38.91",
        "10.07"
      ],
      [
        "TCSum",
        "<bold>36.45</bold>",
        "7.66",
        "36.90",
        "8.61",
        "38.27",
        "9.66"
      ],
      [
        "TCSum−",
        "33.45",
        "6.07",
        "34.02",
        "7.39",
        "35.66",
        "8.66"
      ],
      [
        "SRSum",
        "36.04",
        "8.44",
        "<bold>38.93</bold>",
        "<bold>10.29</bold>",
        "39.29",
        "10.70"
      ],
      [
        "DeepTD",
        "28.74",
        "5.95",
        "31.63",
        "7.09",
        "33.57",
        "7.96"
      ],
      [
        "REAPER",
        "32.43",
        "6.84",
        "35.03",
        "8.11",
        "37.22",
        "8.64"
      ],
      [
        "RELIS",
        "34.73",
        "<bold>8.66</bold>",
        "37.11",
        "9.12",
        "<bold>39.34</bold>",
        "<bold>10.73</bold>"
      ]
    ],
    "id": "b7183bab-8092-4b58-8b4c-32d184f4ece2",
    "claim": "At the same time, RELIS performs on par with neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next.",
    "label": "supports",
    "table_id": "d462f28f-7f86-4229-a2f3-71f92c29bc51"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "-Word-ATT",
        "0.648",
        "0.515",
        "0.395",
        "0.389"
      ],
      [
        "-Capsule",
        "0.635",
        "0.507",
        "0.413",
        "0.386"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ]
    ],
    "id": "290d694b-8ba9-4b59-ab2d-546e5f5cc385",
    "claim": "The single capsule can capture more useful information, while the word-level attention focuses on the entities.",
    "label": "not enough info",
    "table_id": "33a87cb6-8a23-46f2-8fee-97555075aab1"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 1: Results on belinkov2014exploring’s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Schütze (2015) on GloVe.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] Initialization",
      "[BOLD] Embedding",
      "[BOLD] Resources",
      "[BOLD] Test Acc."
    ],
    "table_content_values": [
      [
        "HPCD (full)",
        "Syntactic-SG",
        "Type",
        "WordNet, VerbNet",
        "88.7"
      ],
      [
        "LSTM-PP",
        "GloVe",
        "Type",
        "-",
        "84.3"
      ],
      [
        "LSTM-PP",
        "GloVe-retro",
        "Type",
        "WordNet",
        "84.8"
      ],
      [
        "OntoLSTM-PP",
        "GloVe-extended",
        "Token",
        "WordNet",
        "[BOLD] 89.7"
      ]
    ],
    "id": "ee82637b-643d-49e2-a8a8-f2553f993db0",
    "claim": "OntoLSTM-PP does not outperform HPCD (full), the previous best result on this dataset.",
    "label": "refutes",
    "table_id": "243a1721-f8dd-43b3-aa38-bf136c5048a9"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "acf36685-577f-4ce5-b514-630c07cd800c",
    "claim": "Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models that do not use BERT+MLP,",
    "label": "supports",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 5: Group text features associated with tweets that are complaints and not complaints. Features are sorted by Pearson correlation (r) between their each feature’s normalized frequency and the outcome. We restrict to only the top six categories for each feature type. All correlations are significant at p",
    "table_column_names": [
      "[BOLD] Complaints  [BOLD] Label",
      "[BOLD] Complaints  [BOLD] Words",
      "[BOLD] Complaints  [ITALIC] r",
      "[BOLD] Not Complaints  [BOLD] Label",
      "[BOLD] Not Complaints  [BOLD] Words",
      "[BOLD] Not Complaints  [ITALIC] r"
    ],
    "table_content_values": [
      [
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features",
        "[BOLD] LIWC Features"
      ],
      [
        "NEGATE",
        "not, no, can’t, don’t, never, nothing, doesn’t, won’t",
        ".271",
        "POSEMO",
        "thanks, love, thank, good, great, support, lol, win",
        ".185"
      ],
      [
        "RELATIV",
        "in, on, when, at, out, still, now, up, back, new",
        ".225",
        "AFFECT",
        "thanks, love, thank, good, great, support, lol",
        ".111"
      ],
      [
        "FUNCTION",
        "the, i, to, a, my, and, you, for, is, in",
        ".204",
        "SHEHE",
        "he, his, she, her, him, he’s, himself",
        ".105"
      ],
      [
        "TIME",
        "when, still, now, back, new, never, after, then, waiting",
        ".186",
        "MALE",
        "he, his, man, him, sir, he’s, son",
        ".086"
      ],
      [
        "DIFFER",
        "not, but, if, or, can’t, really, than, other, haven’t",
        ".169",
        "FEMALE",
        "she, her, girl, mom, ma, lady, mother, female, mrs",
        ".084"
      ],
      [
        "COGPROC",
        "not, but, how, if, all, why, or, any, need",
        ".132",
        "ASSENT",
        "yes, ok, awesome, okay, yeah, cool, absolutely, agree",
        ".080"
      ],
      [
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters",
        "[BOLD] Word2Vec Clusters"
      ],
      [
        "Cust. Service",
        "service, customer, contact, job, staff, assist, agent",
        ".136",
        "Gratitude",
        "thanks, thank, good, great, support, everyone, huge, proud",
        ".089"
      ],
      [
        "Order",
        "order, store, buy, free, delivery, available, package",
        ".128",
        "Family",
        "old, friend, family, mom, wife, husband, younger",
        ".063"
      ],
      [
        "Issues",
        "delayed, closed, between, outage, delay, road, accident",
        ".122",
        "Voting",
        "favorite, part, stars, model, vote, models, represent",
        ".060"
      ],
      [
        "Time Ref.",
        "been, yet, haven’t, long, happened, yesterday, took",
        ".122",
        "Contests",
        "Christmas, gift, receive, entered, giveaway, enter, cards",
        ".058"
      ],
      [
        "Tech Parts",
        "battery, laptop, screen, warranty, desktop, printer",
        ".100",
        "Pets",
        "dogs, cat, dog, pet, shepherd, fluffy, treats",
        ".054"
      ],
      [
        "Access",
        "use, using, error, password, access, automatically, reset",
        ".098",
        "Christian",
        "god, shall, heaven, spirit, lord, belongs, soul, believers",
        ".053"
      ]
    ],
    "id": "57547cfd-917f-4e7d-a554-236cf763e4a1",
    "claim": "Several groups of words are much more likely to appear in a complaint, although not used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech).",
    "label": "supports",
    "table_id": "f0f9508c-b5d0-4383-83a2-8e8715f8e9ab"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "baaa1788-387d-4417-be35-6c9a092846ab",
    "claim": "Adding the dependency weight factor with a window size of 5 improves [CONTINUE] the F1 score by 3.2% (A3−A2).",
    "label": "supports",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "69091183-93a9-443b-9f2b-248ee7ef89fc",
    "claim": "GDPL does not outperform three baselines significantly in all aspects (sign test, p-value < 0.01), including the quality compared with ACER.",
    "label": "refutes",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] External",
      "B"
    ],
    "table_content_values": [
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "-",
        "22.0"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "-",
        "23.3"
      ],
      [
        "GCNSEQ (Damonte and Cohen,  2019 )",
        "-",
        "24.4"
      ],
      [
        "DCGCN(single)",
        "-",
        "25.9"
      ],
      [
        "DCGCN(ensemble)",
        "-",
        "[BOLD] 28.2"
      ],
      [
        "TSP (Song et al.,  2016 )",
        "ALL",
        "22.4"
      ],
      [
        "PBMT (Pourdamghani et al.,  2016 )",
        "ALL",
        "26.9"
      ],
      [
        "Tree2Str (Flanigan et al.,  2016 )",
        "ALL",
        "23.0"
      ],
      [
        "SNRG (Song et al.,  2017 )",
        "ALL",
        "25.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "0.2M",
        "27.4"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "0.2M",
        "28.2"
      ],
      [
        "DCGCN(single)",
        "0.1M",
        "29.0"
      ],
      [
        "DCGCN(single)",
        "0.2M",
        "[BOLD] 31.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "2M",
        "32.3"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "2M",
        "33.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "20M",
        "33.8"
      ],
      [
        "DCGCN(single)",
        "0.3M",
        "33.2"
      ],
      [
        "DCGCN(ensemble)",
        "0.3M",
        "[BOLD] 35.3"
      ]
    ],
    "id": "f5f06ec5-0b30-4907-a0bd-d39536b0aae2",
    "claim": "These results show that our model is more effective in terms of using automatically generated AMR graphs.",
    "label": "supports",
    "table_id": "f7b025d4-ffc5-4764-9949-312c3463da35"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "a36005d4-f76f-435c-8f9b-08eab8c4e8c7",
    "claim": "the joint training of encoder and question decoder achieves 10.99 EM and 50.10 F1 for QA-SRL and better results for QA-SRL than MQAN.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Iteration=1",
        "0.531",
        "0.455",
        "0.353",
        "0.201"
      ],
      [
        "Iteration=2",
        "0.592",
        "0.498",
        "0.385",
        "0.375"
      ],
      [
        "Iteration=3",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ],
      [
        "Iteration=4",
        "0.601",
        "0.505",
        "0.422",
        "0.385"
      ],
      [
        "Iteration=5",
        "0.575",
        "0.495",
        "0.394",
        "0.376"
      ]
    ],
    "id": "97aab644-cd95-4b66-bc5e-b81e249e9808",
    "claim": "as shown in Figure 5(a), recall@100 is increasing at different recall thresholds, the best result is achieved at r=0.3, which is the average number of tags of each training sample",
    "label": "not enough info",
    "table_id": "e8d636ab-66cd-4c48-97ea-e05fd1ff69c0"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.",
    "table_column_names": [
      "[EMPTY]",
      "dev perp ↓",
      "dev acc ↑",
      "dev wer ↓",
      "test perp ↓",
      "test acc ↑",
      "test wer ↓"
    ],
    "table_content_values": [
      [
        "Spanish-only-LM",
        "329.68",
        "26.6",
        "30.47",
        "322.26",
        "25.1",
        "29.62"
      ],
      [
        "English-only-LM",
        "320.92",
        "29.3",
        "32.02",
        "314.04",
        "30.3",
        "32.51"
      ],
      [
        "All:CS-last-LM",
        "76.64",
        "47.8",
        "14.56",
        "76.97",
        "49.2",
        "14.13"
      ],
      [
        "All:Shuffled-LM",
        "68.00",
        "51.8",
        "13.64",
        "68.72",
        "51.4",
        "13.89"
      ],
      [
        "CS-only-LM",
        "43.20",
        "60.7",
        "12.60",
        "43.42",
        "57.9",
        "12.18"
      ],
      [
        "CS-only+vocab-LM",
        "45.61",
        "61.0",
        "12.56",
        "45.79",
        "58.8",
        "12.49"
      ],
      [
        "Fine-Tuned-LM",
        "39.76",
        "66.9",
        "10.71",
        "40.11",
        "65.4",
        "10.17"
      ],
      [
        "CS-only-disc",
        "–",
        "72.0",
        "6.35",
        "–",
        "70.5",
        "6.70"
      ],
      [
        "Fine-Tuned-disc",
        "–",
        "[BOLD] 74.2",
        "[BOLD] 5.85",
        "–",
        "[BOLD] 75.5",
        "[BOLD] 5.59"
      ]
    ],
    "id": "6f90e958-7eef-46a6-8b4a-bfbc7d4b391f",
    "claim": "Similarly, when using discriminative training, the CS-ONLY-DISCRIMINATIVE model outperforms the FINE-TUNED-DISCRIMINATIVE model.",
    "label": "refutes",
    "table_id": "d4c8c3ca-899e-4e8c-8efe-81b447163aa5"
  },
  {
    "paper": "What do Deep Networks Like to Read?",
    "paper_id": "1909.04547v1",
    "table_caption": "Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
    "table_column_names": [
      "[EMPTY]",
      "<bold>RNN</bold>",
      "<bold>CNN</bold>",
      "<bold>DAN</bold>"
    ],
    "table_content_values": [
      [
        "Positive",
        "+9.7",
        "+4.3",
        "+<bold>23.6</bold>"
      ],
      [
        "Negative",
        "+6.9",
        "+5.5",
        "+<bold>16.1</bold>"
      ],
      [
        "Flipped to Positive",
        "+20.2",
        "+24.9",
        "+27.4"
      ],
      [
        "Flipped to Negative",
        "+31.5",
        "+28.6",
        "+19.3"
      ]
    ],
    "id": "c12dfef6-a48b-4994-9d35-bac3557acb18",
    "claim": "We see a constant increase in sentiment value in both directions across all three models after finetuning demonstrating that the framework is able to pick up on words that are indicative of sentiment.",
    "label": "supports",
    "table_id": "fe569d5a-7fe9-4318-80c3-68d5d6108755"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "e095aa11-0827-46e0-b728-c21f8b99a728",
    "claim": "However, NSP is able to capture “false” causal information because it can match, e.g., the antecedent with but or because, which may help it show an advantage on less challenging examples.",
    "label": "not enough info",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 6: Recall@1 versus coverage for frequency and clustering whitelists.",
    "table_column_names": [
      "[BOLD] Whitelist",
      "[BOLD] R@1",
      "[BOLD] Coverage"
    ],
    "table_content_values": [
      [
        "Frequency 10K",
        "0.136",
        "45.04%"
      ],
      [
        "Clustering 10K",
        "0.164",
        "38.38%"
      ],
      [
        "Frequency 1K",
        "0.273",
        "33.38%"
      ],
      [
        "Clustering 1K",
        "0.331",
        "23.28%"
      ]
    ],
    "id": "f684361e-9ba9-42b7-b25d-ea65f81115a2",
    "claim": "While the clustering whitelists have higher recall, the frequency whitelists have higher coverage.",
    "label": "supports",
    "table_id": "49bf01f0-6222-412a-bf17-dfe43759b754"
  },
  {
    "paper": "Two Causal Principles for Improving Visual Dialog",
    "paper_id": "1911.10496v2",
    "table_caption": "Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.",
    "table_column_names": [
      "Model",
      "LF ",
      "HCIAE ",
      "CoAtt ",
      "RvA "
    ],
    "table_content_values": [
      [
        "baseline",
        "57.21",
        "56.98",
        "56.46",
        "56.74"
      ],
      [
        "+P1",
        "61.88",
        "60.12",
        "60.27",
        "61.02"
      ],
      [
        "+P2",
        "72.65",
        "71.50",
        "71.41",
        "71.44"
      ],
      [
        "+P1+P2",
        "[BOLD] 73.63",
        "71.99",
        "71.87",
        "72.88"
      ]
    ],
    "id": "719df65c-e7e1-4d81-862e-9f799a929714",
    "claim": "In general, both of our principles can improve all the models in any ablative condition (i.e., P1, P2, P1+P2).",
    "label": "supports",
    "table_id": "df3ac154-71e4-4dc9-845e-7976d7dce3ae"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "0535cc2d-61f9-49b2-954f-40fa67d32687",
    "claim": "Instead, we use different combinations of the IWE table.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "25bcab8e-9441-4c2e-beda-8c47ffe86585",
    "claim": "[CONTINUE] Supervising path attentions (the PRKGC+NS model) is indeed effective for improving the human interpretability of generated NLDs.",
    "label": "supports",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
    "table_column_names": [
      "ID LSTM-800",
      "5-fold CV 70.56",
      "Δ 0.66",
      "Single model 67.54",
      "Δ 0.78",
      "Ensemble 67.65",
      "Δ 0.30"
    ],
    "table_content_values": [
      [
        "LSTM-400",
        "70.50",
        "0.60",
        "[BOLD] 67.59",
        "0.83",
        "[BOLD] 68.00",
        "0.65"
      ],
      [
        "IN-TITLE",
        "70.11",
        "0.21",
        "[EMPTY]",
        "[EMPTY]",
        "67.52",
        "0.17"
      ],
      [
        "[BOLD] SUBMISSION",
        "69.90",
        "–",
        "66.76",
        "–",
        "67.35",
        "–"
      ],
      [
        "NO-HIGHWAY",
        "69.72",
        "−0.18",
        "66.42",
        "−0.34",
        "66.64",
        "−0.71"
      ],
      [
        "NO-OVERLAPS",
        "69.46",
        "−0.44",
        "65.07",
        "−1.69",
        "66.47",
        "−0.88"
      ],
      [
        "LSTM-400-DROPOUT",
        "69.45",
        "−0.45",
        "65.53",
        "−1.23",
        "67.28",
        "−0.07"
      ],
      [
        "NO-TRANSLATIONS",
        "69.42",
        "−0.48",
        "65.92",
        "−0.84",
        "67.23",
        "−0.12"
      ],
      [
        "NO-ELMO-FINETUNING",
        "67.71",
        "−2.19",
        "65.16",
        "−1.60",
        "65.42",
        "−1.93"
      ]
    ],
    "id": "7994ec04-0d3b-4a69-8433-deafd2d52158",
    "claim": "Apart from the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are usually associated with large discrepancies in test set performance.",
    "label": "refutes",
    "table_id": "0980bab3-642c-413e-8b15-6abd868956a8"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "8b8dabcd-08f8-434a-8cab-38912a86d4c9",
    "claim": "The relative improvement averaged over all tasks is less than 8%.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Building a Production Model for Retrieval-Based Chatbots",
    "paper_id": "1906.03209v2",
    "table_caption": "Table 6: Recall@1 versus coverage for frequency and clustering whitelists.",
    "table_column_names": [
      "[BOLD] Whitelist",
      "[BOLD] R@1",
      "[BOLD] Coverage"
    ],
    "table_content_values": [
      [
        "Frequency 10K",
        "0.136",
        "45.04%"
      ],
      [
        "Clustering 10K",
        "0.164",
        "38.38%"
      ],
      [
        "Frequency 1K",
        "0.273",
        "33.38%"
      ],
      [
        "Clustering 1K",
        "0.331",
        "23.28%"
      ]
    ],
    "id": "1cdd42db-f5b4-4e6e-989c-aa0d126f6ee8",
    "claim": "While the frequency whitelists have higher recall, the clustering whitelists have higher coverage.",
    "label": "refutes",
    "table_id": "49bf01f0-6222-412a-bf17-dfe43759b754"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
    "table_column_names": [
      "# of Heads",
      "Accuracy",
      "Val. Loss",
      "Effect"
    ],
    "table_content_values": [
      [
        "1",
        "89.44%",
        "0.2811",
        "-6.84%"
      ],
      [
        "2",
        "91.20%",
        "0.2692",
        "-5.08%"
      ],
      [
        "4",
        "93.85%",
        "0.2481",
        "-2.43%"
      ],
      [
        "8",
        "96.02%",
        "0.2257",
        "-0.26%"
      ],
      [
        "10",
        "96.28%",
        "0.2197",
        "[EMPTY]"
      ],
      [
        "16",
        "96.32%",
        "0.2190",
        "+0.04"
      ]
    ],
    "id": "8e7135c9-5245-4dc2-a2ad-75c47cd2ee70",
    "claim": "As shown in Table 6, reducing the number of attention heads severely decreases multitasking performance.",
    "label": "supports",
    "table_id": "34083185-d444-4bd0-994e-d13f5afe8e82"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 1: Results on belinkov2014exploring’s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Schütze (2015) on GloVe.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] Initialization",
      "[BOLD] Embedding",
      "[BOLD] Resources",
      "[BOLD] Test Acc."
    ],
    "table_content_values": [
      [
        "HPCD (full)",
        "Syntactic-SG",
        "Type",
        "WordNet, VerbNet",
        "88.7"
      ],
      [
        "LSTM-PP",
        "GloVe",
        "Type",
        "-",
        "84.3"
      ],
      [
        "LSTM-PP",
        "GloVe-retro",
        "Type",
        "WordNet",
        "84.8"
      ],
      [
        "OntoLSTM-PP",
        "GloVe-extended",
        "Token",
        "WordNet",
        "[BOLD] 89.7"
      ]
    ],
    "id": "3926cb33-082d-4658-b949-24978f01cc9f",
    "claim": "Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP does not outperform the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%.",
    "label": "refutes",
    "table_id": "243a1721-f8dd-43b3-aa38-bf136c5048a9"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
    "table_column_names": [
      "System",
      "MUC",
      "BCUB",
      "CEAFe",
      "AVG"
    ],
    "table_content_values": [
      [
        "ACE",
        "ACE",
        "ACE",
        "ACE",
        "ACE"
      ],
      [
        "IlliCons",
        "[BOLD] 78.17",
        "81.64",
        "[BOLD] 78.45",
        "[BOLD] 79.42"
      ],
      [
        "KnowComb",
        "77.51",
        "[BOLD] 81.97",
        "77.44",
        "78.97"
      ],
      [
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes"
      ],
      [
        "IlliCons",
        "84.10",
        "[BOLD] 78.30",
        "[BOLD] 68.74",
        "[BOLD] 77.05"
      ],
      [
        "KnowComb",
        "[BOLD] 84.33",
        "78.02",
        "67.95",
        "76.76"
      ]
    ],
    "id": "06a83460-1e39-4475-811c-5697454932b9",
    "claim": "As hard coreference problems are rare in standard coreference datasets, we do not have significant performance improvement.",
    "label": "supports",
    "table_id": "cbbb2b74-fff2-4db8-a6be-b6a395d77483"
  },
  {
    "paper": "Towards Universal Dialogue State Tracking",
    "paper_id": "1810.09587v1",
    "table_caption": "Table 1: Joint goal accuracy on DSTC2 and WOZ 2.0 test set vs. various approaches as reported in the literature.",
    "table_column_names": [
      "[BOLD] DST Models",
      "[BOLD] Joint Acc. DSTC2",
      "[BOLD] Joint Acc. WOZ 2.0"
    ],
    "table_content_values": [
      [
        "Delexicalisation-Based (DB) Model Mrkšić et al. ( 2017 )",
        "69.1",
        "70.8"
      ],
      [
        "DB Model + Semantic Dictionary Mrkšić et al. ( 2017 )",
        "72.9",
        "83.7"
      ],
      [
        "Scalable Multi-domain DST Rastogi et al. ( 2017 )",
        "70.3",
        "-"
      ],
      [
        "MemN2N Perez and Liu ( 2017 )",
        "74.0",
        "-"
      ],
      [
        "PtrNet Xu and Hu ( 2018 )",
        "72.1",
        "-"
      ],
      [
        "Neural Belief Tracker: NBT-DNN Mrkšić et al. ( 2017 )",
        "72.6",
        "84.4"
      ],
      [
        "Neural Belief Tracker: NBT-CNN Mrkšić et al. ( 2017 )",
        "73.4",
        "84.2"
      ],
      [
        "Belief Tracking: Bi-LSTM Ramadan et al. ( 2018 )",
        "-",
        "85.1"
      ],
      [
        "Belief Tracking: CNN Ramadan et al. ( 2018 )",
        "-",
        "85.5"
      ],
      [
        "GLAD Zhong et al. ( 2018 )",
        "74.5",
        "88.1"
      ],
      [
        "StateNet",
        "74.1",
        "87.8"
      ],
      [
        "StateNet_PS",
        "74.5",
        "88.2"
      ],
      [
        "[BOLD] StateNet_PSI",
        "[BOLD] 75.5",
        "[BOLD] 88.9"
      ]
    ],
    "id": "502a5b58-34d5-4304-a106-9b6ab93d3401",
    "claim": "StateNet PS outperforms StateNet, and StateNet PSI performs best among all 3 models.",
    "label": "supports",
    "table_id": "88cdb652-a571-4520-82af-622455d0ae90"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.",
    "table_column_names": [
      "Corpus",
      "Metric",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "Europarl",
        "TotalTerms:",
        "980",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "996",
        "1,000"
      ],
      [
        "Europarl",
        "TotalRoots:",
        "79",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "NumberRels:",
        "1,527",
        "1,031",
        "1,049",
        "1,185",
        "1,093",
        "1,644",
        "999"
      ],
      [
        "Europarl",
        "MaxDepth:",
        "19",
        "902",
        "894",
        "784",
        "849",
        "6",
        "10"
      ],
      [
        "Europarl",
        "MinDepth:",
        "1",
        "902",
        "894",
        "784",
        "849",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgDepth:",
        "9.43",
        "902",
        "894",
        "784",
        "849",
        "2.73",
        "4.29"
      ],
      [
        "Europarl",
        "DepthCohesion:",
        "2.02",
        "1",
        "1",
        "1",
        "1",
        "2.19",
        "2.33"
      ],
      [
        "Europarl",
        "MaxWidth:",
        "27",
        "3",
        "3",
        "4",
        "3",
        "201",
        "58"
      ],
      [
        "Europarl",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgWidth:",
        "1.98",
        "1.03",
        "1.05",
        "1.19",
        "1.09",
        "6.25",
        "2.55"
      ],
      [
        "TED Talks",
        "TotalTerms:",
        "296",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000"
      ],
      [
        "TED Talks",
        "TotalRoots:",
        "101",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "NumberRels:",
        "291",
        "1,045",
        "1,229",
        "3,637",
        "4,284",
        "2,875",
        "999"
      ],
      [
        "TED Talks",
        "MaxDepth:",
        "10",
        "860",
        "727",
        "388",
        "354",
        "252",
        "17"
      ],
      [
        "TED Talks",
        "MinDepth:",
        "1",
        "860",
        "727",
        "388",
        "354",
        "249",
        "1"
      ],
      [
        "TED Talks",
        "AvgDepth:",
        "3.94",
        "860",
        "727",
        "388",
        "354",
        "250.43",
        "6.16"
      ],
      [
        "TED Talks",
        "DepthCohesion:",
        "2.54",
        "1",
        "1",
        "1",
        "1",
        "1.01",
        "2.76"
      ],
      [
        "TED Talks",
        "MaxWidth:",
        "37",
        "3",
        "79",
        "18",
        "13",
        "9",
        "41"
      ],
      [
        "TED Talks",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "AvgWidth:",
        "1.79",
        "1.05",
        "1.23",
        "3.64",
        "4.29",
        "2.94",
        "2.37"
      ]
    ],
    "id": "3cdfdbe2-07d7-40fb-b7f9-ae12971c7575",
    "claim": "[CONTINUE] The results for the Portuguese corpora are quite similar to the ones generated by the English corpora, having terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating deep taxonomies, affirming the characteristics of each method.",
    "label": "supports",
    "table_id": "886ce51d-34a7-471b-bb4d-9e97051486de"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.",
    "table_column_names": [
      "Dataset",
      "Accuracy",
      "Fleiss’ kappa  [ITALIC] k"
    ],
    "table_content_values": [
      [
        "Original COPA",
        "100.0",
        "0.973"
      ],
      [
        "Balanced COPA",
        "97.0",
        "0.798"
      ]
    ],
    "id": "f495a3a4-3bb4-4d6c-b794-21a9100e5211",
    "claim": "two annotators were used for each dataset.",
    "label": "not enough info",
    "table_id": "58160fa7-ce6a-4ff8-805b-43776f982ae5"
  },
  {
    "paper": "Variational Self-attention Model for Sentence Representation",
    "paper_id": "1812.11559v4",
    "table_caption": "Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.",
    "table_column_names": [
      "Model",
      "Accuracy (%) agree",
      "Accuracy (%) disagree",
      "Accuracy (%) discuss",
      "Accuracy (%) unrelated",
      "Micro F1(%)"
    ],
    "table_content_values": [
      [
        "Average of Word2vec Embedding",
        "12.43",
        "01.30",
        "43.32",
        "74.24",
        "45.53"
      ],
      [
        "CNN-based Sentence Embedding",
        "24.54",
        "05.06",
        "53.24",
        "79.53",
        "81.72"
      ],
      [
        "RNN-based Sentence Embedding",
        "24.42",
        "05.42",
        "69.05",
        "65.34",
        "78.70"
      ],
      [
        "Self-attention Sentence Embedding",
        "23.53",
        "04.63",
        "63.59",
        "80.34",
        "80.11"
      ],
      [
        "Our model",
        "28.53",
        "10.43",
        "65.43",
        "82.43",
        "[BOLD] 83.54"
      ]
    ],
    "id": "23b7092a-6047-4963-9ad7-0bd5b23ee3ec",
    "claim": "As for the micro F1 evaluation metric, our model achieves the highest performance (83.54%) on the FNC-1 testing subset.",
    "label": "supports",
    "table_id": "5a5d9c55-ed88-4608-be8f-913c3acdcfc4"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.",
    "table_column_names": [
      "[ITALIC] k",
      "Ar",
      "Es",
      "Fr",
      "Ru",
      "Zh",
      "En"
    ],
    "table_content_values": [
      [
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy",
        "POS Tagging Accuracy"
      ],
      [
        "0",
        "88.0",
        "87.9",
        "87.9",
        "87.8",
        "87.7",
        "87.4"
      ],
      [
        "1",
        "92.4",
        "91.9",
        "92.1",
        "92.1",
        "91.5",
        "89.4"
      ],
      [
        "2",
        "91.9",
        "91.8",
        "91.8",
        "91.8",
        "91.3",
        "88.3"
      ],
      [
        "3",
        "92.0",
        "92.3",
        "92.1",
        "91.6",
        "91.2",
        "87.9"
      ],
      [
        "4",
        "92.1",
        "92.4",
        "92.5",
        "92.0",
        "90.5",
        "86.9"
      ],
      [
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy",
        "SEM Tagging Accuracy"
      ],
      [
        "0",
        "81.9",
        "81.9",
        "81.8",
        "81.8",
        "81.8",
        "81.2"
      ],
      [
        "1",
        "87.9",
        "87.7",
        "87.8",
        "87.9",
        "87.7",
        "84.5"
      ],
      [
        "2",
        "87.4",
        "87.5",
        "87.4",
        "87.3",
        "87.2",
        "83.2"
      ],
      [
        "3",
        "87.8",
        "87.9",
        "87.9",
        "87.3",
        "87.3",
        "82.9"
      ],
      [
        "4",
        "88.3",
        "88.6",
        "88.4",
        "88.1",
        "87.7",
        "82.1"
      ],
      [
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU",
        "BLEU"
      ],
      [
        "[EMPTY]",
        "32.7",
        "49.1",
        "38.5",
        "34.2",
        "32.1",
        "96.6"
      ]
    ],
    "id": "c3076ba9-a8f1-40bc-ac14-40e17489112b",
    "claim": "[CONTINUE] we found that En-En encoder-decoders (that is, English autoencoders) produce poor representations for POS and SEM tagging (last column in Table 3).",
    "label": "supports",
    "table_id": "430d822d-f0b9-4d4b-b9a6-b995a9e11686"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "0cf5b110-6f1b-4af8-9bcb-61b13bb71044",
    "claim": "our learned reward based evaluation of the lead baseline improves ROUGE precision and recall, relative to normal ROUGE.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "1d4258e4-b9ea-4b5d-a3a0-c3cb126a5fd3",
    "claim": "For DAMD, we fix K=50.",
    "label": "not enough info",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).",
    "table_column_names": [
      "Model",
      "#Params",
      "Base",
      "+Elmo"
    ],
    "table_content_values": [
      [
        "rnet*",
        "-",
        "71.1/79.5",
        "-/-"
      ],
      [
        "LSTM",
        "2.67M",
        "[BOLD] 70.46/78.98",
        "75.17/82.79"
      ],
      [
        "GRU",
        "2.31M",
        "70.41/ [BOLD] 79.15",
        "75.81/83.12"
      ],
      [
        "ATR",
        "1.59M",
        "69.73/78.70",
        "75.06/82.76"
      ],
      [
        "SRU",
        "2.44M",
        "69.27/78.41",
        "74.56/82.50"
      ],
      [
        "LRN",
        "2.14M",
        "70.11/78.83",
        "[BOLD] 76.14/ [BOLD] 83.83"
      ]
    ],
    "id": "d1dff78c-4750-4fd2-b97c-a90c86c61345",
    "claim": "In this task, LRN outperforms ATR and SRU in terms of both EM and F1 score.",
    "label": "supports",
    "table_id": "e8718aba-7d2a-43da-ab9f-fe42d951b94a"
  },
  {
    "paper": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training",
    "paper_id": "1810.11895v3",
    "table_caption": "Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).",
    "table_column_names": [
      "[EMPTY]",
      "dev CS",
      "dev mono",
      "test CS",
      "test mono"
    ],
    "table_content_values": [
      [
        "CS-only-LM",
        "45.20",
        "65.87",
        "43.20",
        "62.80"
      ],
      [
        "Fine-Tuned-LM",
        "49.60",
        "72.67",
        "47.60",
        "71.33"
      ],
      [
        "CS-only-disc",
        "[BOLD] 75.60",
        "70.40",
        "70.80",
        "70.53"
      ],
      [
        "Fine-Tuned-disc",
        "70.80",
        "[BOLD] 74.40",
        "[BOLD] 75.33",
        "[BOLD] 75.87"
      ]
    ],
    "id": "9bf4c3fe-a123-4e6c-98e3-14e4a13d4f09",
    "claim": "[CONTINUE] the FINE-TUNEDDISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions.",
    "label": "supports",
    "table_id": "1e599458-4bd1-4761-9fc4-8743eb9e544b"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Iteration=1",
        "0.531",
        "0.455",
        "0.353",
        "0.201"
      ],
      [
        "Iteration=2",
        "0.592",
        "0.498",
        "0.385",
        "0.375"
      ],
      [
        "Iteration=3",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ],
      [
        "Iteration=4",
        "0.601",
        "0.505",
        "0.422",
        "0.385"
      ],
      [
        "Iteration=5",
        "0.575",
        "0.495",
        "0.394",
        "0.376"
      ]
    ],
    "id": "32dc7e99-55c4-4b53-aadd-bea2051846d9",
    "claim": "for example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9).",
    "label": "not enough info",
    "table_id": "e8d636ab-66cd-4c48-97ea-e05fd1ff69c0"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "47128d7b-a5a9-40c7-8e9c-a2dead1340aa",
    "claim": "Compared to Zhou et\\xa0al.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "6172cecc-4b05-4e6d-80ec-97c8a9c62411",
    "claim": "InferSent-Cosine achieves a stronger agreement with the selection of sentences between human and the metric than BERT-Cosine.",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. “Acc. Inc.” refers to the boost in performance contributed by the pretraining step. “% of Perf.” refers to the percentage of the total performance that the pretraining step contributes.",
    "table_column_names": [
      "Finetuning",
      "Pretrained?",
      "Accuracy",
      "Val. Loss",
      "Acc. Inc.",
      "% of Perf."
    ],
    "table_content_values": [
      [
        "Multitasking",
        "No",
        "53.61%",
        "0.7217",
        "-",
        "-"
      ],
      [
        "[EMPTY]",
        "Yes",
        "96.28%",
        "0.2197",
        "+42.67%",
        "44.32%"
      ],
      [
        "Standard",
        "No",
        "51.02%",
        "0.7024",
        "-",
        "-"
      ],
      [
        "[EMPTY]",
        "Yes",
        "90.99%",
        "0.1826",
        "+39.97%",
        "43.93%"
      ]
    ],
    "id": "9bde8eb0-16a1-41f1-bde7-8d9066b32406",
    "claim": "In Table 5, it can be seen that generative pretraining via language modeling does account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup.",
    "label": "supports",
    "table_id": "3d184b37-c091-4992-beff-c60005fbd2b4"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.",
    "table_column_names": [
      "[EMPTY]",
      "Italian → En",
      "Italian En →",
      "German → En",
      "German En →"
    ],
    "table_content_values": [
      [
        "Orig",
        "58.73",
        "59.68",
        "47.58",
        "50.48"
      ],
      [
        "Debias",
        "[BOLD] 60.03",
        "[BOLD] 60.96",
        "[BOLD] 47.89",
        "[BOLD] 51.76"
      ]
    ],
    "id": "1f092e69-62e3-4ec5-a7e8-17cc384fd74b",
    "claim": "The results reported in Table 7 show that precision on BDI does not increase as a result of the reduced effect of grammatical gender on the embeddings for German and Italian.",
    "label": "refutes",
    "table_id": "13fef5cb-6185-4d5a-aeac-e5586df49ae7"
  },
  {
    "paper": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "paper_id": "1906.01749v3",
    "table_caption": "Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
    "table_column_names": [
      "[BOLD] Dataset",
      "[BOLD] # pairs",
      "[BOLD] # words (doc)",
      "[BOLD] # sents (docs)",
      "[BOLD] # words (summary)",
      "[BOLD] # sents (summary)",
      "[BOLD] vocab size"
    ],
    "table_content_values": [
      [
        "Multi-News",
        "44,972/5,622/5,622",
        "2,103.49",
        "82.73",
        "263.66",
        "9.97",
        "666,515"
      ],
      [
        "DUC03+04",
        "320",
        "4,636.24",
        "173.15",
        "109.58",
        "2.88",
        "19,734"
      ],
      [
        "TAC 2011",
        "176",
        "4,695.70",
        "188.43",
        "99.70",
        "1.00",
        "24,672"
      ],
      [
        "CNNDM",
        "287,227/13,368/11,490",
        "810.57",
        "39.78",
        "56.20",
        "3.68",
        "717,951"
      ]
    ],
    "id": "383dd023-7ed9-4ccf-ae4c-879ef4bdff0a",
    "claim": "The number of examples in our Multi-News dataset is not significantly larger than previous MDS news data.",
    "label": "refutes",
    "table_id": "9fda9198-5e75-4f79-b42c-82b7dc3ee31f"
  },
  {
    "paper": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns",
    "paper_id": "1810.05201v1",
    "table_caption": "Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
    "table_column_names": [
      "[EMPTY]",
      "M",
      "F",
      "B",
      "O"
    ],
    "table_content_values": [
      [
        "Random",
        "43.6",
        "39.3",
        "[ITALIC] 0.90",
        "41.5"
      ],
      [
        "Token Distance",
        "50.1",
        "42.4",
        "[ITALIC] 0.85",
        "46.4"
      ],
      [
        "Topical Entity",
        "51.5",
        "43.7",
        "[ITALIC] 0.85",
        "47.7"
      ],
      [
        "Syntactic Distance",
        "63.0",
        "56.2",
        "[ITALIC] 0.89",
        "59.7"
      ],
      [
        "Parallelism",
        "[BOLD] 67.1",
        "[BOLD] 63.1",
        "[ITALIC]  [BOLD] 0.94",
        "[BOLD] 65.2"
      ],
      [
        "Parallelism+URL",
        "[BOLD] 71.1",
        "[BOLD] 66.9",
        "[ITALIC]  [BOLD] 0.94",
        "[BOLD] 69.0"
      ],
      [
        "Transformer-Single",
        "58.6",
        "51.2",
        "[ITALIC] 0.87",
        "55.0"
      ],
      [
        "Transformer-Multi",
        "59.3",
        "52.9",
        "[ITALIC] 0.89",
        "56.2"
      ]
    ],
    "id": "697c2b0c-fe97-4d2c-b0fe-70e21db0f34d",
    "claim": "[CONTINUE] TRANSFORMER-MULTI is weaker than TRANSFORMER-SINGLE [CONTINUE] .2% overall decrease in performance compared to TRANSFORMER-SINGLE for the goldtwo-mention task.",
    "label": "refutes",
    "table_id": "4e776dc6-bb84-4275-8461-46b00582b898"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "10362ffd-84e6-423f-a42e-7c91c823a931",
    "claim": "the results show that GDPL, the proposed method, improves the task-completion rate by 27.6% over the state-of-the-art baseline and is 2.43 times closer to the upper bound on this measure as well as 3.22 times closer to the upper bound on the success rate measure over 10 random seeds.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "afd33f80-8078-43b6-bc31-857981266ce4",
    "claim": "[CONTINUE] As we can observe in Table 3, Patt has the best values of precision for the English corpora while DocSub has the best values for the Portuguese corpora.",
    "label": "supports",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 5: Performance of different agents on the neural user simulator.",
    "table_column_names": [
      "Method",
      "VHUS Turns",
      "VHUS Inform",
      "VHUS Match",
      "VHUS Success"
    ],
    "table_content_values": [
      [
        "ACER",
        "22.35",
        "55.13",
        "33.08",
        "18.6"
      ],
      [
        "PPO",
        "[BOLD] 19.23",
        "[BOLD] 56.31",
        "33.08",
        "18.3"
      ],
      [
        "ALDM",
        "26.90",
        "54.37",
        "24.15",
        "16.4"
      ],
      [
        "GDPL",
        "22.43",
        "52.58",
        "[BOLD] 36.21",
        "[BOLD] 19.7"
      ]
    ],
    "id": "ca818aa7-ae44-4de7-bc61-8995cb899288",
    "claim": "In the natural state space with 75 actions, training does not converge within a reasonable training time.",
    "label": "not enough info",
    "table_id": "813734b3-14a6-472b-8fca-5401f621ca90"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 3: Number of tweets annotated as complaints across the nine domains.",
    "table_column_names": [
      "[BOLD] Category",
      "[BOLD] Complaints",
      "[BOLD] Not Complaints"
    ],
    "table_content_values": [
      [
        "Food & Beverage",
        "95",
        "35"
      ],
      [
        "Apparel",
        "141",
        "117"
      ],
      [
        "Retail",
        "124",
        "75"
      ],
      [
        "Cars",
        "67",
        "25"
      ],
      [
        "Services",
        "207",
        "130"
      ],
      [
        "Software & Online Services",
        "189",
        "103"
      ],
      [
        "Transport",
        "139",
        "109"
      ],
      [
        "Electronics",
        "174",
        "112"
      ],
      [
        "Other",
        "96",
        "33"
      ],
      [
        "Total",
        "1232",
        "739"
      ]
    ],
    "id": "3ce60497-4677-4795-bb32-cfc18403af9e",
    "claim": "In total, 739 tweets (37.6%) are complaints and 1,232 are not complaints (62.4%).",
    "label": "refutes",
    "table_id": "557b1d52-6b5b-49b6-a22d-c130bc5e7d4d"
  },
  {
    "paper": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "paper_id": "1910.14161v1",
    "table_caption": "Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.",
    "table_column_names": [
      "[EMPTY]",
      "Italian Original",
      "Italian Debiased",
      "Italian English",
      "Italian Reduction",
      "German Original",
      "German Debiased",
      "German English",
      "German Reduction"
    ],
    "table_content_values": [
      [
        "Same Gender",
        "0.442",
        "0.434",
        "0.424",
        "–",
        "0.491",
        "0.478",
        "0.446",
        "–"
      ],
      [
        "Different Gender",
        "0.385",
        "0.421",
        "0.415",
        "–",
        "0.415",
        "0.435",
        "0.403",
        "–"
      ],
      [
        "difference",
        "0.057",
        "0.013",
        "0.009",
        "[BOLD] 91.67%",
        "0.076",
        "0.043",
        "0.043",
        "[BOLD] 100%"
      ]
    ],
    "id": "9c2d8eeb-0c25-43a9-80e9-93904819315d",
    "claim": "In German, we get a reduction of less than 100%.",
    "label": "refutes",
    "table_id": "a3f7a8f4-e69f-4e32-beb4-281137d9a4a5"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "3afea35e-02b3-4a45-ab2a-45bc3da9ecc9",
    "claim": "Our NeuralTD system is trained only with a simple learning signal that was automatically induced from human ratings, and outperforms the advanced models with access to the gold labels.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "4c16d276-516e-49e6-a251-401cbabc6596",
    "claim": "The reason may be that a large neural network  (BERT) with its accompanying large input space allows the network to learn a meaningful reward function with greater scope, while the shallower network used in both SimRed and PMeans-RNN may not be adequate for training the same type of reward.",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "10c5b2a6-50ed-4507-a418-d52834572fc8",
    "claim": "To further explore the limitations of DAMD, we focus on the 10-Action Generation task.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
    "table_column_names": [
      "System",
      "All P",
      "All R",
      "All F1",
      "In  [ITALIC] E+ P",
      "In  [ITALIC] E+ R",
      "In  [ITALIC] E+ F1"
    ],
    "table_content_values": [
      [
        "Name matching",
        "15.03",
        "15.03",
        "15.03",
        "29.13",
        "29.13",
        "29.13"
      ],
      [
        "MIL (model 1)",
        "35.87",
        "35.87",
        "35.87 ±0.72",
        "69.38",
        "69.38",
        "69.38 ±1.29"
      ],
      [
        "MIL-ND (model 2)",
        "37.42",
        "[BOLD] 37.42",
        "37.42 ±0.35",
        "72.50",
        "[BOLD] 72.50",
        "[BOLD] 72.50 ±0.68"
      ],
      [
        "[ITALIC] τMIL-ND (model 2)",
        "[BOLD] 38.91",
        "36.73",
        "[BOLD] 37.78 ±0.26",
        "[BOLD] 73.19",
        "71.15",
        "72.16 ±0.48"
      ],
      [
        "Supervised learning",
        "42.90",
        "42.90",
        "42.90 ±0.59",
        "83.12",
        "83.12",
        "83.12 ±1.15"
      ]
    ],
    "id": "5f3db10e-3fe4-4526-b808-dad896f4ac6e",
    "claim": "Because all the test data points are valid for the 'In E+' setting, using the ND classifier had a slight negative effect on F1.",
    "label": "supports",
    "table_id": "ce21ef70-1b61-4a48-9c97-69426861922c"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.",
    "table_column_names": [
      "Dataset",
      "Accuracy",
      "Fleiss’ kappa  [ITALIC] k"
    ],
    "table_content_values": [
      [
        "Original COPA",
        "100.0",
        "0.973"
      ],
      [
        "Balanced COPA",
        "97.0",
        "0.798"
      ]
    ],
    "id": "ed202752-8bac-401b-89ef-565c1be1319a",
    "claim": "The human evaluation shows that our mirrored instances are comparable in difficulty to the original ones (see Table 3).",
    "label": "supports",
    "table_id": "58160fa7-ce6a-4ff8-805b-43776f982ae5"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "fb0cbc1f-7acb-43e3-b84e-968a72ba2a88",
    "claim": "we see significant improvements in each of the five cases.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Syntactic Dependency Representations in Neural Relation Classification",
    "paper_id": "1805.11461v1",
    "table_caption": "Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
    "table_column_names": [
      "[BOLD] Representation",
      "[BOLD] Hyper parameters Filter size",
      "[BOLD] Hyper parameters Num. Feature maps",
      "[BOLD] Hyper parameters Activation func.",
      "[BOLD] Hyper parameters L2 Reg.",
      "[BOLD] Hyper parameters Learning rate",
      "[BOLD] Hyper parameters Dropout Prob.",
      "[BOLD] F1.(avg. in 5-fold) with default values",
      "[BOLD] F1.(avg. in 5-fold) with optimal values"
    ],
    "table_content_values": [
      [
        "CoNLL08",
        "4-5",
        "1000",
        "Softplus",
        "1.15e+01",
        "1.13e-03",
        "1",
        "73.34",
        "74.49"
      ],
      [
        "SB",
        "4-5",
        "806",
        "Sigmoid",
        "8.13e-02",
        "1.79e-03",
        "0.87",
        "72.83",
        "[BOLD] 75.05"
      ],
      [
        "UD v1.3",
        "5",
        "716",
        "Softplus",
        "1.66e+00",
        "9.63E-04",
        "1",
        "68.93",
        "69.57"
      ]
    ],
    "id": "f021eb05-ef24-40f8-9729-39dbcb2c1baf",
    "claim": "We see that the optimized parameter settings vary for the different representations, showing the importance of tuning for these types of comparisons.",
    "label": "supports",
    "table_id": "602023a1-3ab5-4788-8101-59ae8f0c6ce1"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "82dbb1e6-82ef-4fef-860f-e26ee0e3b964",
    "claim": "The models have better results when handling sentences with 20 or fewer tokens.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "paper_id": "1909.03546v2",
    "table_caption": "Table 3: F1 scores on Relation.",
    "table_column_names": [
      "[EMPTY]",
      "ACE05",
      "SciERC",
      "WLPC"
    ],
    "table_content_values": [
      [
        "BERT + LSTM",
        "60.6",
        "40.3",
        "65.1"
      ],
      [
        "+RelProp",
        "61.9",
        "41.1",
        "65.3"
      ],
      [
        "+CorefProp",
        "59.7",
        "42.6",
        "-"
      ],
      [
        "BERT FineTune",
        "[BOLD] 62.1",
        "44.3",
        "65.4"
      ],
      [
        "+RelProp",
        "62.0",
        "43.0",
        "[BOLD] 65.5"
      ],
      [
        "+CorefProp",
        "60.0",
        "[BOLD] 45.3",
        "-"
      ]
    ],
    "id": "900aab18-0c84-4791-b0e8-3c2d4270ff79",
    "claim": "CorefProp does not improve relation extraction on SciERC.",
    "label": "refutes",
    "table_id": "3fc48613-1f39-4968-a1bc-84c2cae46001"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
    "table_column_names": [
      "ID LSTM-800",
      "5-fold CV 70.56",
      "Δ 0.66",
      "Single model 67.54",
      "Δ 0.78",
      "Ensemble 67.65",
      "Δ 0.30"
    ],
    "table_content_values": [
      [
        "LSTM-400",
        "70.50",
        "0.60",
        "[BOLD] 67.59",
        "0.83",
        "[BOLD] 68.00",
        "0.65"
      ],
      [
        "IN-TITLE",
        "70.11",
        "0.21",
        "[EMPTY]",
        "[EMPTY]",
        "67.52",
        "0.17"
      ],
      [
        "[BOLD] SUBMISSION",
        "69.90",
        "–",
        "66.76",
        "–",
        "67.35",
        "–"
      ],
      [
        "NO-HIGHWAY",
        "69.72",
        "−0.18",
        "66.42",
        "−0.34",
        "66.64",
        "−0.71"
      ],
      [
        "NO-OVERLAPS",
        "69.46",
        "−0.44",
        "65.07",
        "−1.69",
        "66.47",
        "−0.88"
      ],
      [
        "LSTM-400-DROPOUT",
        "69.45",
        "−0.45",
        "65.53",
        "−1.23",
        "67.28",
        "−0.07"
      ],
      [
        "NO-TRANSLATIONS",
        "69.42",
        "−0.48",
        "65.92",
        "−0.84",
        "67.23",
        "−0.12"
      ],
      [
        "NO-ELMO-FINETUNING",
        "67.71",
        "−2.19",
        "65.16",
        "−1.60",
        "65.42",
        "−1.93"
      ]
    ],
    "id": "337b26da-3751-4e60-b0a0-f78b9af8cafe",
    "claim": "[CONTINUE] Perhaps the most striking thing about the ablation results is that the 'traditional' LSTM layout outsperformed the 'alternating' one we chose for our submission.",
    "label": "supports",
    "table_id": "0980bab3-642c-413e-8b15-6abd868956a8"
  },
  {
    "paper": "Towards Quantifying the Distance between Opinions",
    "paper_id": "2001.09879v1",
    "table_caption": "Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
    "table_column_names": [
      "Topic Name",
      "Size",
      "TF-IDF ARI",
      "WMD ARI",
      "Sent2vec ARI",
      "Doc2vec ARI",
      "BERT ARI",
      "[ITALIC] OD-w2v ARI",
      "[ITALIC] OD-d2v ARI",
      "TF-IDF  [ITALIC] Sil.",
      "WMD  [ITALIC] Sil.",
      "Sent2vec  [ITALIC] Sil.",
      "Doc2vec  [ITALIC] Sil.",
      "BERT  [ITALIC] Sil.",
      "[ITALIC] OD-w2v  [ITALIC] Sil.",
      "[ITALIC] OD-d2v  [ITALIC] Sil."
    ],
    "table_content_values": [
      [
        "Affirmative Action",
        "81",
        "-0.07",
        "-0.02",
        "0.03",
        "-0.01",
        "-0.02",
        "[BOLD] 0.14",
        "[ITALIC] 0.02",
        "0.01",
        "0.01",
        "-0.01",
        "-0.02",
        "-0.04",
        "[BOLD] 0.06",
        "[ITALIC] 0.01"
      ],
      [
        "Atheism",
        "116",
        "[BOLD] 0.19",
        "0.07",
        "0.00",
        "0.03",
        "-0.01",
        "0.11",
        "[ITALIC] 0.16",
        "0.02",
        "0.01",
        "0.02",
        "0.01",
        "0.01",
        "[ITALIC] 0.05",
        "[BOLD] 0.07"
      ],
      [
        "Austerity Measures",
        "20",
        "[ITALIC] 0.04",
        "[ITALIC] 0.04",
        "-0.01",
        "-0.05",
        "0.04",
        "[BOLD] 0.21",
        "-0.01",
        "0.06",
        "0.07",
        "0.05",
        "-0.03",
        "0.10",
        "[BOLD] 0.19",
        "0.1"
      ],
      [
        "Democratization",
        "76",
        "0.02",
        "-0.01",
        "0.00",
        "[ITALIC] 0.09",
        "-0.01",
        "[BOLD] 0.11",
        "0.07",
        "0.01",
        "0.01",
        "0.02",
        "0.02",
        "0.03",
        "[BOLD] 0.16",
        "[ITALIC] 0.11"
      ],
      [
        "Education Voucher Scheme",
        "30",
        "[BOLD] 0.25",
        "0.12",
        "0.08",
        "-0.02",
        "0.04",
        "0.13",
        "[ITALIC] 0.19",
        "0.01",
        "0.01",
        "0.01",
        "-0.01",
        "0.02",
        "[ITALIC] 0.38",
        "[BOLD] 0.40"
      ],
      [
        "Gambling",
        "60",
        "-0.06",
        "-0.01",
        "-0.02",
        "0.04",
        "0.09",
        "[ITALIC] 0.35",
        "[BOLD] 0.39",
        "0.01",
        "0.02",
        "0.03",
        "0.01",
        "0.09",
        "[BOLD] 0.30",
        "[ITALIC] 0.22"
      ],
      [
        "Housing",
        "30",
        "0.01",
        "-0.01",
        "-0.01",
        "-0.02",
        "0.08",
        "[BOLD] 0.27",
        "0.01",
        "0.02",
        "0.03",
        "0.03",
        "0.01",
        "0.11",
        "[BOLD] 0.13",
        "[ITALIC] 0.13"
      ],
      [
        "Hydroelectric Dams",
        "110",
        "[BOLD] 0.47",
        "[ITALIC] 0.45",
        "[ITALIC] 0.45",
        "-0.01",
        "0.38",
        "0.35",
        "0.14",
        "0.04",
        "0.08",
        "0.12",
        "0.01",
        "0.19",
        "[BOLD] 0.26",
        "[ITALIC] 0.09"
      ],
      [
        "Intellectual Property",
        "66",
        "0.01",
        "0.01",
        "0.00",
        "0.03",
        "0.03",
        "[ITALIC] 0.05",
        "[BOLD] 0.14",
        "0.01",
        "[ITALIC] 0.04",
        "0.03",
        "0.01",
        "0.03",
        "[ITALIC] 0.04",
        "[BOLD] 0.12"
      ],
      [
        "Keystone pipeline",
        "18",
        "0.01",
        "0.01",
        "0.00",
        "-0.13",
        "[BOLD] 0.07",
        "-0.01",
        "[BOLD] 0.07",
        "-0.01",
        "-0.03",
        "-0.03",
        "-0.07",
        "0.03",
        "[BOLD] 0.05",
        "[ITALIC] 0.02"
      ],
      [
        "Monarchy",
        "61",
        "-0.04",
        "0.01",
        "0.00",
        "0.03",
        "-0.02",
        "[BOLD] 0.15",
        "[BOLD] 0.15",
        "0.01",
        "0.02",
        "0.02",
        "0.01",
        "0.01",
        "[BOLD] 0.11",
        "[ITALIC] 0.09"
      ],
      [
        "National Service",
        "33",
        "0.14",
        "-0.03",
        "-0.01",
        "0.02",
        "0.01",
        "[ITALIC] 0.31",
        "[BOLD] 0.39",
        "0.02",
        "0.04",
        "0.02",
        "0.01",
        "0.02",
        "[BOLD] 0.25",
        "[BOLD] 0.25"
      ],
      [
        "One-child policy China",
        "67",
        "-0.05",
        "0.01",
        "[BOLD] 0.11",
        "-0.02",
        "0.02",
        "[BOLD] 0.11",
        "0.01",
        "0.01",
        "0.02",
        "[ITALIC] 0.04",
        "-0.01",
        "0.03",
        "[BOLD] 0.07",
        "-0.02"
      ],
      [
        "Open-source Software",
        "48",
        "-0.02",
        "-0.01",
        "[ITALIC] 0.05",
        "0.01",
        "0.12",
        "[BOLD] 0.09",
        "-0.02",
        "0.01",
        "-0.01",
        "0.00",
        "-0.02",
        "0.03",
        "[BOLD] 0.18",
        "0.01"
      ],
      [
        "Pornography",
        "52",
        "-0.02",
        "0.01",
        "0.01",
        "-0.02",
        "-0.01",
        "[BOLD] 0.41",
        "[BOLD] 0.41",
        "0.01",
        "0.01",
        "0.02",
        "-0.01",
        "0.03",
        "[BOLD] 0.47",
        "[ITALIC] 0.41"
      ],
      [
        "Seanad Abolition",
        "25",
        "0.23",
        "0.09",
        "-0.01",
        "-0.01",
        "0.03",
        "[ITALIC] 0.32",
        "[BOLD] 0.54",
        "0.02",
        "0.01",
        "-0.01",
        "-0.03",
        "-0.04",
        "[ITALIC] 0.15",
        "[BOLD] 0.31"
      ],
      [
        "Trades Unions",
        "19",
        "[ITALIC] 0.44",
        "[ITALIC] 0.44",
        "[BOLD] 0.60",
        "-0.05",
        "0.44",
        "[ITALIC] 0.44",
        "0.29",
        "0.1",
        "0.17",
        "0.21",
        "0.01",
        "0.26",
        "[BOLD] 0.48",
        "[ITALIC] 0.32"
      ],
      [
        "Video Games",
        "72",
        "-0.01",
        "0.01",
        "0.12",
        "0.01",
        "0.08",
        "[ITALIC] 0.40",
        "[BOLD] 0.56",
        "0.01",
        "0.01",
        "0.06",
        "0.01",
        "0.05",
        "[ITALIC] 0.32",
        "[BOLD] 0.42"
      ],
      [
        "Average",
        "54.67",
        "0.09",
        "0.07",
        "0.08",
        "0.01",
        "0.08",
        "[BOLD] 0.22",
        "[ITALIC] 0.20",
        "0.02",
        "0.03",
        "0.04",
        "-0.01",
        "0.05",
        "[BOLD] 0.20",
        "[ITALIC] 0.17"
      ]
    ],
    "id": "aff15db7-a64b-4e93-9e29-0a34989164f0",
    "claim": "The semantic threshold for OD-d2v is set at 0.6 while for OD-w2v is set at 0.3.",
    "label": "refutes",
    "table_id": "df35adfa-4085-4c3c-b0e0-74d5e71e4fb9"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VI: Correlations for Word Similarity Tests",
    "table_column_names": [
      "Dataset (EN-)",
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "WS-353-ALL",
        "0.612",
        "0.7156",
        "0.634",
        "0.622",
        "0.173",
        "0.690",
        "0.657"
      ],
      [
        "SIMLEX-999",
        "0.359",
        "0.3939",
        "0.295",
        "0.355",
        "0.090",
        "0.380",
        "0.381"
      ],
      [
        "VERB-143",
        "0.326",
        "0.4430",
        "0.255",
        "0.271",
        "0.293",
        "0.271",
        "0.348"
      ],
      [
        "SimVerb-3500",
        "0.193",
        "0.2856",
        "0.184",
        "0.197",
        "0.035",
        "0.234",
        "0.245"
      ],
      [
        "WS-353-REL",
        "0.578",
        "0.6457",
        "0.595",
        "0.578",
        "0.134",
        "0.695",
        "0.619"
      ],
      [
        "RW-STANF.",
        "0.378",
        "0.4858",
        "0.316",
        "0.373",
        "0.122",
        "0.390",
        "0.382"
      ],
      [
        "YP-130",
        "0.524",
        "0.5211",
        "0.353",
        "0.482",
        "0.169",
        "0.420",
        "0.589"
      ],
      [
        "MEN-TR-3k",
        "0.710",
        "0.7528",
        "0.684",
        "0.696",
        "0.298",
        "0.769",
        "0.725"
      ],
      [
        "RG-65",
        "0.768",
        "0.8051",
        "0.736",
        "0.732",
        "0.338",
        "0.761",
        "0.774"
      ],
      [
        "MTurk-771",
        "0.650",
        "0.6712",
        "0.593",
        "0.623",
        "0.199",
        "0.665",
        "0.671"
      ],
      [
        "WS-353-SIM",
        "0.682",
        "0.7883",
        "0.713",
        "0.702",
        "0.220",
        "0.720",
        "0.720"
      ],
      [
        "MC-30",
        "0.749",
        "0.8112",
        "0.799",
        "0.726",
        "0.330",
        "0.735",
        "0.776"
      ],
      [
        "MTurk-287",
        "0.649",
        "0.6645",
        "0.591",
        "0.631",
        "0.295",
        "0.674",
        "0.634"
      ],
      [
        "Average",
        "0.552",
        "0.6141",
        "0.519",
        "0.538",
        "0.207",
        "0.570",
        "0.579"
      ]
    ],
    "id": "c596a875-72aa-4af6-b9de-df0c2111521d",
    "claim": "The proposed approach is seen to perform well against the other unsupervised models.",
    "label": "not enough info",
    "table_id": "1551d88a-ea00-4a48-a576-e3535dd3b391"
  },
  {
    "paper": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data",
    "paper_id": "1904.10743v1",
    "table_caption": "Table 1: Performance of supervised learning models with different features.",
    "table_column_names": [
      "Feature",
      "LR P",
      "LR R",
      "LR F1",
      "SVM P",
      "SVM R",
      "SVM F1",
      "ANN P",
      "ANN R",
      "ANN F1"
    ],
    "table_content_values": [
      [
        "+BoW",
        "0.93",
        "0.91",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+BoC (Wiki-PubMed-PMC)",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.94",
        "0.92",
        "[BOLD] 0.93",
        "0.91",
        "0.91",
        "[BOLD] 0.91"
      ],
      [
        "+BoC (GloVe)",
        "0.93",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ],
      [
        "+ASM",
        "0.90",
        "0.85",
        "0.88",
        "0.90",
        "0.86",
        "0.88",
        "0.89",
        "0.89",
        "0.89"
      ],
      [
        "+Sentence Embeddings(SEs)",
        "0.89",
        "0.89",
        "0.89",
        "0.90",
        "0.86",
        "0.88",
        "0.88",
        "0.88",
        "0.88"
      ],
      [
        "+BoC(Wiki-PubMed-PMC)+SEs",
        "0.92",
        "0.92",
        "0.92",
        "0.94",
        "0.92",
        "0.93",
        "0.91",
        "0.91",
        "0.91"
      ]
    ],
    "id": "5c1296f4-0fdf-4ab1-9ef2-c8cb4e4a2e3f",
    "claim": "Word embeddings derived from GloVe outperform Wiki-PubMed-PMC-based embeddings (Table 1).",
    "label": "refutes",
    "table_id": "042a154a-add4-4bcb-be23-541a760b2e1c"
  },
  {
    "paper": "Localization of Fake News Detection via Multitask Transfer Learning",
    "paper_id": "1910.09295v3",
    "table_caption": "Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
    "table_column_names": [
      "# of Heads",
      "Accuracy",
      "Val. Loss",
      "Effect"
    ],
    "table_content_values": [
      [
        "1",
        "89.44%",
        "0.2811",
        "-6.84%"
      ],
      [
        "2",
        "91.20%",
        "0.2692",
        "-5.08%"
      ],
      [
        "4",
        "93.85%",
        "0.2481",
        "-2.43%"
      ],
      [
        "8",
        "96.02%",
        "0.2257",
        "-0.26%"
      ],
      [
        "10",
        "96.28%",
        "0.2197",
        "[EMPTY]"
      ],
      [
        "16",
        "96.32%",
        "0.2190",
        "+0.04"
      ]
    ],
    "id": "2781ce47-903b-4c89-85a8-6153ef7c5707",
    "claim": "As shown in Table 6, increasing the number of attention heads does not necessarily improve multitasking performance.",
    "label": "refutes",
    "table_id": "34083185-d444-4bd0-994e-d13f5afe8e82"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "2220ceb0-019d-443e-89ee-f7ef12630a84",
    "claim": "The performance of each approach that interacts with the agenda-based user simulator is shown in Table 3, with GDPL outperforming all other methods.",
    "label": "refutes",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "de861aa8-af5f-4e43-89a4-34d49c90a470",
    "claim": "The reward obtained from other metrics is lower than the blue marker because they have many situations that cannot receive full rewards even in correct behavior.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 4: Results of Self-Play Evaluation.",
    "table_column_names": [
      "System",
      "TGPC Succ. (%)",
      "TGPC #Turns",
      "CWC Succ. (%)",
      "CWC #Turns"
    ],
    "table_content_values": [
      [
        "Retrieval ",
        "7.16",
        "4.17",
        "0",
        "-"
      ],
      [
        "Retrieval-Stgy ",
        "47.80",
        "6.7",
        "44.6",
        "7.42"
      ],
      [
        "PMI ",
        "35.36",
        "6.38",
        "47.4",
        "5.29"
      ],
      [
        "Neural ",
        "54.76",
        "4.73",
        "47.6",
        "5.16"
      ],
      [
        "Kernel ",
        "62.56",
        "4.65",
        "53.2",
        "4.08"
      ],
      [
        "DKRN (ours)",
        "[BOLD] 89.0",
        "5.02",
        "[BOLD] 84.4",
        "4.20"
      ]
    ],
    "id": "08aab654-a0bc-4fc3-9ba0-5c1ae544fc69",
    "claim": "Although the average number of turns of our approach is slightly more than Kernel, our system obtains the highest success rate, significantly improving over other approaches.",
    "label": "supports",
    "table_id": "5a8c9b40-6ff6-44b7-af2e-4bf75b46492f"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
    "table_column_names": [
      "[BOLD] Model",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN4",
        "25.5",
        "55.4"
      ],
      [
        "-{4} dense block",
        "24.8",
        "54.9"
      ],
      [
        "-{3, 4} dense blocks",
        "23.8",
        "54.1"
      ],
      [
        "-{2, 3, 4} dense blocks",
        "23.2",
        "53.1"
      ]
    ],
    "id": "de569012-eb52-4a91-b41c-4f97bd382305",
    "claim": "Although these four models have the same number of layers, dense connections do not necessarily lead to better performance.",
    "label": "refutes",
    "table_id": "255ea063-8be8-4807-9326-462c9bfa47ee"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "2c5d4216-a6de-4d7c-ba81-4cde5a1639d8",
    "claim": "Table 3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL.",
    "label": "supports",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Original",
        "TGen−",
        "63.37",
        "7.7188",
        "41.99",
        "68.53",
        "1.9355",
        "00.06",
        "15.77",
        "00.11",
        "15.94"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen",
        "66.41",
        "8.5565",
        "45.07",
        "69.17",
        "2.2253",
        "00.14",
        "04.11",
        "00.03",
        "04.27"
      ],
      [
        "Original",
        "[BOLD] Original",
        "TGen+",
        "67.06",
        "8.5871",
        "45.83",
        "69.73",
        "2.2681",
        "00.04",
        "01.75",
        "00.01",
        "01.80"
      ],
      [
        "Original",
        "[BOLD] Original",
        "SC-LSTM",
        "39.11",
        "5.6704",
        "36.83",
        "50.02",
        "0.6045",
        "02.79",
        "18.90",
        "09.79",
        "31.51"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen−",
        "65.87",
        "8.6400",
        "44.20",
        "67.51",
        "2.1710",
        "00.20",
        "00.56",
        "00.21",
        "00.97"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen",
        "66.24",
        "8.6889",
        "44.66",
        "67.85",
        "2.2181",
        "00.10",
        "00.02",
        "00.00",
        "00.12"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "TGen+",
        "65.97",
        "8.6630",
        "44.45",
        "67.59",
        "2.1855",
        "00.02",
        "00.00",
        "00.00",
        "00.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Original",
        "SC-LSTM",
        "38.52",
        "5.7125",
        "37.45",
        "48.50",
        "0.4343",
        "03.85",
        "17.39",
        "08.12",
        "29.37"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen−",
        "66.28",
        "8.5202",
        "43.96",
        "67.83",
        "2.1375",
        "00.14",
        "02.26",
        "00.22",
        "02.61"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen",
        "67.00",
        "8.6889",
        "44.97",
        "68.19",
        "2.2228",
        "00.06",
        "00.44",
        "00.03",
        "00.53"
      ],
      [
        "Cleaned missing",
        "[BOLD] Original",
        "TGen+",
        "66.74",
        "8.6649",
        "44.84",
        "67.95",
        "2.2018",
        "00.00",
        "00.21",
        "00.03",
        "00.24"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen−",
        "64.40",
        "7.9692",
        "42.81",
        "68.87",
        "2.0563",
        "00.01",
        "13.08",
        "00.00",
        "13.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen",
        "66.23",
        "8.5578",
        "45.12",
        "68.87",
        "2.2548",
        "00.04",
        "03.04",
        "00.00",
        "03.09"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Original",
        "TGen+",
        "65.96",
        "8.5238",
        "45.49",
        "68.79",
        "2.2456",
        "00.00",
        "01.44",
        "00.00",
        "01.45"
      ]
    ],
    "id": "69d3706c-7e8f-4407-bb39-5eea75a9fb9c",
    "claim": "WOMs are slightly higher for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams.",
    "label": "refutes",
    "table_id": "729f3ae4-c5db-4e3c-804c-d4cd63d57011"
  },
  {
    "paper": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents",
    "paper_id": "1901.02081v1",
    "table_caption": "Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
    "table_column_names": [
      "ID LSTM-800",
      "5-fold CV 70.56",
      "Δ 0.66",
      "Single model 67.54",
      "Δ 0.78",
      "Ensemble 67.65",
      "Δ 0.30"
    ],
    "table_content_values": [
      [
        "LSTM-400",
        "70.50",
        "0.60",
        "[BOLD] 67.59",
        "0.83",
        "[BOLD] 68.00",
        "0.65"
      ],
      [
        "IN-TITLE",
        "70.11",
        "0.21",
        "[EMPTY]",
        "[EMPTY]",
        "67.52",
        "0.17"
      ],
      [
        "[BOLD] SUBMISSION",
        "69.90",
        "–",
        "66.76",
        "–",
        "67.35",
        "–"
      ],
      [
        "NO-HIGHWAY",
        "69.72",
        "−0.18",
        "66.42",
        "−0.34",
        "66.64",
        "−0.71"
      ],
      [
        "NO-OVERLAPS",
        "69.46",
        "−0.44",
        "65.07",
        "−1.69",
        "66.47",
        "−0.88"
      ],
      [
        "LSTM-400-DROPOUT",
        "69.45",
        "−0.45",
        "65.53",
        "−1.23",
        "67.28",
        "−0.07"
      ],
      [
        "NO-TRANSLATIONS",
        "69.42",
        "−0.48",
        "65.92",
        "−0.84",
        "67.23",
        "−0.12"
      ],
      [
        "NO-ELMO-FINETUNING",
        "67.71",
        "−2.19",
        "65.16",
        "−1.60",
        "65.42",
        "−1.93"
      ]
    ],
    "id": "cc888efa-44f2-4095-adad-3055c1539c12",
    "claim": "[CONTINUE] Apart of the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are sometimes associated with large discrepancies in test set performance.",
    "label": "supports",
    "table_id": "0980bab3-642c-413e-8b15-6abd868956a8"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.",
    "table_column_names": [
      "Model",
      "Encoder",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ",
      "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
      "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ",
      "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
      "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
    ],
    "table_content_values": [
      [
        "MLP",
        "CNN-RNN",
        ".311",
        ".340",
        ".486",
        ".532",
        ".318",
        ".335",
        ".481",
        ".524"
      ],
      [
        "MLP",
        "PMeans-RNN",
        ".313",
        ".331",
        ".489",
        ".536",
        ".354",
        ".375",
        ".502",
        ".556"
      ],
      [
        "MLP",
        "BERT",
        "[BOLD] .487",
        "[BOLD] .526",
        "[BOLD] .544",
        "[BOLD] .597",
        "[BOLD] .505",
        "[BOLD] .531",
        "[BOLD] .556",
        "[BOLD] .608"
      ],
      [
        "SimRed",
        "CNN",
        ".340",
        ".392",
        ".470",
        ".515",
        ".396",
        ".443",
        ".499",
        ".549"
      ],
      [
        "SimRed",
        "PMeans",
        ".354",
        ".393",
        ".493",
        ".541",
        ".370",
        ".374",
        ".507",
        ".551"
      ],
      [
        "SimRed",
        "BERT",
        ".266",
        ".296",
        ".458",
        ".495",
        ".325",
        ".338",
        ".485",
        ".533"
      ],
      [
        "Peyrard and Gurevych ( 2018 )",
        "Peyrard and Gurevych ( 2018 )",
        ".177",
        ".189",
        ".271",
        ".306",
        ".175",
        ".186",
        ".268",
        ".174"
      ]
    ],
    "id": "fd980e0e-8f17-437e-8ed0-1a121f78f7a3",
    "claim": "summary-level BLEU and REG are positively correlated with all metrics (Table 2) and all variants of the trained reward function, which implies that we can optimize our reinforcement learning framework with all existing reward functions",
    "label": "not enough info",
    "table_id": "0b75db2d-383c-4824-87cf-c844c0666ecd"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "cf8e09f8-f768-45a9-94df-7a749623df8a",
    "claim": "We consider all words that are semantically related to the words related to the story as negative samples",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "85ebe21b-ff40-4c23-b524-599d069dd7a5",
    "claim": "This indicates that our architecture cannot learn to generate better signals for text generation.",
    "label": "refutes",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2",
    "paper_id": "1809.00832v1",
    "table_caption": "Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
    "table_column_names": [
      "Batch size",
      "Throughput (instances/s) Balanced",
      "Throughput (instances/s) Moderate",
      "Throughput (instances/s) Linear"
    ],
    "table_content_values": [
      [
        "1",
        "46.7",
        "27.3",
        "7.6"
      ],
      [
        "10",
        "125.2",
        "78.2",
        "22.7"
      ],
      [
        "25",
        "129.7",
        "83.1",
        "45.4"
      ]
    ],
    "id": "0f052b57-c133-422e-be0d-97281f7665a3",
    "claim": "On the contrary, for the linear dataset, the recursive implementation efficiently makes use of CPU resources and thus the performance gain provided by increasing the batch size is relatively low.",
    "label": "refutes",
    "table_id": "6d2c8fe5-7707-42bd-a19c-bcaad90e8d1a"
  },
  {
    "paper": "Argument Generation with Retrieval, Planning, and Realization",
    "paper_id": "1906.03717v1",
    "table_caption": "Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
    "table_column_names": [
      "[EMPTY]",
      "[ITALIC] w/ System Retrieval  [BOLD] B-2",
      "[ITALIC] w/ System Retrieval  [BOLD] B-4",
      "[ITALIC] w/ System Retrieval  [BOLD] R-2",
      "[ITALIC] w/ System Retrieval  [BOLD] MTR",
      "[ITALIC] w/ System Retrieval  [BOLD] #Word",
      "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
      "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
    ],
    "table_content_values": [
      [
        "Human",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22",
        "-",
        "-",
        "-",
        "-",
        "66",
        "22"
      ],
      [
        "Retrieval",
        "7.55",
        "1.11",
        "8.64",
        "14.38",
        "123",
        "23",
        "10.97",
        "3.05",
        "23.49",
        "20.08",
        "140",
        "21"
      ],
      [
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[BOLD] Comparisons",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Seq2seq",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15",
        "6.92",
        "2.13",
        "13.02",
        "15.08",
        "68",
        "15"
      ],
      [
        "Seq2seqAug",
        "8.26",
        "2.24",
        "13.79",
        "15.75",
        "78",
        "14",
        "10.98",
        "4.41",
        "22.97",
        "19.62",
        "71",
        "14"
      ],
      [
        "[ITALIC] w/o psg",
        "7.94",
        "2.28",
        "10.13",
        "15.71",
        "75",
        "12",
        "9.89",
        "3.34",
        "14.20",
        "18.40",
        "66",
        "12"
      ],
      [
        "H&W Hua and Wang ( 2018 )",
        "3.64",
        "0.92",
        "8.83",
        "11.78",
        "51",
        "12",
        "8.51",
        "2.86",
        "18.89",
        "17.18",
        "58",
        "12"
      ],
      [
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[BOLD] Our Models",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "CANDELA",
        "12.02∗",
        "[BOLD] 2.99∗",
        "[BOLD] 14.93∗",
        "[BOLD] 16.92∗",
        "119",
        "22",
        "15.80∗",
        "[BOLD] 5.00∗",
        "[BOLD] 23.75",
        "[BOLD] 20.18",
        "116",
        "22"
      ],
      [
        "[ITALIC] w/o psg",
        "[BOLD] 12.33∗",
        "2.86∗",
        "14.53∗",
        "16.60∗",
        "123",
        "23",
        "[BOLD] 16.33∗",
        "4.98∗",
        "23.65",
        "19.94",
        "123",
        "23"
      ]
    ],
    "id": "dc829323-cd24-4c1e-a8aa-97dd288a0320",
    "claim": "Under oracle setup, all models are notably improved due to the higher quality of reranked passages, and our model achieves statistically significantly better BLEU scores.",
    "label": "supports",
    "table_id": "19cbc09e-9a44-4e2e-b755-e5e40d5cdaff"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 3: Results of Turn-level Evaluation.",
    "table_column_names": [
      "Dataset",
      "System",
      "Keyword Prediction  [ITALIC] Rw@1",
      "Keyword Prediction  [ITALIC] Rw@3",
      "Keyword Prediction  [ITALIC] Rw@5",
      "Keyword Prediction P@1",
      "Response Retrieval  [ITALIC] R20@1",
      "Response Retrieval  [ITALIC] R20@3",
      "Response Retrieval  [ITALIC] R20@5",
      "Response Retrieval MRR"
    ],
    "table_content_values": [
      [
        "TGPC",
        "Retrieval ",
        "-",
        "-",
        "-",
        "-",
        "0.5063",
        "0.7615",
        "0.8676",
        "0.6589"
      ],
      [
        "TGPC",
        "PMI ",
        "0.0585",
        "0.1351",
        "0.1872",
        "0.0871",
        "0.5441",
        "0.7839",
        "0.8716",
        "0.6847"
      ],
      [
        "TGPC",
        "Neural ",
        "0.0708",
        "0.1438",
        "0.1820",
        "0.1321",
        "0.5311",
        "0.7905",
        "0.8800",
        "0.6822"
      ],
      [
        "TGPC",
        "Kernel ",
        "0.0632",
        "0.1377",
        "0.1798",
        "0.1172",
        "0.5386",
        "0.8012",
        "0.8924",
        "0.6877"
      ],
      [
        "TGPC",
        "DKRN (ours)",
        "[BOLD] 0.0909",
        "[BOLD] 0.1903",
        "[BOLD] 0.2477",
        "[BOLD] 0.1685",
        "[BOLD] 0.5729",
        "[BOLD] 0.8132",
        "[BOLD] 0.8966",
        "[BOLD] 0.7110"
      ],
      [
        "CWC",
        "Retrieval ",
        "-",
        "-",
        "-",
        "-",
        "0.5785",
        "0.8101",
        "0.8999",
        "0.7141"
      ],
      [
        "CWC",
        "PMI ",
        "0.0555",
        "0.1001",
        "0.1212",
        "0.0969",
        "0.5945",
        "0.8185",
        "0.9054",
        "0.7257"
      ],
      [
        "CWC",
        "Neural ",
        "0.0654",
        "0.1194",
        "0.1450",
        "0.1141",
        "0.6044",
        "0.8233",
        "0.9085",
        "0.7326"
      ],
      [
        "CWC",
        "Kernel ",
        "0.0592",
        "0.1113",
        "0.1337",
        "0.1011",
        "0.6017",
        "0.8234",
        "0.9087",
        "0.7320"
      ],
      [
        "CWC",
        "DKRN (ours)",
        "[BOLD] 0.0680",
        "[BOLD] 0.1254",
        "[BOLD] 0.1548",
        "[BOLD] 0.1185",
        "[BOLD] 0.6324",
        "[BOLD] 0.8416",
        "[BOLD] 0.9183",
        "[BOLD] 0.7533"
      ]
    ],
    "id": "a22b9660-188b-4c29-b248-c811154705b7",
    "claim": "Our approach DKRN does not outperform all state-of-the-art methods in terms of all metrics on both datasets with two tasks.",
    "label": "refutes",
    "table_id": "907466a1-844b-4d00-9d93-5667496de4b9"
  },
  {
    "paper": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
    "paper_id": "1809.09078v2",
    "table_caption": "Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.",
    "table_column_names": [
      "[BOLD] Method",
      "[BOLD] Trigger  [BOLD] Identification (%)",
      "[BOLD] Trigger  [BOLD] Identification (%)",
      "[BOLD] Trigger  [BOLD] Identification (%)",
      "[BOLD] Trigger  [BOLD] Classification (%)",
      "[BOLD] Trigger  [BOLD] Classification (%)",
      "[BOLD] Trigger  [BOLD] Classification (%)",
      "[BOLD] Argument  [BOLD] Identification (%)",
      "[BOLD] Argument  [BOLD] Identification (%)",
      "[BOLD] Argument  [BOLD] Identification (%)",
      "[BOLD] Argument  [BOLD] Role (%)",
      "[BOLD] Argument  [BOLD] Role (%)",
      "[BOLD] Argument  [BOLD] Role (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] Method",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1",
        "[ITALIC] P",
        "[ITALIC] R",
        "[ITALIC] F1"
      ],
      [
        "Cross-Event",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "68.7",
        "68.9",
        "68.8",
        "50.9",
        "49.7",
        "50.3",
        "45.1",
        "44.1",
        "44.6"
      ],
      [
        "JointBeam",
        "76.9",
        "65.0",
        "70.4",
        "73.7",
        "62.3",
        "67.5",
        "69.8",
        "47.9",
        "56.8",
        "64.7",
        "44.4",
        "52.7"
      ],
      [
        "DMCNN",
        "[BOLD] 80.4",
        "67.7",
        "73.5",
        "75.6",
        "63.6",
        "69.1",
        "68.8",
        "51.9",
        "59.1",
        "62.2",
        "46.9",
        "53.5"
      ],
      [
        "PSL",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "75.3",
        "64.4",
        "69.4",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "JRNN",
        "68.5",
        "[BOLD] 75.7",
        "71.9",
        "66.0",
        "[BOLD] 73.0",
        "69.3",
        "61.4",
        "64.2",
        "62.8",
        "54.2",
        "56.7",
        "55.4"
      ],
      [
        "dbRNN",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "74.1",
        "69.8",
        "71.9",
        "71.3",
        "64.5",
        "67.7",
        "66.2",
        "52.8",
        "58.7"
      ],
      [
        "[BOLD] JMEE",
        "80.2",
        "72.1",
        "[BOLD] 75.9",
        "[BOLD] 76.3",
        "71.3",
        "[BOLD] 73.7",
        "[BOLD] 71.4",
        "[BOLD] 65.6",
        "[BOLD] 68.4",
        "[BOLD] 66.8",
        "[BOLD] 54.9",
        "[BOLD] 60.3"
      ]
    ],
    "id": "0018d644-5832-4e3f-ac9b-9b6069ff5550",
    "claim": "From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods.",
    "label": "supports",
    "table_id": "cb128f63-9ce7-4d32-ba3b-4924dc7b8b7d"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
    "table_column_names": [
      "[BOLD] Model",
      "D",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "DCGCN(1)",
        "300",
        "10.9M",
        "20.9",
        "52.0"
      ],
      [
        "DCGCN(2)",
        "180",
        "10.9M",
        "[BOLD] 22.2",
        "[BOLD] 52.3"
      ],
      [
        "DCGCN(2)",
        "240",
        "11.3M",
        "22.8",
        "52.8"
      ],
      [
        "DCGCN(4)",
        "180",
        "11.4M",
        "[BOLD] 23.4",
        "[BOLD] 53.4"
      ],
      [
        "DCGCN(1)",
        "420",
        "12.6M",
        "22.2",
        "52.4"
      ],
      [
        "DCGCN(2)",
        "300",
        "12.5M",
        "23.8",
        "53.8"
      ],
      [
        "DCGCN(3)",
        "240",
        "12.3M",
        "[BOLD] 23.9",
        "[BOLD] 54.1"
      ],
      [
        "DCGCN(2)",
        "360",
        "14.0M",
        "24.2",
        "[BOLD] 54.4"
      ],
      [
        "DCGCN(3)",
        "300",
        "14.0M",
        "[BOLD] 24.4",
        "54.2"
      ],
      [
        "DCGCN(2)",
        "420",
        "15.6M",
        "24.1",
        "53.7"
      ],
      [
        "DCGCN(4)",
        "300",
        "15.6M",
        "[BOLD] 24.6",
        "[BOLD] 54.8"
      ],
      [
        "DCGCN(3)",
        "420",
        "18.6M",
        "24.5",
        "54.6"
      ],
      [
        "DCGCN(4)",
        "360",
        "18.4M",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "0c917759-6018-4282-9826-73b13410d748",
    "claim": "For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN1 obtains 20.9 BLEU points, which is higher than DCGCN2 (22.2).",
    "label": "refutes",
    "table_id": "135bc50f-f12e-493b-854d-58858c4c5c86"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "b3e222d9-7bea-433d-bfad-a5e18e07af19",
    "claim": "Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is not necessarily obtained when using TVMAX in the output attention.",
    "label": "refutes",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Low-supervision urgency detection and transfer in short crisis messages",
    "paper_id": "1907.06745v1",
    "table_caption": "TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
    "table_column_names": [
      "System",
      "Accuracy",
      "Precision",
      "Recall",
      "F-Measure"
    ],
    "table_content_values": [
      [
        "Local",
        "63.97%",
        "64.27%",
        "64.50%",
        "63.93%"
      ],
      [
        "Manual",
        "64.25%",
        "[BOLD] 70.84%∗∗",
        "48.50%",
        "57.11%"
      ],
      [
        "Wiki",
        "67.25%",
        "66.51%",
        "69.50%",
        "67.76%"
      ],
      [
        "Local-Manual",
        "65.75%",
        "67.96%",
        "59.50%",
        "62.96%"
      ],
      [
        "Wiki-Local",
        "67.40%",
        "65.54%",
        "68.50%",
        "66.80%"
      ],
      [
        "Wiki-Manual",
        "67.75%",
        "70.38%",
        "63.00%",
        "65.79%"
      ],
      [
        "[ITALIC] Our Approach",
        "[BOLD] 69.25%∗∗∗",
        "68.76%",
        "[BOLD] 70.50%∗∗",
        "[BOLD] 69.44%∗∗∗"
      ]
    ],
    "id": "4495c767-a361-43b9-8ebb-0290a8013b03",
    "claim": "The results illustrate the lack of viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics.",
    "label": "refutes",
    "table_id": "678b9f33-65b1-4b0a-b4f6-96de47883169"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "5d062ac6-288d-442b-93c9-4946442eb48e",
    "claim": "our approach reliably identifies meanings to sentences that are otherwise challenging even to humans.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Domain Adaptive Inference for Neural Machine Translation",
    "paper_id": "1906.00408v1",
    "table_caption": "Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
    "table_column_names": [
      "[BOLD] Language pair",
      "[BOLD] Model type",
      "[BOLD] Oracle model",
      "[BOLD] Decoder configuration  [BOLD] Uniform",
      "[BOLD] Decoder configuration  [BOLD] BI + IS"
    ],
    "table_content_values": [
      [
        "es-en",
        "Unadapted",
        "36.4",
        "34.7",
        "36.6"
      ],
      [
        "es-en",
        "No-reg",
        "36.6",
        "34.8",
        "-"
      ],
      [
        "es-en",
        "EWC",
        "37.0",
        "36.3",
        "[BOLD] 37.2"
      ],
      [
        "en-de",
        "Unadapted",
        "36.4",
        "26.8",
        "38.8"
      ],
      [
        "en-de",
        "No-reg",
        "41.7",
        "31.8",
        "-"
      ],
      [
        "en-de",
        "EWC",
        "42.1",
        "38.6",
        "[BOLD] 42.0"
      ]
    ],
    "id": "70c9a077-aea5-467e-97cd-520f06da7cd4",
    "claim": "Uniform no-reg ensembling outperforms unadapted uniform ensembling, since fine-tuning gives better in-domain performance.",
    "label": "supports",
    "table_id": "a02493f5-adad-4462-a582-8a2cfe6f431d"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).",
    "table_column_names": [
      "Model",
      "#Params",
      "Base",
      "+Elmo"
    ],
    "table_content_values": [
      [
        "rnet*",
        "-",
        "71.1/79.5",
        "-/-"
      ],
      [
        "LSTM",
        "2.67M",
        "[BOLD] 70.46/78.98",
        "75.17/82.79"
      ],
      [
        "GRU",
        "2.31M",
        "70.41/ [BOLD] 79.15",
        "75.81/83.12"
      ],
      [
        "ATR",
        "1.59M",
        "69.73/78.70",
        "75.06/82.76"
      ],
      [
        "SRU",
        "2.44M",
        "69.27/78.41",
        "74.56/82.50"
      ],
      [
        "LRN",
        "2.14M",
        "70.11/78.83",
        "[BOLD] 76.14/ [BOLD] 83.83"
      ]
    ],
    "id": "550e39a7-dda1-4dac-ac92-76636384b64b",
    "claim": "After integrating Elmo for contextual modeling, the performance of LRN reaches the best (76.1 [CONTINUE] EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1).",
    "label": "supports",
    "table_id": "e8718aba-7d2a-43da-ab9f-fe42d951b94a"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.",
    "table_column_names": [
      "[EMPTY]",
      "Ours",
      "Refresh",
      "ExtAbsRL"
    ],
    "table_content_values": [
      [
        "Avg. Human Rating",
        "[BOLD] 2.52",
        "2.27",
        "1.66"
      ],
      [
        "Best%",
        "[BOLD] 70.0",
        "33.3",
        "6.7"
      ]
    ],
    "id": "534a5798-3961-4596-b9a1-5612d65668aa",
    "claim": "our model achieves better scores in both BLEU and METEOR scores in general, with the largest improvements especially seen in specific categories like “geography” and “people”.",
    "label": "not enough info",
    "table_id": "5d2ca6d4-e540-491b-8ac7-b1fe963dcabb"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "1807ab12-124f-4a77-9ec3-99844ee78da9",
    "claim": "the feature engineering approach only achieved an average of 0.52 F1 score.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "40c5898e-fbfc-4961-b2ec-1a2a06a58791",
    "claim": "the low performance of to can be explained by the fact that as shown in the first part of Table 2, it is responsible for only 4.6% of the inference in the training set.",
    "label": "not enough info",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "30481648-b1a9-4760-9e23-c354ea992432",
    "claim": "System A is our new system trained with all data.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "feb01e4b-e4e7-4f6c-a264-d2fb2e3f1962",
    "claim": "Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is obtained when using TVMAX in the output attention.",
    "label": "supports",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "a2ea71dc-298d-49ba-8699-c91fbcf5159c",
    "claim": "More importantly, their G-Pre and G-Rec scores are all below .50, which means that more than half of the good summaries identified by the metrics are actually not good, and more than 50%",
    "label": "supports",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
    "table_column_names": [
      "System",
      "MUC",
      "BCUB",
      "CEAFe",
      "AVG"
    ],
    "table_content_values": [
      [
        "ACE",
        "ACE",
        "ACE",
        "ACE",
        "ACE"
      ],
      [
        "IlliCons",
        "[BOLD] 78.17",
        "81.64",
        "[BOLD] 78.45",
        "[BOLD] 79.42"
      ],
      [
        "KnowComb",
        "77.51",
        "[BOLD] 81.97",
        "77.44",
        "78.97"
      ],
      [
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes",
        "OntoNotes"
      ],
      [
        "IlliCons",
        "84.10",
        "[BOLD] 78.30",
        "[BOLD] 68.74",
        "[BOLD] 77.05"
      ],
      [
        "KnowComb",
        "[BOLD] 84.33",
        "78.02",
        "67.95",
        "76.76"
      ]
    ],
    "id": "9e01d648-e9ad-493e-979e-f695d4f329f4",
    "claim": "Our KnowComb system achieves the same level of performance as does the state-of-art general coreference system we base it on.",
    "label": "supports",
    "table_id": "cbbb2b74-fff2-4db8-a6be-b6a395d77483"
  },
  {
    "paper": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation",
    "paper_id": "2002.01196v2",
    "table_caption": "Table 4: Results of Self-Play Evaluation.",
    "table_column_names": [
      "System",
      "TGPC Succ. (%)",
      "TGPC #Turns",
      "CWC Succ. (%)",
      "CWC #Turns"
    ],
    "table_content_values": [
      [
        "Retrieval ",
        "7.16",
        "4.17",
        "0",
        "-"
      ],
      [
        "Retrieval-Stgy ",
        "47.80",
        "6.7",
        "44.6",
        "7.42"
      ],
      [
        "PMI ",
        "35.36",
        "6.38",
        "47.4",
        "5.29"
      ],
      [
        "Neural ",
        "54.76",
        "4.73",
        "47.6",
        "5.16"
      ],
      [
        "Kernel ",
        "62.56",
        "4.65",
        "53.2",
        "4.08"
      ],
      [
        "DKRN (ours)",
        "[BOLD] 89.0",
        "5.02",
        "[BOLD] 84.4",
        "4.20"
      ]
    ],
    "id": "99b4876b-e7fd-48d6-b96a-d9f0c2fedb05",
    "claim": "Although the average number of turns of our approach is slightly more than Kernel, the success rate of our system is not significantly better than other approaches.",
    "label": "refutes",
    "table_id": "5a8c9b40-6ff6-44b7-af2e-4bf75b46492f"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "b598c64e-ca52-4716-a6b8-4c78cbbc2195",
    "claim": "HAN models do not outperform both LogReg and SVM using the current set of features.",
    "label": "refutes",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "c2b01824-1085-4df5-8d14-5a2f4d99c3fd",
    "claim": "multi-turn models, who need to produce more than one sentence for each dialog turn, are disadvantaged in comparison to single-turn models which only need to generate a single sentence at a time, because there are always bad sentences in a dialog session which might always ruin the performance of multi-turn models.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "cc1fb307-7edd-4eb4-bd7b-4eb8b22825ea",
    "claim": "Table 1) and crashes less frequently than all the baseline methods.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "ff17d252-6aaa-4f84-b667-e7a4e533743c",
    "claim": "NeuralTD achieves comparable performances to state-of-the-art approaches while utilising a significantly simpler and lower-cost learning process with only a small quality drop, which we attribute to the reliance on an imperfect summary evaluation function",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "2c2a4d97-aacc-474f-a381-6fb70c1daa1f",
    "claim": "On the TREC task, CBOW outperforms CMOW by 2.3 points.",
    "label": "refutes",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "3f49d5e4-4226-44ed-b64b-6fd8e62e1cc5",
    "claim": "G2S-GGNN has 33.5% and 5.2% worse entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively.",
    "label": "refutes",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 2: Ratings of annotated NLDs by human judges.",
    "table_column_names": [
      "# steps",
      "Reachability",
      "Derivability Step 1",
      "Derivability Step 2",
      "Derivability Step 3"
    ],
    "table_content_values": [
      [
        "1",
        "3.0",
        "3.8",
        "-",
        "-"
      ],
      [
        "2",
        "2.8",
        "3.8",
        "3.7",
        "-"
      ],
      [
        "3",
        "2.3",
        "3.9",
        "3.8",
        "3.8"
      ]
    ],
    "id": "6684d294-b666-4718-affe-953ad1c47f8b",
    "claim": "The evaluation results shown in Table 2 indicate that the annotated NLDs are of low quality (Reachability), and each NLD is not properly derived from supporting documents (Derivability).",
    "label": "refutes",
    "table_id": "8326a179-f543-4f97-8229-d2ed8172a663"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
    "table_column_names": [
      "[BOLD] Training data",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] Disfl"
    ],
    "table_content_values": [
      [
        "Original",
        "0",
        "22",
        "0",
        "14"
      ],
      [
        "Cleaned added",
        "0",
        "23",
        "0",
        "14"
      ],
      [
        "Cleaned missing",
        "0",
        "1",
        "0",
        "2"
      ],
      [
        "Cleaned",
        "0",
        "0",
        "0",
        "5"
      ]
    ],
    "id": "3af33c66-1cff-404c-be5f-b3aa9c3b3cf4",
    "claim": "The results in Table 4 refute the findings of the automatic metrics: systems trained on the fully cleaned set or the set with cleaned missing slots do not have nearperfect performance, with the fully-cleaned one showing more errors than the other.",
    "label": "refutes",
    "table_id": "82ce68c2-64df-452b-ac0b-064c7fdaefa8"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.",
    "table_column_names": [
      "GP-MBCM",
      "ACER",
      "PPO",
      "ALDM",
      "GDPL"
    ],
    "table_content_values": [
      [
        "1.666",
        "0.775",
        "0.639",
        "1.069",
        "[BOLD] 0.238"
      ]
    ],
    "id": "bdc1f2cb-09b4-4561-a6d4-1f69c0ee67d1",
    "claim": "These poor conversational performances are reflected in a more diverse KL-divergence scores.",
    "label": "not enough info",
    "table_id": "07ae1f89-4b1a-46c3-a9f1-73dbc9ac574d"
  },
  {
    "paper": "A Lightweight Recurrent Network for Sequence Modeling",
    "paper_id": "1905.13324v1",
    "table_caption": "Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
    "table_column_names": [
      "Model",
      "#Params",
      "BLEU",
      "Train",
      "Decode"
    ],
    "table_content_values": [
      [
        "GNMT",
        "-",
        "24.61",
        "-",
        "-"
      ],
      [
        "GRU",
        "206M",
        "26.28",
        "2.67",
        "45.35"
      ],
      [
        "ATR",
        "122M",
        "25.70",
        "1.33",
        "[BOLD] 34.40"
      ],
      [
        "SRU",
        "170M",
        "25.91",
        "1.34",
        "42.84"
      ],
      [
        "LRN",
        "143M",
        "26.26",
        "[BOLD] 0.99",
        "36.50"
      ],
      [
        "oLRN",
        "164M",
        "[BOLD] 26.73",
        "1.15",
        "40.19"
      ]
    ],
    "id": "232b4447-1be4-49ce-afe3-5bed802143ac",
    "claim": "In addition, the training time results in Table 3 confirm the computational advantage of LRN over all other recurrent units, where LRN speeds up over ATR and SRU by approximately 25%.",
    "label": "supports",
    "table_id": "114a56ee-4121-4202-aa27-f28278f03855"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "755e5f3b-ed26-43b5-8320-cc5853b2b815",
    "claim": "by averaging the column results, we can see that a pure effect of coverage appears on out-of-domain tasks, as applying it improves the performance of the standard models by an average of 7.02% in accuracy.",
    "label": "not enough info",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "3caabd87-48f9-4343-b91c-d8b677797b0e",
    "claim": "A complementary behavior can be observed for H-CBOW, whose scores on Word Content are decreased.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "714fe045-6b79-4ff5-81dc-e4b5434e1d66",
    "claim": "HDSA with a fixed threshold achieves higher values than DAMD with a sampled threshold because actions are easy to predict with a fixed threshold, even for a random policy, as there are only about 5-6 actions for each state",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "c9a135e8-f977-46e0-a2cb-10737ce5245c",
    "claim": "We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels.",
    "label": "supports",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
    "table_column_names": [
      "Type",
      "Inform Mean",
      "Inform Num",
      "Match Mean",
      "Match Num",
      "Success Mean",
      "Success Num"
    ],
    "table_content_values": [
      [
        "Full",
        "8.413",
        "903",
        "10.59",
        "450",
        "11.18",
        "865"
      ],
      [
        "Other",
        "-99.95",
        "76",
        "-48.15",
        "99",
        "-71.62",
        "135"
      ]
    ],
    "id": "fe0e9c2a-c10b-4bbf-a94c-20cf7b2bc9b9",
    "claim": "we further evaluate the inform, match and success of the predictions under 50% threshold and show them in Fig.",
    "label": "not enough info",
    "table_id": "48fbdbdb-8721-4b0c-a75c-165090b71ebe"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "84e16b2a-6ae0-428b-811e-962ffd7c0381",
    "claim": "these metrics generally are ineffective in capturing the semantic similarity of multiple documents: the pearsons correlation between them and human judgments is below .3, and their r is often negative or 0.",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "92d74cff-f9f2-4fb1-adfb-4a10df43de9a",
    "claim": "In contrast, our DCGCN models can be trained using a large number of layers.",
    "label": "supports",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
    "paper_id": "1705.02925v1",
    "table_caption": "Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] PPA Acc."
    ],
    "table_content_values": [
      [
        "full",
        "89.7"
      ],
      [
        "- sense priors",
        "88.4"
      ],
      [
        "- attention",
        "87.5"
      ]
    ],
    "id": "29dedc61-018c-4f4a-985b-5998ec32dbe4",
    "claim": "The second row in Table 3 shows the test accuracy of a system trained without sense priors and the third row shows that removing attention from the model actually improved the accuracy, suggesting that context sensitivity is not necessary for good performance.",
    "label": "refutes",
    "table_id": "6f5e49e4-970d-41d9-99e5-fe58653ea248"
  },
  {
    "paper": "Adversarial Removal of Demographic Attributes from Text Data",
    "paper_id": "1808.06640v2",
    "table_caption": "Table 3: Performances on different datasets with an adversarial training. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.",
    "table_column_names": [
      "Data",
      "Task",
      "Protected Attribute",
      "Task Acc",
      "Leakage",
      "Δ"
    ],
    "table_content_values": [
      [
        "Dial",
        "Sentiment",
        "Race",
        "64.7",
        "56.0",
        "5.0"
      ],
      [
        "[EMPTY]",
        "Mention",
        "Race",
        "81.5",
        "63.1",
        "9.2"
      ],
      [
        "PAN16",
        "Mention",
        "Gender",
        "75.6",
        "58.5",
        "8.0"
      ],
      [
        "[EMPTY]",
        "Mention",
        "Age",
        "72.5",
        "57.3",
        "6.9"
      ]
    ],
    "id": "38158ec1-ed12-4527-92c5-00f4c3f9312c",
    "claim": "In all cases, the adversarial's success rate is around 50%, while the attacker's rate is substantially higher.",
    "label": "supports",
    "table_id": "502870a9-3417-4da3-a647-1a5db2abc4ec"
  },
  {
    "paper": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    "paper_id": "1801.07772v1",
    "table_caption": "Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
    "table_column_names": [
      "Uni",
      "POS",
      "0 87.9",
      "1 92.0",
      "2 91.7",
      "3 91.8",
      "4 91.9"
    ],
    "table_content_values": [
      [
        "Uni",
        "SEM",
        "81.8",
        "87.8",
        "87.4",
        "87.6",
        "88.2"
      ],
      [
        "Bi",
        "POS",
        "87.9",
        "93.3",
        "92.9",
        "93.2",
        "92.8"
      ],
      [
        "Bi",
        "SEM",
        "81.9",
        "91.3",
        "90.8",
        "91.9",
        "91.9"
      ],
      [
        "Res",
        "POS",
        "87.9",
        "92.5",
        "91.9",
        "92.0",
        "92.4"
      ],
      [
        "Res",
        "SEM",
        "81.9",
        "88.2",
        "87.5",
        "87.6",
        "88.5"
      ]
    ],
    "id": "ef361478-7869-46cf-b2dd-3c136f8ec77a",
    "claim": "We observe that POS tagging does benefit from features from the upper layers, while SEM tagging does not improve with layer 4 representations.",
    "label": "refutes",
    "table_id": "56a61e48-903a-4dc4-8cbf-2048d2c8ee3c"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 2: Human evaluation results on MSCOCO.",
    "table_column_names": [
      "[EMPTY]",
      "caption",
      "attention relevance"
    ],
    "table_content_values": [
      [
        "softmax",
        "3.50",
        "3.38"
      ],
      [
        "sparsemax",
        "3.71",
        "3.89"
      ],
      [
        "TVmax",
        "[BOLD] 3.87",
        "[BOLD] 4.10"
      ]
    ],
    "id": "ee25e11d-4b14-4114-8977-22a22aa4b799",
    "claim": "The superior score on attention relevance shows that TVMAX is better at selecting the relevant features and its output is more interpretable.",
    "label": "supports",
    "table_id": "77097db2-3bbc-4070-a3a0-2eaad08e47de"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.",
    "table_column_names": [
      "[BOLD] GCN +RC (2)",
      "B 16.8",
      "C 48.1",
      "[BOLD] GCN +RC+LA (2)",
      "B 18.3",
      "C 47.9"
    ],
    "table_content_values": [
      [
        "+RC (4)",
        "18.4",
        "49.6",
        "+RC+LA (4)",
        "18.0",
        "51.1"
      ],
      [
        "+RC (6)",
        "19.9",
        "49.7",
        "+RC+LA (6)",
        "21.3",
        "50.8"
      ],
      [
        "+RC (9)",
        "[BOLD] 21.1",
        "50.5",
        "+RC+LA (9)",
        "[BOLD] 22.0",
        "52.6"
      ],
      [
        "+RC (10)",
        "20.7",
        "[BOLD] 50.7",
        "+RC+LA (10)",
        "21.2",
        "[BOLD] 52.9"
      ],
      [
        "DCGCN1 (9)",
        "22.9",
        "53.0",
        "DCGCN3 (27)",
        "24.8",
        "54.7"
      ],
      [
        "DCGCN2 (18)",
        "24.2",
        "54.4",
        "DCGCN4 (36)",
        "[BOLD] 25.5",
        "[BOLD] 55.4"
      ]
    ],
    "id": "72ae36b8-bb32-4a0d-9f55-10b95d234b1c",
    "claim": "For example, DCGCN4 contains 36 layers and has the lowest performance on both datasets.",
    "label": "refutes",
    "table_id": "7f471c1c-4d51-4559-87fd-0c4b8514b3d2"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
    "table_column_names": [
      "[BOLD] Training data",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] Disfl"
    ],
    "table_content_values": [
      [
        "Original",
        "0",
        "22",
        "0",
        "14"
      ],
      [
        "Cleaned added",
        "0",
        "23",
        "0",
        "14"
      ],
      [
        "Cleaned missing",
        "0",
        "1",
        "0",
        "2"
      ],
      [
        "Cleaned",
        "0",
        "0",
        "0",
        "5"
      ]
    ],
    "id": "68a6fee8-086e-407d-9604-300988692905",
    "claim": "The systems trained on the original data or with cleaned added slots perform better in terms of both semantic accuracy and fluency.",
    "label": "refutes",
    "table_id": "82ce68c2-64df-452b-ac0b-064c7fdaefa8"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "18b3279b-d503-4a14-90e2-b9f94c857026",
    "claim": "Compared with the fixed threshold, the sampled threshold surprisingly gives higher Bleu score but worse TER score, especially on the 10-action task.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Sparse and Structured Visual Attention",
    "paper_id": "2002.05556v1",
    "table_caption": "Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
    "table_column_names": [
      "[EMPTY]",
      "Att. to image",
      "Att. to bounding boxes",
      "Test-Dev Yes/No",
      "Test-Dev Number",
      "Test-Dev Other",
      "Test-Dev Overall",
      "Test-Standard Yes/No",
      "Test-Standard Number",
      "Test-Standard Other",
      "Test-Standard Overall"
    ],
    "table_content_values": [
      [
        "softmax",
        "✓",
        "[EMPTY]",
        "83.08",
        "42.65",
        "55.74",
        "65.52",
        "83.55",
        "42.68",
        "56.01",
        "65.97"
      ],
      [
        "sparsemax",
        "✓",
        "[EMPTY]",
        "83.08",
        "43.19",
        "55.79",
        "65.60",
        "83.33",
        "42.99",
        "56.06",
        "65.94"
      ],
      [
        "soft-TVmax",
        "✓",
        "[EMPTY]",
        "83.13",
        "43.53",
        "56.01",
        "65.76",
        "83.63",
        "43.24",
        "56.10",
        "66.11"
      ],
      [
        "sparse-TVmax",
        "✓",
        "[EMPTY]",
        "83.10",
        "43.30",
        "56.14",
        "65.79",
        "83.66",
        "43.18",
        "56.21",
        "66.17"
      ],
      [
        "softmax",
        "[EMPTY]",
        "✓",
        "85.14",
        "49.59",
        "58.72",
        "68.57",
        "85.56",
        "49.54",
        "59.11",
        "69.04"
      ],
      [
        "sparsemax",
        "[EMPTY]",
        "✓",
        "[BOLD] 85.40",
        "[BOLD] 50.87",
        "58.67",
        "68.79",
        "[BOLD] 85.80",
        "50.18",
        "59.08",
        "69.19"
      ],
      [
        "softmax",
        "✓",
        "✓",
        "85.33",
        "50.49",
        "58.88",
        "68.82",
        "85.58",
        "50.42",
        "59.18",
        "69.17"
      ],
      [
        "sparse-TVmax",
        "✓",
        "✓",
        "85.35",
        "50.52",
        "[BOLD] 59.15",
        "[BOLD] 68.96",
        "85.72",
        "[BOLD] 50.66",
        "[BOLD] 59.22",
        "[BOLD] 69.28"
      ]
    ],
    "id": "b0a81f4a-88ce-4478-bbbc-c06f440ff7db",
    "claim": "Additionally, when using bounding box features, softmax outperforms sparsemax, showing that selecting only the bounding boxes of the relevant objects does not lead to a better answering capability.",
    "label": "refutes",
    "table_id": "a78f9beb-9bfa-4f24-9847-a8ffc97882a4"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 4: Cue classification on the test set.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] F-Score  [BOLD] Baseline",
      "[BOLD] F-Score  [BOLD] Proposed",
      "[BOLD] Support"
    ],
    "table_content_values": [
      [
        "False cues",
        "0.61",
        "0.68",
        "47"
      ],
      [
        "Actual cues",
        "0.97",
        "0.98",
        "557"
      ]
    ],
    "id": "870227ae-f7f9-41e8-a840-f22a92847b02",
    "claim": "The proposed architecture achieves a 0.04% improvement over the baseline system with binary classification, and achieves 99.5% precision in true cues.",
    "label": "not enough info",
    "table_id": "08a8fe8c-92a2-41dc-a6e0-f97b95380cae"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>BLEU</bold>",
      "<bold>METEOR</bold>"
    ],
    "table_content_values": [
      [
        "LDC2015E86",
        "LDC2015E86",
        "LDC2015E86"
      ],
      [
        "Konstas et al. (2017)",
        "22.00",
        "-"
      ],
      [
        "Song et al. (2018)",
        "23.28",
        "30.10"
      ],
      [
        "Cao et al. (2019)",
        "23.50",
        "-"
      ],
      [
        "Damonte et al.(2019)",
        "24.40",
        "23.60"
      ],
      [
        "Guo et al. (2019)",
        "<bold>25.70</bold>",
        "-"
      ],
      [
        "S2S",
        "22.55 ± 0.17",
        "29.90 ± 0.31"
      ],
      [
        "G2S-GIN",
        "22.93 ± 0.20",
        "29.72 ± 0.09"
      ],
      [
        "G2S-GAT",
        "23.42 ± 0.16",
        "29.87 ± 0.14"
      ],
      [
        "G2S-GGNN",
        "24.32 ± 0.16",
        "<bold>30.53</bold> ± 0.30"
      ],
      [
        "LDC2017T10",
        "LDC2017T10",
        "LDC2017T10"
      ],
      [
        "Back et al. (2018)",
        "23.30",
        "-"
      ],
      [
        "Song et al. (2018)",
        "24.86",
        "31.56"
      ],
      [
        "Damonte et al.(2019)",
        "24.54",
        "24.07"
      ],
      [
        "Cao et al. (2019)",
        "26.80",
        "-"
      ],
      [
        "Guo et al. (2019)",
        "27.60",
        "-"
      ],
      [
        "S2S",
        "22.73 ± 0.18",
        "30.15 ± 0.14"
      ],
      [
        "G2S-GIN",
        "26.90 ± 0.19",
        "32.62 ± 0.04"
      ],
      [
        "G2S-GAT",
        "26.72 ± 0.20",
        "32.52 ± 0.02"
      ],
      [
        "G2S-GGNN",
        "<bold>27.87</bold> ± 0.15",
        "<bold>33.21</bold> ± 0.15"
      ]
    ],
    "id": "4a2c8295-40a1-49a0-9ac3-4d2275c74349",
    "claim": "This indicates that our architecture can learn to generate better signals for text generation.",
    "label": "supports",
    "table_id": "57aa005a-e310-4a04-8cfa-5f7e292b2148"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
    "table_column_names": [
      "VS.",
      "Efficiency W",
      "Efficiency D",
      "Efficiency L",
      "Quality W",
      "Quality D",
      "Quality L",
      "Success W",
      "Success D",
      "Success L"
    ],
    "table_content_values": [
      [
        "ACER",
        "55",
        "25",
        "20",
        "44",
        "32",
        "24",
        "52",
        "30",
        "18"
      ],
      [
        "PPO",
        "74",
        "13",
        "13",
        "56",
        "26",
        "18",
        "59",
        "31",
        "10"
      ],
      [
        "ALDM",
        "69",
        "19",
        "12",
        "49",
        "25",
        "26",
        "61",
        "24",
        "15"
      ]
    ],
    "id": "8e82ff01-e164-4739-b890-f28e353b3a47",
    "claim": "In other words, it has the strongest tendency to predict dialog state transition accurately.",
    "label": "not enough info",
    "table_id": "a3182204-1785-4465-8e3c-506be0edf504"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "8b58e919-ddcb-4f76-85ef-dfeb17d817a7",
    "claim": "This shows that using additional information about the word locations would help to gain a better generalization across the datasets.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 4: Main results on English-German and English-Czech datasets.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Type",
      "[BOLD] English-German #P",
      "[BOLD] English-German B",
      "[BOLD] English-German C",
      "[BOLD] English-Czech #P",
      "[BOLD] English-Czech B",
      "[BOLD] English-Czech C"
    ],
    "table_content_values": [
      [
        "BoW+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "12.2",
        "-",
        "-",
        "7.5",
        "-"
      ],
      [
        "CNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "13.7",
        "-",
        "-",
        "8.7",
        "-"
      ],
      [
        "BiRNN+GCN (Bastings et al.,  2017 )",
        "Single",
        "-",
        "16.1",
        "-",
        "-",
        "9.6",
        "-"
      ],
      [
        "PB-SMT (Beck et al.,  2018 )",
        "Single",
        "-",
        "12.8",
        "43.2",
        "-",
        "8.6",
        "36.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Single",
        "41.4M",
        "15.5",
        "40.8",
        "39.1M",
        "8.9",
        "33.8"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Single",
        "41.2M",
        "16.7",
        "42.4",
        "38.8M",
        "9.8",
        "33.3"
      ],
      [
        "DCGCN (ours)",
        "Single",
        "[BOLD]  29.7M",
        "[BOLD] 19.0",
        "[BOLD] 44.1",
        "[BOLD]  28.3M",
        "[BOLD] 12.1",
        "[BOLD] 37.1"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "Ensemble",
        "207M",
        "19.0",
        "44.1",
        "195M",
        "11.3",
        "36.4"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "Ensemble",
        "206M",
        "19.6",
        "45.1",
        "194M",
        "11.7",
        "35.9"
      ],
      [
        "DCGCN (ours)",
        "Ensemble",
        "[BOLD]  149M",
        "[BOLD] 20.5",
        "[BOLD] 45.8",
        "[BOLD]  142M",
        "[BOLD] 13.1",
        "[BOLD] 37.8"
      ]
    ],
    "id": "33575f0c-0627-4e2e-b5a6-668a9f447a3a",
    "claim": "In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs.",
    "label": "supports",
    "table_id": "ce0e7cde-0b65-4618-b46b-9952fecf42a8"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "f9ee4a0c-c9ea-4202-9e4f-4810c4def2db",
    "claim": "This creates an artificial outlier alternative which has low applicability and productivity, but has high coverage which stems from this outlier alternative.",
    "label": "not enough info",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "Solving Hard Coreference Problems",
    "paper_id": "1907.05524v1",
    "table_caption": "Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
    "table_column_names": [
      "Schema",
      "AntePre(Test)",
      "AntePre(Train)"
    ],
    "table_content_values": [
      [
        "Type 1",
        "76.67",
        "86.79"
      ],
      [
        "Type 2",
        "79.55",
        "88.86"
      ],
      [
        "Type 1 (Cat1)",
        "90.26",
        "93.64"
      ],
      [
        "Type 2 (Cat2)",
        "83.38",
        "92.49"
      ]
    ],
    "id": "ba0b99c4-316c-4a92-b84e-5f072c99b79e",
    "claim": "They showthat both Type 1 and Type 2 schema knowledge havehigher precision on Category 1 and Category 2 datainstances, respectively, compared to that on full data.",
    "label": "supports",
    "table_id": "18072983-1b23-4da0-8e08-1b6a92fbc124"
  },
  {
    "paper": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks",
    "paper_id": "1805.10390v2",
    "table_caption": "Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
    "table_column_names": [
      "[BOLD] System",
      "[BOLD] ROUGE-1  [BOLD] R (%)",
      "[BOLD] ROUGE-1  [BOLD] P (%)",
      "[BOLD] ROUGE-1  [BOLD] F (%)",
      "[BOLD] ROUGE-2  [BOLD] R (%)",
      "[BOLD] ROUGE-2  [BOLD] P (%)",
      "[BOLD] ROUGE-2  [BOLD] F (%)",
      "[BOLD] Sentence-Level  [BOLD] R (%)",
      "[BOLD] Sentence-Level  [BOLD] P (%)",
      "[BOLD] Sentence-Level  [BOLD] F (%)"
    ],
    "table_content_values": [
      [
        "[BOLD] ILP",
        "24.5",
        "41.1",
        "29.3±0.5",
        "7.9",
        "15.0",
        "9.9±0.5",
        "13.6",
        "22.6",
        "15.6±0.4"
      ],
      [
        "[BOLD] Sum-Basic",
        "28.4",
        "44.4",
        "33.1±0.5",
        "8.5",
        "15.6",
        "10.4±0.4",
        "14.7",
        "22.9",
        "16.7±0.5"
      ],
      [
        "[BOLD] KL-Sum",
        "39.5",
        "34.6",
        "35.5±0.5",
        "13.0",
        "12.7",
        "12.3±0.5",
        "15.2",
        "21.1",
        "16.3±0.5"
      ],
      [
        "[BOLD] LexRank",
        "42.1",
        "39.5",
        "38.7±0.5",
        "14.7",
        "15.3",
        "14.2±0.5",
        "14.3",
        "21.5",
        "16.0±0.5"
      ],
      [
        "[BOLD] MEAD",
        "45.5",
        "36.5",
        "38.5± 0.5",
        "17.9",
        "14.9",
        "15.4±0.5",
        "27.8",
        "29.2",
        "26.8±0.5"
      ],
      [
        "[BOLD] SVM",
        "19.0",
        "48.8",
        "24.7±0.8",
        "7.5",
        "21.1",
        "10.0±0.5",
        "32.7",
        "34.3",
        "31.4±0.4"
      ],
      [
        "[BOLD] LogReg",
        "26.9",
        "34.5",
        "28.7±0.6",
        "6.4",
        "9.9",
        "7.3±0.4",
        "12.2",
        "14.9",
        "12.7±0.5"
      ],
      [
        "[BOLD] LogReg [ITALIC] r",
        "28.0",
        "34.8",
        "29.4±0.6",
        "6.9",
        "10.4",
        "7.8±0.4",
        "12.1",
        "14.5",
        "12.5±0.5"
      ],
      [
        "[BOLD] HAN",
        "31.0",
        "42.8",
        "33.7±0.7",
        "11.2",
        "17.8",
        "12.7±0.5",
        "26.9",
        "34.1",
        "32.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT",
        "32.2",
        "42.4",
        "34.4±0.7",
        "11.5",
        "17.5",
        "12.9±0.5",
        "29.6",
        "35.8",
        "32.2±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU",
        "32.1",
        "42.1",
        "33.8±0.7",
        "11.6",
        "17.6",
        "12.9±0.5",
        "30.1",
        "35.6",
        "32.3±0.5"
      ],
      [
        "[BOLD] HAN [ITALIC] r",
        "38.1",
        "40.5",
        "[BOLD] 37.8±0.5",
        "14.0",
        "17.1",
        "[BOLD] 14.7±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainT [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.5",
        "16.8",
        "[BOLD] 14.4±0.5",
        "32.5",
        "34.4",
        "[BOLD] 33.4±0.5"
      ],
      [
        "[BOLD] HAN+pretrainU [ITALIC] r",
        "37.9",
        "40.4",
        "[BOLD] 37.6±0.5",
        "13.6",
        "16.9",
        "[BOLD] 14.4±0.5",
        "33.9",
        "33.8",
        "[BOLD] 33.8±0.5"
      ]
    ],
    "id": "58155cc4-23ec-4f64-b7ec-568af0621eaa",
    "claim": "[CONTINUE] We observe that the redundancy removal step is crucial for the HAN models to achieve outstanding results.",
    "label": "supports",
    "table_id": "3f7e91a7-3222-49ad-8059-b54141b429be"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Iteration=1",
        "0.531",
        "0.455",
        "0.353",
        "0.201"
      ],
      [
        "Iteration=2",
        "0.592",
        "0.498",
        "0.385",
        "0.375"
      ],
      [
        "Iteration=3",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ],
      [
        "Iteration=4",
        "0.601",
        "0.505",
        "0.422",
        "0.385"
      ],
      [
        "Iteration=5",
        "0.575",
        "0.495",
        "0.394",
        "0.376"
      ]
    ],
    "id": "02a92ece-1cfd-4fb7-827c-472608cc154c",
    "claim": "the KG itself has the most relevance to the results.",
    "label": "not enough info",
    "table_id": "e8d636ab-66cd-4c48-97ea-e05fd1ff69c0"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Iteration=1",
        "0.531",
        "0.455",
        "0.353",
        "0.201"
      ],
      [
        "Iteration=2",
        "0.592",
        "0.498",
        "0.385",
        "0.375"
      ],
      [
        "Iteration=3",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ],
      [
        "Iteration=4",
        "0.601",
        "0.505",
        "0.422",
        "0.385"
      ],
      [
        "Iteration=5",
        "0.575",
        "0.495",
        "0.394",
        "0.376"
      ]
    ],
    "id": "a08fc051-d0c1-4de4-b17f-791f586addcd",
    "claim": "It is possible that a specific KG has different patterns of its regularity, the result of which affects the learning ability of the complex KG embeddings",
    "label": "not enough info",
    "table_id": "e8d636ab-66cd-4c48-97ea-e05fd1ff69c0"
  },
  {
    "paper": "Suggestion Mining from Online Reviews using ULMFiT",
    "paper_id": "1904.09076v1",
    "table_caption": "Table 1: Dataset Distribution for Sub Task A - Task 9: Suggestion Mining from Online Reviews.",
    "table_column_names": [
      "[BOLD] Label",
      "[BOLD] Train",
      "[BOLD] Trial"
    ],
    "table_content_values": [
      [
        "[BOLD] Suggestion",
        "2085",
        "296"
      ],
      [
        "[BOLD] Non Suggestion",
        "6415",
        "296"
      ]
    ],
    "id": "95543831-0ee0-4bea-a141-061abbf717e9",
    "claim": "As evident from Table 1, there is a significant imbalance in the distribution of training instances that are suggestions and non-suggestions, 2https://www.uservoice.com/ [CONTINUE] For Sub Task A, the organizers shared a training and a validation dataset whose label distribution (suggestion or a non-suggestion) is presented in Table 1.",
    "label": "supports",
    "table_id": "1f7bb5b7-ffb5-404f-8c14-647be1993a66"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
    "table_column_names": [
      "Corpus",
      "Metric",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "Europarl",
        "TotalTerms:",
        "957",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "836",
        "1,000"
      ],
      [
        "Europarl",
        "TotalRoots:",
        "44",
        "1",
        "1",
        "1",
        "1",
        "43",
        "1"
      ],
      [
        "Europarl",
        "NumberRels:",
        "1,588",
        "1,025",
        "1,028",
        "1,185",
        "1,103",
        "1,184",
        "999"
      ],
      [
        "Europarl",
        "MaxDepth:",
        "21",
        "921",
        "901",
        "788",
        "835",
        "8",
        "15"
      ],
      [
        "Europarl",
        "MinDepth:",
        "1",
        "921",
        "901",
        "788",
        "835",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgDepth:",
        "11.82",
        "921",
        "901",
        "788",
        "835",
        "3.05",
        "8.46"
      ],
      [
        "Europarl",
        "DepthCohesion:",
        "1.78",
        "1",
        "1",
        "1",
        "1",
        "2.62",
        "1.77"
      ],
      [
        "Europarl",
        "MaxWidth:",
        "20",
        "2",
        "3",
        "4",
        "3",
        "88",
        "41"
      ],
      [
        "Europarl",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "Europarl",
        "AvgWidth:",
        "1.99",
        "1.03",
        "1.03",
        "1.19",
        "1.10",
        "4.20",
        "2.38"
      ],
      [
        "TED Talks",
        "TotalTerms:",
        "476",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000",
        "1,000"
      ],
      [
        "TED Talks",
        "TotalRoots:",
        "164",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "NumberRels:",
        "521",
        "1,029",
        "1,331",
        "3,025",
        "3,438",
        "3,802",
        "1,009"
      ],
      [
        "TED Talks",
        "MaxDepth:",
        "16",
        "915",
        "658",
        "454",
        "395",
        "118",
        "12"
      ],
      [
        "TED Talks",
        "MinDepth:",
        "1",
        "913",
        "658",
        "454",
        "395",
        "110",
        "1"
      ],
      [
        "TED Talks",
        "AvgDepth:",
        "5.82",
        "914",
        "658",
        "454",
        "395",
        "112.24",
        "5.95"
      ],
      [
        "TED Talks",
        "DepthCohesion:",
        "2.75",
        "1",
        "1",
        "1",
        "1",
        "1.05",
        "2.02"
      ],
      [
        "TED Talks",
        "MaxWidth:",
        "25",
        "2",
        "77",
        "13",
        "12",
        "66",
        "98"
      ],
      [
        "TED Talks",
        "MinWidth:",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      [
        "TED Talks",
        "AvgWidth:",
        "1.83",
        "1.03",
        "1.36",
        "3.03",
        "3.44",
        "6.64",
        "2.35"
      ]
    ],
    "id": "d84d4003-6890-4170-b3d7-c47ee7e05cf6",
    "claim": "For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 788 terms with different values of term frequency, while having only 1 that share the same value of term frequency with other terms.",
    "label": "refutes",
    "table_id": "7fd32b5d-95c3-40d7-8e3d-16391cad7f14"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. † denotes the previous best state-of-the-art model.",
    "table_column_names": [
      "Model",
      "NYT10 Prec.",
      "NYT10 Rec.",
      "NYT10 F1",
      "NYT11 Prec.",
      "NYT11 Rec.",
      "NYT11 F1"
    ],
    "table_content_values": [
      [
        "CNN zeng2014relation",
        "0.413",
        "0.591",
        "0.486",
        "0.444",
        "0.625",
        "0.519"
      ],
      [
        "PCNN zeng2015distant",
        "0.380",
        "[BOLD] 0.642",
        "0.477",
        "0.446",
        "0.679",
        "0.538†"
      ],
      [
        "EA huang2016attention",
        "0.443",
        "0.638",
        "0.523†",
        "0.419",
        "0.677",
        "0.517"
      ],
      [
        "BGWA jat2018attention",
        "0.364",
        "0.632",
        "0.462",
        "0.417",
        "[BOLD] 0.692",
        "0.521"
      ],
      [
        "BiLSTM-CNN",
        "0.490",
        "0.507",
        "0.498",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "Our model",
        "[BOLD] 0.541",
        "0.595",
        "[BOLD] 0.566*",
        "[BOLD] 0.507",
        "0.652",
        "[BOLD] 0.571*"
      ]
    ],
    "id": "fb51ce8e-cdc5-4c9d-8e17-c2043abea92e",
    "claim": "Our model improves the precision scores on both datasets with good recall scores.",
    "label": "supports",
    "table_id": "c6b2cd01-ccb9-4fdd-90ab-a8ed5ae0d132"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1173",
        "0.0366",
        "0.0503",
        "0.0554",
        "0.0548",
        "0.0443",
        "0.0761"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1125",
        "0.0301",
        "0.0382",
        "0.0425",
        "0.0441",
        "0.0710",
        "0.0664"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5163",
        "0.3330",
        "0.5257",
        "0.6109",
        "0.5984",
        "[BOLD] 0.7311",
        "0.5676"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.5387",
        "0.2907",
        "0.5300",
        "0.6117",
        "0.6159",
        "[BOLD] 0.6533",
        "0.5656"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0396",
        "0.3999",
        "0.5499",
        "[BOLD] 0.6045",
        "0.5887",
        "0.0023",
        "0.0017"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0018",
        "0.4442",
        "0.5377",
        "0.5657",
        "[BOLD] 0.6077",
        "0.2666",
        "0.0019"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0111",
        "0.3554",
        "0.5795",
        "[BOLD] 0.6727",
        "0.5184",
        "0.0053",
        "0.0012"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0004",
        "0.3142",
        "0.5484",
        "[BOLD] 0.6877",
        "0.5515",
        "0.4706",
        "0.0011"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0591",
        "0.0671",
        "0.0922",
        "[BOLD] 0.1015",
        "0.1003",
        "0.0044",
        "0.0033"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0035",
        "0.0564",
        "0.0713",
        "0.0791",
        "0.0822",
        "[BOLD] 0.1121",
        "0.0037"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0217",
        "0.3438",
        "0.5513",
        "[BOLD] 0.6403",
        "0.5555",
        "0.0105",
        "0.0024"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "0.0008",
        "0.3020",
        "0.5390",
        "[BOLD] 0.6475",
        "0.5819",
        "0.5471",
        "0.0022"
      ]
    ],
    "id": "7a54fd92-fef9-4f12-8e39-7da48cdc52eb",
    "claim": "As we can observe in Table 3, Patt has the best values of precision for the English corpora while SLQS has the best values for the Portuguese corpora.",
    "label": "refutes",
    "table_id": "db874c83-259a-44ca-a134-3e22bec27a07"
  },
  {
    "paper": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation",
    "paper_id": "1907.12894v1",
    "table_caption": "Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
    "table_column_names": [
      "[EMPTY]",
      "DUC’01 <italic>R</italic>1",
      "DUC’01 <italic>R</italic>2",
      "DUC’02 <italic>R</italic>1",
      "DUC’02 <italic>R</italic>2",
      "DUC’04 <italic>R</italic>1",
      "DUC’04 <italic>R</italic>2"
    ],
    "table_content_values": [
      [
        "ICSI",
        "33.31",
        "7.33",
        "35.04",
        "8.51",
        "37.31",
        "9.36"
      ],
      [
        "PriorSum",
        "35.98",
        "7.89",
        "36.63",
        "8.97",
        "38.91",
        "10.07"
      ],
      [
        "TCSum",
        "<bold>36.45</bold>",
        "7.66",
        "36.90",
        "8.61",
        "38.27",
        "9.66"
      ],
      [
        "TCSum−",
        "33.45",
        "6.07",
        "34.02",
        "7.39",
        "35.66",
        "8.66"
      ],
      [
        "SRSum",
        "36.04",
        "8.44",
        "<bold>38.93</bold>",
        "<bold>10.29</bold>",
        "39.29",
        "10.70"
      ],
      [
        "DeepTD",
        "28.74",
        "5.95",
        "31.63",
        "7.09",
        "33.57",
        "7.96"
      ],
      [
        "REAPER",
        "32.43",
        "6.84",
        "35.03",
        "8.11",
        "37.22",
        "8.64"
      ],
      [
        "RELIS",
        "34.73",
        "<bold>8.66</bold>",
        "37.11",
        "9.12",
        "<bold>39.34</bold>",
        "<bold>10.73</bold>"
      ]
    ],
    "id": "a1761e4d-eabf-472c-a9bf-311759005b27",
    "claim": "RELIS does not significantly outperform the other RL-based systems.",
    "label": "refutes",
    "table_id": "d462f28f-7f86-4229-a2f3-71f92c29bc51"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
    "table_column_names": [
      "Model",
      "Method",
      "Training Data",
      "Overall",
      "Easy",
      "Hard",
      "p-value (%)"
    ],
    "table_content_values": [
      [
        "goodwin-etal-2012-utdhlt",
        "PMI",
        "unsupervised",
        "61.8",
        "64.7",
        "60.0",
        "19.8"
      ],
      [
        "gordon_commonsense_2011-1",
        "PMI",
        "unsupervised",
        "65.4",
        "65.8",
        "65.2",
        "83.5"
      ],
      [
        "sasaki-etal-2017-handling",
        "PMI",
        "unsupervised",
        "71.4",
        "75.3",
        "69.0",
        "4.8∗"
      ],
      [
        "Word frequency",
        "wordfreq",
        "COPA",
        "53.5",
        "57.4",
        "51.3",
        "9.8"
      ],
      [
        "BERT-large-FT",
        "LM, NSP",
        "COPA",
        "76.5 (± 2.7)",
        "83.9 (± 4.4)",
        "71.9 (± 2.5)",
        "0.0∗"
      ],
      [
        "RoBERTa-large-FT",
        "LM",
        "COPA",
        "87.7 (± 0.9)",
        "91.6 (± 1.1)",
        "85.3 (± 2.0)",
        "0.0∗"
      ]
    ],
    "id": "7cb369b8-55fa-4147-b838-68f47b127ec9",
    "claim": "This seems to contradict previous research reporting that RoBERT does not improve on existing models for multiple-choice QA (Schick et al., 2020).",
    "label": "not enough info",
    "table_id": "486a8be7-66d4-4422-9813-0b8b418b1c5a"
  },
  {
    "paper": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation",
    "paper_id": "1909.00754v2",
    "table_caption": "Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model Mrksic et al. (2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ Budzianowski et al. (2018).",
    "table_column_names": [
      "[BOLD] DST Models",
      "[BOLD] Joint Acc. WoZ 2.0",
      "[BOLD] Joint Acc. MultiWoZ",
      "[BOLD] ITC"
    ],
    "table_content_values": [
      [
        "Baselines Mrksic et al. ( 2017 )",
        "70.8%",
        "25.83%",
        "[ITALIC] O( [ITALIC] mn)"
      ],
      [
        "NBT-CNN Mrksic et al. ( 2017 )",
        "84.2%",
        "-",
        "[ITALIC] O( [ITALIC] mn)"
      ],
      [
        "StateNet_PSI Ren et al. ( 2018 )",
        "[BOLD] 88.9%",
        "-",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "GLAD Nouri and Hosseini-Asl ( 2018 )",
        "88.5%",
        "35.58%",
        "[ITALIC] O( [ITALIC] mn)"
      ],
      [
        "HyST (ensemble) Goel et al. ( 2019 )",
        "-",
        "44.22%",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "DSTRead (ensemble) Gao et al. ( 2019 )",
        "-",
        "42.12%",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "TRADE Wu et al. ( 2019 )",
        "-",
        "48.62%",
        "[ITALIC] O( [ITALIC] n)"
      ],
      [
        "COMER",
        "88.6%",
        "[BOLD] 48.79%",
        "[ITALIC] O(1)"
      ]
    ],
    "id": "fc367a7a-61f9-4d01-80cb-71f58d303c05",
    "claim": "On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which marginally outperforms the previous state-of-the-art.",
    "label": "supports",
    "table_id": "d5d1590e-42a1-4452-9cc7-1bce08b243b7"
  },
  {
    "paper": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages",
    "paper_id": "1811.03245v1",
    "table_caption": "Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
    "table_column_names": [
      "[EMPTY]",
      "Lang",
      "Corpus",
      "Patt",
      "DSim",
      "SLQS",
      "TF",
      "DF",
      "DocSub",
      "HClust"
    ],
    "table_content_values": [
      [
        "P",
        "EN",
        "Europarl",
        "[BOLD] 0.1192",
        "0.0083",
        "0.0137",
        "0.0150",
        "0.0150",
        "0.0445",
        "0.0326"
      ],
      [
        "P",
        "EN",
        "Ted Talks",
        "[BOLD] 0.1022",
        "0.0069",
        "0.0060",
        "0.0092",
        "0.0090",
        "0.0356",
        "0.0162"
      ],
      [
        "P",
        "PT",
        "Europarl",
        "0.5710",
        "0.1948",
        "0.3855",
        "0.5474",
        "0.4485",
        "[BOLD] 0.8052",
        "0.4058"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "[BOLD] 0.6304",
        "0.1870",
        "0.3250",
        "0.5312",
        "0.4576",
        "0.6064",
        "0.3698"
      ],
      [
        "R",
        "EN",
        "Europarl",
        "0.0037",
        "0.3278",
        "0.5941",
        "0.6486",
        "[BOLD] 0.6490",
        "0.0017",
        "0.0003"
      ],
      [
        "R",
        "EN",
        "Ted Talks",
        "0.0002",
        "0.1486",
        "0.4332",
        "[BOLD] 0.6467",
        "0.6332",
        "0.0967",
        "0.0003"
      ],
      [
        "R",
        "PT",
        "Europarl",
        "0.0002",
        "0.1562",
        "0.5157",
        "[BOLD] 0.7255",
        "0.5932",
        "0.0032",
        "0.0001"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "2.10-5",
        "0.0507",
        "0.4492",
        "[BOLD] 0.7000",
        "0.5887",
        "0.1390",
        "0.0002"
      ],
      [
        "F",
        "EN",
        "Europarl",
        "0.0073",
        "0.0162",
        "0.0268",
        "[BOLD] 0.0293",
        "[BOLD] 0.0293",
        "0.0033",
        "0.0006"
      ],
      [
        "F",
        "EN",
        "Ted Talks",
        "0.0004",
        "0.0132",
        "0.0118",
        "0.0181",
        "0.0179",
        "[BOLD] 0.0520",
        "0.0005"
      ],
      [
        "F",
        "PT",
        "Europarl",
        "0.0005",
        "0.1733",
        "0.4412",
        "[BOLD] 0.6240",
        "0.5109",
        "0.0064",
        "0.0002"
      ],
      [
        "[EMPTY]",
        "PT",
        "Ted Talks",
        "4.10-5",
        "0.0798",
        "0.3771",
        "[BOLD] 0.6040",
        "0.5149",
        "0.2261",
        "0.0004"
      ]
    ],
    "id": "495a65cc-6f0c-4b6a-b19c-a3bd11784062",
    "claim": "[CONTINUE] The lowest values of precision are achieved by DSim model, and the lowest recalls are obtained by HClust and Patt models.",
    "label": "supports",
    "table_id": "b72ee7d7-411b-4a2a-9516-208770eba7eb"
  },
  {
    "paper": "Semantic Neural Machine Translation using AMR",
    "paper_id": "1902.07282v1",
    "table_caption": "Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.",
    "table_column_names": [
      "System",
      "NC-v11 BLEU",
      "NC-v11 TER↓",
      "NC-v11 Meteor",
      "Full BLEU",
      "Full TER↓",
      "Full Meteor"
    ],
    "table_content_values": [
      [
        "OpenNMT-tf",
        "15.1",
        "0.6902",
        "0.3040",
        "24.3",
        "0.5567",
        "0.4225"
      ],
      [
        "Transformer-tf",
        "17.1",
        "0.6647",
        "0.3578",
        "25.1",
        "0.5537",
        "0.4344"
      ],
      [
        "Seq2seq",
        "16.0",
        "0.6695",
        "0.3379",
        "23.7",
        "0.5590",
        "0.4258"
      ],
      [
        "Dual2seq-LinAMR",
        "17.3",
        "0.6530",
        "0.3612",
        "24.0",
        "0.5643",
        "0.4246"
      ],
      [
        "Duel2seq-SRL",
        "17.2",
        "0.6591",
        "0.3644",
        "23.8",
        "0.5626",
        "0.4223"
      ],
      [
        "Dual2seq-Dep",
        "17.8",
        "0.6516",
        "0.3673",
        "25.0",
        "0.5538",
        "0.4328"
      ],
      [
        "Dual2seq",
        "[BOLD] *19.2*",
        "[BOLD] 0.6305",
        "[BOLD] 0.3840",
        "[BOLD] *25.5*",
        "[BOLD] 0.5480",
        "[BOLD] 0.4376"
      ]
    ],
    "id": "9f1a619e-4ad3-401c-8c76-b44d8a33ef89",
    "claim": "Dual2seq-LinAMR shows much worse performance than our model and only slightly outperforms the Seq2seq baseline.",
    "label": "supports",
    "table_id": "e084cacd-b155-4f86-b489-d0bfabe6ca29"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "8f2d3f7f-1558-4d27-a479-4d02c594336f",
    "claim": "On 7 out of 11 supervised tasks, the joint model does not improve upon the better model, and on SST2, SST5, and MRPC the difference is less than 1 point.",
    "label": "refutes",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain MultiNLI",
      "out-of-domain SNLI",
      "out-of-domain Glockner",
      "out-of-domain SICK"
    ],
    "table_content_values": [
      [
        "MQAN",
        "72.30",
        "60.91",
        "41.82",
        "53.95"
      ],
      [
        "+ coverage",
        "<bold>73.84</bold>",
        "<bold>65.38</bold>",
        "<bold>78.69</bold>",
        "<bold>54.55</bold>"
      ],
      [
        "ESIM (ELMO)",
        "80.04",
        "68.70",
        "60.21",
        "51.37"
      ],
      [
        "+ coverage",
        "<bold>80.38</bold>",
        "<bold>70.05</bold>",
        "<bold>67.47</bold>",
        "<bold>52.65</bold>"
      ]
    ],
    "id": "2aecaa7e-e91a-47ba-9108-97dd18064ff6",
    "claim": "[CONTINUE] The results show that coverage information considerably improves the generalization of both examined models across various NLI datasets.",
    "label": "supports",
    "table_id": "4ececaed-0a92-42c4-8ded-adc22f6f4615"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "faabddbe-aa9d-42f7-abf4-f242e5a01e76",
    "claim": "Our single DCGCN model does not obtain better results than previous ensemble models.",
    "label": "refutes",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Adversarial Removal of Demographic Attributes from Text Data",
    "paper_id": "1808.06640v2",
    "table_caption": "Table 3: Performances on different datasets with an adversarial training. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.",
    "table_column_names": [
      "Data",
      "Task",
      "Protected Attribute",
      "Task Acc",
      "Leakage",
      "Δ"
    ],
    "table_content_values": [
      [
        "Dial",
        "Sentiment",
        "Race",
        "64.7",
        "56.0",
        "5.0"
      ],
      [
        "[EMPTY]",
        "Mention",
        "Race",
        "81.5",
        "63.1",
        "9.2"
      ],
      [
        "PAN16",
        "Mention",
        "Gender",
        "75.6",
        "58.5",
        "8.0"
      ],
      [
        "[EMPTY]",
        "Mention",
        "Age",
        "72.5",
        "57.3",
        "6.9"
      ]
    ],
    "id": "980f3548-a1c0-4e26-90d1-017f81573985",
    "claim": "In all cases, the adversarial's success rate is higher than the attacker's rate, with a difference of at least 5%.",
    "label": "refutes",
    "table_id": "502870a9-3417-4da3-a647-1a5db2abc4ec"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
    "table_column_names": [
      "<bold>Model</bold>",
      "REF ⇒ GEN <bold>ENT</bold>",
      "REF ⇒ GEN <bold>CON</bold>",
      "REF ⇒ GEN <bold>NEU</bold>"
    ],
    "table_content_values": [
      [
        "S2S",
        "38.45",
        "11.17",
        "50.38"
      ],
      [
        "G2S-GIN",
        "49.78",
        "9.80",
        "40.42"
      ],
      [
        "G2S-GAT",
        "49.48",
        "8.09",
        "42.43"
      ],
      [
        "G2S-GGNN",
        "51.32",
        "8.82",
        "39.86"
      ],
      [
        "[EMPTY]",
        "GEN ⇒ REF",
        "GEN ⇒ REF",
        "GEN ⇒ REF"
      ],
      [
        "<bold>Model</bold>",
        "<bold>ENT</bold>",
        "<bold>CON</bold>",
        "<bold>NEU</bold>"
      ],
      [
        "S2S",
        "73.79",
        "12.75",
        "13.46"
      ],
      [
        "G2S-GIN",
        "76.27",
        "10.65",
        "13.08"
      ],
      [
        "G2S-GAT",
        "77.54",
        "8.54",
        "13.92"
      ],
      [
        "G2S-GGNN",
        "77.64",
        "9.64",
        "12.72"
      ]
    ],
    "id": "968f23a7-c045-48ba-8bff-71a31abdd3d8",
    "claim": "This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.",
    "label": "supports",
    "table_id": "de8edae0-050d-4f22-9390-c71ba9efc12c"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "4a286ba2-cb10-4692-8016-513776cdc6b3",
    "claim": "[CONTINUE] It also improves the generalization ability of question answering.",
    "label": "supports",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "5e5549b7-46c9-4d2a-be76-7bf8049380a4",
    "claim": "[CONTINUE] Yet, the PRKGC model do not give considerably good results, which indicates the non-triviality of RC-QEDE.",
    "label": "supports",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task",
    "paper_id": "1910.03291v1",
    "table_caption": "Table 4: Image-caption ranking results for Japanese (MS-COCO)",
    "table_column_names": [
      "[EMPTY]",
      "Image to Text R@1",
      "Image to Text R@5",
      "Image to Text R@10",
      "Image to Text Mr",
      "Text to Image R@1",
      "Text to Image R@5",
      "Text to Image R@10",
      "Text to Image Mr",
      "Alignment"
    ],
    "table_content_values": [
      [
        "[BOLD] symmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Mono",
        "42.7",
        "77.7",
        "88.5",
        "2",
        "33.1",
        "69.8",
        "84.3",
        "3",
        "-"
      ],
      [
        "FME",
        "40.7",
        "77.7",
        "88.3",
        "2",
        "30.0",
        "68.9",
        "83.1",
        "3",
        "92.70%"
      ],
      [
        "AME",
        "[BOLD] 50.2",
        "[BOLD] 85.6",
        "[BOLD] 93.1",
        "[BOLD] 1",
        "[BOLD] 40.2",
        "[BOLD] 76.7",
        "[BOLD] 87.8",
        "[BOLD] 2",
        "82.54%"
      ],
      [
        "[BOLD] asymmetric",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Mono",
        "49.9",
        "83.4",
        "93.7",
        "2",
        "39.7",
        "76.5",
        "88.3",
        "[BOLD] 2",
        "-"
      ],
      [
        "FME",
        "48.8",
        "81.9",
        "91.9",
        "2",
        "37.0",
        "74.8",
        "87.0",
        "[BOLD] 2",
        "92.70%"
      ],
      [
        "AME",
        "[BOLD] 55.5",
        "[BOLD] 87.9",
        "[BOLD] 95.2",
        "[BOLD] 1",
        "[BOLD] 44.9",
        "[BOLD] 80.7",
        "[BOLD] 89.3",
        "[BOLD] 2",
        "84.99%"
      ]
    ],
    "id": "d87f2213-5235-4ce8-9871-44178ae506af",
    "claim": "For the Japanese captions, AME does not reach better results on average compared to monolingual model in symmetric and asymmetric modes, respectively.",
    "label": "refutes",
    "table_id": "32469655-2bf5-4a17-84a4-3600d3a2caf5"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] External",
      "B"
    ],
    "table_content_values": [
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "-",
        "22.0"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "-",
        "23.3"
      ],
      [
        "GCNSEQ (Damonte and Cohen,  2019 )",
        "-",
        "24.4"
      ],
      [
        "DCGCN(single)",
        "-",
        "25.9"
      ],
      [
        "DCGCN(ensemble)",
        "-",
        "[BOLD] 28.2"
      ],
      [
        "TSP (Song et al.,  2016 )",
        "ALL",
        "22.4"
      ],
      [
        "PBMT (Pourdamghani et al.,  2016 )",
        "ALL",
        "26.9"
      ],
      [
        "Tree2Str (Flanigan et al.,  2016 )",
        "ALL",
        "23.0"
      ],
      [
        "SNRG (Song et al.,  2017 )",
        "ALL",
        "25.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "0.2M",
        "27.4"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "0.2M",
        "28.2"
      ],
      [
        "DCGCN(single)",
        "0.1M",
        "29.0"
      ],
      [
        "DCGCN(single)",
        "0.2M",
        "[BOLD] 31.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "2M",
        "32.3"
      ],
      [
        "GraphLSTM (Song et al.,  2018 )",
        "2M",
        "33.6"
      ],
      [
        "Seq2SeqK (Konstas et al.,  2017 )",
        "20M",
        "33.8"
      ],
      [
        "DCGCN(single)",
        "0.3M",
        "33.2"
      ],
      [
        "DCGCN(ensemble)",
        "0.3M",
        "[BOLD] 35.3"
      ]
    ],
    "id": "554754d1-e781-421a-b781-05fba5c213dd",
    "claim": "When using the same amount of 0.2M data, the performance of DCGCN is not necessarily higher than Seq2SeqK and GraphLSTM.",
    "label": "refutes",
    "table_id": "f7b025d4-ffc5-4764-9949-312c3463da35"
  },
  {
    "paper": "Improving Generalization by Incorporating Coverage in Natural Language Inference",
    "paper_id": "1909.08940v1",
    "table_caption": "Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
    "table_column_names": [
      "[EMPTY]",
      "in-domain SQuAD",
      "in-domain SQuAD",
      "out-of-domain QA-SRL",
      "out-of-domain QA-SRL"
    ],
    "table_content_values": [
      [
        "[EMPTY]",
        "EM",
        "F1",
        "EM",
        "F1"
      ],
      [
        "MQAN",
        "31.76",
        "75.37",
        "<bold>10.99</bold>",
        "50.10"
      ],
      [
        "+coverage",
        "<bold>32.67</bold>",
        "<bold>76.83</bold>",
        "10.63",
        "<bold>50.89</bold>"
      ],
      [
        "BIDAF (ELMO)",
        "70.43",
        "79.76",
        "28.35",
        "49.98"
      ],
      [
        "+coverage",
        "<bold>71.07</bold>",
        "<bold>80.15</bold>",
        "<bold>30.58</bold>",
        "<bold>52.43</bold>"
      ]
    ],
    "id": "4032dd15-2c3c-49fa-b8cb-89a8f16ef60f",
    "claim": "we observe that the performance of both models decreases as the task becomes more dissimilar to the training data.",
    "label": "not enough info",
    "table_id": "c0f140df-0b50-4935-9630-4cbca993836e"
  },
  {
    "paper": "Effective Attention Modeling for Neural Relation Extraction",
    "paper_id": "1912.03832v1",
    "table_caption": "Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
    "table_column_names": [
      "[EMPTY]",
      "Prec.",
      "Rec.",
      "F1"
    ],
    "table_content_values": [
      [
        "(A1) BiLSTM-CNN",
        "0.473",
        "0.606",
        "0.531"
      ],
      [
        "(A2) Standard attention",
        "0.466",
        "0.638",
        "0.539"
      ],
      [
        "(A3) Window size ( [ITALIC] ws)=5",
        "0.507",
        "0.652",
        "[BOLD] 0.571"
      ],
      [
        "(A4) Window size ( [ITALIC] ws)=10",
        "0.510",
        "0.640",
        "0.568"
      ],
      [
        "(A5) Softmax",
        "0.490",
        "0.658",
        "0.562"
      ],
      [
        "(A6) Max-pool",
        "0.492",
        "0.600",
        "0.541"
      ]
    ],
    "id": "30ef8633-a942-41a8-8686-40d62e8d3848",
    "claim": "Adding the dependency weight factor with a window size of 10 decreases the F1 score by 0.7% (A4−A2).",
    "label": "refutes",
    "table_id": "6db9a94b-ab15-4b51-81b9-80d8c41bc18a"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "a16a3f84-4381-4d4e-bdc0-859ddfd9beb3",
    "claim": "Our ICA framework outperforms the other baselines for all tasks.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "71b4b4de-2559-490c-83ab-04da9dda769e",
    "claim": "[CONTINUE] Regarding the probing tasks, we observe that CBOW embeddings better encode the linguistic properties of sentences than CMOW.",
    "label": "refutes",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition",
    "paper_id": "1811.02182v1",
    "table_caption": "TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
    "table_column_names": [
      "Method",
      "WER (%)",
      "DCE"
    ],
    "table_content_values": [
      [
        "No enhancement",
        "17.3",
        "0.828"
      ],
      [
        "Wiener filter",
        "19.5",
        "0.722"
      ],
      [
        "Minimizing DCE",
        "15.8",
        "[BOLD] 0.269"
      ],
      [
        "FSEGAN",
        "14.9",
        "0.291"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
        "15.6",
        "0.330"
      ],
      [
        "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
        "[BOLD] 14.4",
        "0.303"
      ],
      [
        "Clean speech",
        "5.7",
        "0.0"
      ]
    ],
    "id": "931fadd7-9e3a-468e-a366-5b69a84720f6",
    "claim": "[CONTINUE] In Librispeech + DEMAND, acoustic supervision (15.6%) and multi-task learning (14.4%) achieves a lower WER than minimizing DCE (15.8%) and FSEGAN (14.9%).",
    "label": "supports",
    "table_id": "3c6c1f56-ed0b-418a-ac6c-6449488a89e9"
  },
  {
    "paper": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation",
    "paper_id": "1907.12894v1",
    "table_caption": "Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
    "table_column_names": [
      "[EMPTY]",
      "DUC’01 <italic>R</italic>1",
      "DUC’01 <italic>R</italic>2",
      "DUC’02 <italic>R</italic>1",
      "DUC’02 <italic>R</italic>2",
      "DUC’04 <italic>R</italic>1",
      "DUC’04 <italic>R</italic>2"
    ],
    "table_content_values": [
      [
        "ICSI",
        "33.31",
        "7.33",
        "35.04",
        "8.51",
        "37.31",
        "9.36"
      ],
      [
        "PriorSum",
        "35.98",
        "7.89",
        "36.63",
        "8.97",
        "38.91",
        "10.07"
      ],
      [
        "TCSum",
        "<bold>36.45</bold>",
        "7.66",
        "36.90",
        "8.61",
        "38.27",
        "9.66"
      ],
      [
        "TCSum−",
        "33.45",
        "6.07",
        "34.02",
        "7.39",
        "35.66",
        "8.66"
      ],
      [
        "SRSum",
        "36.04",
        "8.44",
        "<bold>38.93</bold>",
        "<bold>10.29</bold>",
        "39.29",
        "10.70"
      ],
      [
        "DeepTD",
        "28.74",
        "5.95",
        "31.63",
        "7.09",
        "33.57",
        "7.96"
      ],
      [
        "REAPER",
        "32.43",
        "6.84",
        "35.03",
        "8.11",
        "37.22",
        "8.64"
      ],
      [
        "RELIS",
        "34.73",
        "<bold>8.66</bold>",
        "37.11",
        "9.12",
        "<bold>39.34</bold>",
        "<bold>10.73</bold>"
      ]
    ],
    "id": "cee6661e-b5b1-4fc8-b1cd-c9c014565b09",
    "claim": "At the same time, RELIS performs worse than neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next.",
    "label": "refutes",
    "table_id": "d462f28f-7f86-4229-a2f3-71f92c29bc51"
  },
  {
    "paper": "The MeMAD Submission to the WMT18 Multimodal Translation Task",
    "paper_id": "1808.10802v2",
    "table_caption": "Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and − for removing a component or data set. Multiple modifications are indicated by increasing the indentation.",
    "table_column_names": [
      "en-fr",
      "flickr16",
      "flickr17",
      "mscoco17"
    ],
    "table_content_values": [
      [
        "subs3M [ITALIC]  [ITALIC] LM detectron",
        "68.30",
        "62.45",
        "52.86"
      ],
      [
        "+ensemble-of-3",
        "68.72",
        "62.70",
        "53.06"
      ],
      [
        "−visual features",
        "[BOLD] 68.74",
        "[BOLD] 62.71",
        "53.14"
      ],
      [
        "−MS-COCO",
        "67.13",
        "61.17",
        "[BOLD] 53.34"
      ],
      [
        "−multi-lingual",
        "68.21",
        "61.99",
        "52.40"
      ],
      [
        "subs6M [ITALIC]  [ITALIC] LM detectron",
        "68.29",
        "61.73",
        "53.05"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM gn2048",
        "67.74",
        "61.78",
        "52.76"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM text-only",
        "67.72",
        "61.75",
        "53.02"
      ],
      [
        "en-de",
        "flickr16",
        "flickr17",
        "mscoco17"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM detectron",
        "45.09",
        "40.81",
        "36.94"
      ],
      [
        "+ensemble-of-3",
        "45.52",
        "[BOLD] 41.84",
        "[BOLD] 37.49"
      ],
      [
        "−visual features",
        "[BOLD] 45.59",
        "41.75",
        "37.43"
      ],
      [
        "−MS-COCO",
        "45.11",
        "40.52",
        "36.47"
      ],
      [
        "−multi-lingual",
        "44.95",
        "40.09",
        "35.28"
      ],
      [
        "subs6M [ITALIC]  [ITALIC] LM detectron",
        "45.50",
        "41.01",
        "36.81"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM gn2048",
        "45.38",
        "40.07",
        "36.82"
      ],
      [
        "subs3M [ITALIC]  [ITALIC] LM text-only",
        "44.87",
        "41.27",
        "36.59"
      ],
      [
        "+multi-modal finetune",
        "44.56",
        "41.61",
        "36.93"
      ]
    ],
    "id": "0d7db1a3-15a7-448f-a5ff-bee89e7168f1",
    "claim": "When the experiment was repeated so that the finetuning phase included the text-only data, the performance returned to approximately the same level as without tuning (+multi-modal finetune row in Table 6).",
    "label": "supports",
    "table_id": "dccd5c25-66c7-421e-b062-ae2e1f5c88d2"
  },
  {
    "paper": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
    "paper_id": "1908.05957v2",
    "table_caption": "Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] T",
      "#P",
      "B",
      "C"
    ],
    "table_content_values": [
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "S",
        "28,4M",
        "21.7",
        "49.1"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "S",
        "28.3M",
        "23.3",
        "50.4"
      ],
      [
        "Seq2SeqB (Beck et al.,  2018 )",
        "E",
        "142M",
        "26.6",
        "52.5"
      ],
      [
        "GGNN2Seq (Beck et al.,  2018 )",
        "E",
        "141M",
        "27.5",
        "53.5"
      ],
      [
        "DCGCN (ours)",
        "S",
        "[BOLD] 19.1M",
        "27.9",
        "57.3"
      ],
      [
        "DCGCN (ours)",
        "E",
        "92.5M",
        "[BOLD] 30.4",
        "[BOLD] 59.6"
      ]
    ],
    "id": "34c20bf4-d238-447f-bd59-ad3fde68d867",
    "claim": "Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms.",
    "label": "supports",
    "table_id": "cf468f9f-eb92-4c34-8cb3-df5680aaa706"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "33fb4b62-d5db-43e1-b19f-9006c5c5c618",
    "claim": "[CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions.",
    "label": "supports",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
    "table_column_names": [
      "System",
      "Reward",
      "R-1",
      "R-2",
      "R-L"
    ],
    "table_content_values": [
      [
        "Kryscinski et al. ( 2018 )",
        "R-L",
        "40.2",
        "17.4",
        "37.5"
      ],
      [
        "Narayan et al. ( 2018b )",
        "R-1,2,L",
        "40.0",
        "18.2",
        "36.6"
      ],
      [
        "Chen and Bansal ( 2018 )",
        "R-L",
        "41.5",
        "18.7",
        "37.8"
      ],
      [
        "Dong et al. ( 2018 )",
        "R-1,2,L",
        "41.5",
        "18.7",
        "37.6"
      ],
      [
        "Zhang et al. ( 2018 )",
        "[EMPTY]",
        "41.1",
        "18.8",
        "37.5"
      ],
      [
        "Zhou et al. ( 2018 )",
        "[EMPTY]",
        "41.6",
        "19.0",
        "38.0"
      ],
      [
        "Kedzie et al. ( 2018 )",
        "[EMPTY]",
        "39.1",
        "17.9",
        "35.9"
      ],
      [
        "(ours) NeuralTD",
        "Learned",
        "39.6",
        "18.1",
        "36.5"
      ]
    ],
    "id": "88e30212-929a-46ff-9072-3b6013ef55ac",
    "claim": "The results of using NeuralTD to generate summaries are in the bottommost row; the overall F-score is only lower by 1.4 for each metric.",
    "label": "not enough info",
    "table_id": "b319e0cb-57b2-4ed0-a70d-7bfb1924edb3"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 1: Precisions on the NYT dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "0.4",
      "AUC"
    ],
    "table_content_values": [
      [
        "PCNN+ATT",
        "0.698",
        "0.606",
        "0.518",
        "0.446",
        "0.323"
      ],
      [
        "Rank+ExATT",
        "0.789",
        "0.726",
        "0.620",
        "0.514",
        "0.395"
      ],
      [
        "Our Model",
        "0.788",
        "[BOLD] 0.743",
        "[BOLD] 0.654",
        "[BOLD] 0.546",
        "[BOLD] 0.397"
      ]
    ],
    "id": "4e700d01-7b1d-422d-aa5b-e9fffa5f7cfb",
    "claim": "the joint-training strategy has more significant performance gains in recall from 0.1 to 0.4 than the fine-tuning strategy.",
    "label": "not enough info",
    "table_id": "f4fa37fb-36c7-4ace-b563-6bd07bd3cf9e"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "b55f3404-c386-4e10-a2c0-b686c31536b7",
    "claim": "This is expected as the joint model introduces a greater capacity to the model and, therefore, can deal with more complex entity coreference.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
    "table_column_names": [
      "Model",
      "Belief State Type",
      "System Action Type",
      "System Action Form",
      "Inform (%)",
      "Success (%)",
      "BLEU",
      "Combined Score"
    ],
    "table_content_values": [
      [
        "1. Seq2Seq + Attention ",
        "oracle",
        "-",
        "-",
        "71.3",
        "61.0",
        "[BOLD] 18.9",
        "85.1"
      ],
      [
        "2. Seq2Seq + Copy",
        "oracle",
        "-",
        "-",
        "86.2",
        "[BOLD] 72.0",
        "15.7",
        "94.8"
      ],
      [
        "3. MD-Sequicity",
        "oracle",
        "-",
        "-",
        "[BOLD] 86.6",
        "71.6",
        "16.8",
        "[BOLD] 95.9"
      ],
      [
        "4. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "generated",
        "one-hot",
        "82.7",
        "72.1",
        "16.3",
        "93.7"
      ],
      [
        "5. HDSA ",
        "oracle",
        "generated",
        "graph",
        "82.9",
        "68.9",
        "[BOLD] 23.6",
        "99.5"
      ],
      [
        "6. DAMD",
        "oracle",
        "generated",
        "span",
        "[BOLD] 89.5",
        "75.8",
        "18.3",
        "100.9"
      ],
      [
        "7. DAMD + multi-action data augmentation",
        "oracle",
        "generated",
        "span",
        "89.2",
        "[BOLD] 77.9",
        "18.6",
        "[BOLD] 102.2"
      ],
      [
        "8. SFN + RL (Mehri et al. mehri2019structured)",
        "oracle",
        "oracle",
        "one-hot",
        "-",
        "-",
        "29.0",
        "106.0"
      ],
      [
        "9. HDSA ",
        "oracle",
        "oracle",
        "graph",
        "87.9",
        "78.0",
        "[BOLD] 30.4",
        "113.4"
      ],
      [
        "10. DAMD + multi-action data augmentation",
        "oracle",
        "oracle",
        "span",
        "[BOLD] 95.4",
        "[BOLD] 87.2",
        "27.3",
        "[BOLD] 118.5"
      ],
      [
        "11. SFN + RL (Mehri et al. mehri2019structured)",
        "generated",
        "generated",
        "one-hot",
        "73.8",
        "58.6",
        "[BOLD] 16.9",
        "83.0"
      ],
      [
        "12. DAMD + multi-action data augmentation",
        "generated",
        "generated",
        "span",
        "[BOLD] 76.3",
        "[BOLD] 60.4",
        "16.6",
        "[BOLD] 85.0"
      ]
    ],
    "id": "20c21547-c9f4-4420-88ce-bf96dd5d0418",
    "claim": "DAMD (generated actions)  is the state-of-the-art for combining action modeling and belief state augmentation for task-oriented response generation.",
    "label": "not enough info",
    "table_id": "cae4ef27-b55c-4a78-9efd-e8ab769f96f1"
  },
  {
    "paper": "Improved Semantics for the End-to-End Generation Challenge Corpus",
    "paper_id": "1911.03905v1",
    "table_caption": "Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).",
    "table_column_names": [
      "Train",
      "Test",
      "[BOLD] System",
      "[BOLD] BLEU",
      "[BOLD] NIST",
      "[BOLD] METEOR",
      "[BOLD] ROUGE-L",
      "[BOLD] CIDEr",
      "[BOLD] Add",
      "[BOLD] Miss",
      "[BOLD] Wrong",
      "[BOLD] SER"
    ],
    "table_content_values": [
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen−",
        "36.85",
        "5.3782",
        "35.14",
        "55.01",
        "1.6016",
        "00.34",
        "09.81",
        "00.15",
        "10.31"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen",
        "39.23",
        "6.0217",
        "36.97",
        "55.52",
        "1.7623",
        "00.40",
        "03.59",
        "00.07",
        "04.05"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "TGen+",
        "40.25",
        "6.1448",
        "37.50",
        "56.19",
        "1.8181",
        "00.21",
        "01.99",
        "00.05",
        "02.24"
      ],
      [
        "Original",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.88",
        "3.9310",
        "32.11",
        "39.90",
        "0.5036",
        "07.73",
        "17.76",
        "09.52",
        "35.03"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen−",
        "40.19",
        "6.0543",
        "37.38",
        "55.88",
        "1.8104",
        "00.17",
        "01.31",
        "00.25",
        "01.72"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen",
        "40.73",
        "6.1711",
        "37.76",
        "56.09",
        "1.8518",
        "00.07",
        "00.72",
        "00.08",
        "00.87"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "TGen+",
        "40.51",
        "6.1226",
        "37.61",
        "55.98",
        "1.8286",
        "00.02",
        "00.63",
        "00.06",
        "00.70"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
        "[BOLD] Cleaned",
        "SC-LSTM",
        "23.66",
        "3.9511",
        "32.93",
        "39.29",
        "0.3855",
        "07.89",
        "15.60",
        "08.44",
        "31.94"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen−",
        "40.48",
        "6.0269",
        "37.26",
        "56.19",
        "1.7999",
        "00.43",
        "02.84",
        "00.26",
        "03.52"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen",
        "41.57",
        "6.2830",
        "37.99",
        "56.36",
        "1.8849",
        "00.37",
        "01.40",
        "00.09",
        "01.86"
      ],
      [
        "Cleaned missing",
        "[BOLD] Cleaned",
        "TGen+",
        "41.56",
        "6.2700",
        "37.94",
        "56.38",
        "1.8827",
        "00.21",
        "01.04",
        "00.07",
        "01.31"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen−",
        "35.99",
        "5.0734",
        "34.74",
        "54.79",
        "1.5259",
        "00.02",
        "11.58",
        "00.02",
        "11.62"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen",
        "40.07",
        "6.1243",
        "37.45",
        "55.81",
        "1.8026",
        "00.05",
        "03.23",
        "00.01",
        "03.29"
      ],
      [
        "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
        "[BOLD] Cleaned",
        "TGen+",
        "40.80",
        "6.2197",
        "37.86",
        "56.13",
        "1.8422",
        "00.01",
        "01.87",
        "00.01",
        "01.88"
      ]
    ],
    "id": "33601bd8-365d-423b-9207-1d62d6031441",
    "claim": "Again, one possible explanation is that cleaning the missing slots provided more complex training examples.",
    "label": "supports",
    "table_id": "b8e696fd-935d-4f0d-8185-3a55ef054d90"
  },
  {
    "paper": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
    "paper_id": "1909.00352v1",
    "table_caption": "Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
    "table_column_names": [
      "<bold>Model</bold>",
      "<bold>Graph Diameter</bold> 0-7 Δ",
      "<bold>Graph Diameter</bold> 7-13 Δ",
      "<bold>Graph Diameter</bold> 14-20 Δ"
    ],
    "table_content_values": [
      [
        "S2S",
        "33.2",
        "29.7",
        "28.8"
      ],
      [
        "G2S-GIN",
        "35.2 +6.0%",
        "31.8 +7.4%",
        "31.5 +9.2%"
      ],
      [
        "G2S-GAT",
        "35.1 +5.9%",
        "32.0 +7.8%",
        "31.5 +9.51%"
      ],
      [
        "G2S-GGNN",
        "36.2 +9.0%",
        "33.0 +11.4%",
        "30.7 +6.7%"
      ],
      [
        "[EMPTY]",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>",
        "<bold>Sentence Length</bold>"
      ],
      [
        "[EMPTY]",
        "0-20 Δ",
        "20-50 Δ",
        "50-240 Δ"
      ],
      [
        "S2S",
        "34.9",
        "29.9",
        "25.1"
      ],
      [
        "G2S-GIN",
        "36.7 +5.2%",
        "32.2 +7.8%",
        "26.5 +5.8%"
      ],
      [
        "G2S-GAT",
        "36.9 +5.7%",
        "32.3 +7.9%",
        "26.6 +6.1%"
      ],
      [
        "G2S-GGNN",
        "37.9 +8.5%",
        "33.3 +11.2%",
        "26.9 +6.8%"
      ],
      [
        "[EMPTY]",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>",
        "<bold>Max Node Out-degree</bold>"
      ],
      [
        "[EMPTY]",
        "0-3 Δ",
        "4-8 Δ",
        "9-18 Δ"
      ],
      [
        "S2S",
        "31.7",
        "30.0",
        "23.9"
      ],
      [
        "G2S-GIN",
        "33.9 +6.9%",
        "32.1 +6.9%",
        "25.4 +6.2%"
      ],
      [
        "G2S-GAT",
        "34.3 +8.0%",
        "32.0 +6.7%",
        "22.5 -6.0%"
      ],
      [
        "G2S-GGNN",
        "35.0 +10.3%",
        "33.1 +10.4%",
        "22.2 -7.3%"
      ]
    ],
    "id": "51db7e0d-e291-45d8-bb0a-0210fa7cda1d",
    "claim": "[CONTINUE] Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters.",
    "label": "supports",
    "table_id": "9e37a061-17e7-4c8a-8f2b-15190abf75f6"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CBOW/784",
        "90.0",
        "[BOLD] 79.2",
        "[BOLD] 74.0",
        "87.1",
        "71.6",
        "85.6",
        "78.9",
        "78.5",
        "42.1",
        "61.0",
        "[BOLD] 78.1"
      ],
      [
        "CMOW/784",
        "87.5",
        "73.4",
        "70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "77.2",
        "74.7",
        "37.9",
        "56.5",
        "76.2"
      ],
      [
        "Hybrid",
        "[BOLD] 90.2",
        "78.7",
        "73.7",
        "[BOLD] 87.3",
        "[BOLD] 72.7",
        "87.6",
        "[BOLD] 79.4",
        "[BOLD] 79.6",
        "[BOLD] 43.3",
        "[BOLD] 63.4",
        "77.8"
      ],
      [
        "cmp. CBOW",
        "+0.2%",
        "-0.6%",
        "-0.4%",
        "+0.2%",
        "+1.5%",
        "+2.3%",
        "+0.6%",
        "+1.4%",
        "+2.9%",
        "+3.9%",
        "-0.4%"
      ],
      [
        "cmp. CMOW",
        "+3.1%",
        "+7.2%",
        "+4.4%",
        "+0%",
        "+4.5%",
        "-0.5%",
        "+2.9%",
        "+6.7%",
        "+14.3",
        "+12.2%",
        "+2.1%"
      ]
    ],
    "id": "eb4529a1-e468-4159-9afe-25270977f2dc",
    "claim": "On the TREC task, on the other hand, CMOW outperforms CBOW by 2.5 points.",
    "label": "supports",
    "table_id": "f13de50d-8900-4c3c-8254-964d8ad0f695"
  },
  {
    "paper": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension",
    "paper_id": "1910.04601v1",
    "table_caption": "Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
    "table_column_names": [
      "Model",
      "Answerability Macro P/R/F",
      "# Answerable",
      "Answer Prec.",
      "Derivation Prec. RG-L (P/R/F)",
      "Derivation Prec. BL-4"
    ],
    "table_content_values": [
      [
        "Shortest Path",
        "54.8/55.5/53.2",
        "976",
        "3.6",
        "56.7/38.5/41.5",
        "31.3"
      ],
      [
        "PRKGC",
        "52.6/51.5/50.7",
        "1,021",
        "45.2",
        "40.7/60.7/44.7",
        "30.9"
      ],
      [
        "PRKGC+NS",
        "53.6/54.1/52.1",
        "980",
        "45.4",
        "42.2/61.6/46.1",
        "33.4"
      ]
    ],
    "id": "017f3d64-a37c-4bcf-b972-a505fe7d5004",
    "claim": "Although the PRKGC+NS model receives supervision about human-generated NLDs, paths with the maximum score do not match human-generated NLDs to any significant extent.",
    "label": "refutes",
    "table_id": "920eab6e-8e7f-4478-8747-4a518c04bec1"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "74589c10-ac86-4451-a306-ebab9d94558d",
    "claim": "On the other side, H-CMOW shows, among others, improvements at BShift.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog",
    "paper_id": "1908.10719v1",
    "table_caption": "Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
    "table_column_names": [
      "Method",
      "Agenda Turns",
      "Agenda Inform",
      "Agenda Match",
      "Agenda Success"
    ],
    "table_content_values": [
      [
        "GP-MBCM",
        "2.99",
        "19.04",
        "44.29",
        "28.9"
      ],
      [
        "ACER",
        "10.49",
        "77.98",
        "62.83",
        "50.8"
      ],
      [
        "PPO",
        "9.83",
        "83.34",
        "69.09",
        "59.1"
      ],
      [
        "ALDM",
        "12.47",
        "81.20",
        "62.60",
        "61.2"
      ],
      [
        "GDPL-sess",
        "[BOLD] 7.49",
        "88.39",
        "77.56",
        "76.4"
      ],
      [
        "GDPL-discr",
        "7.86",
        "93.21",
        "80.43",
        "80.5"
      ],
      [
        "GDPL",
        "7.64",
        "[BOLD] 94.97",
        "[BOLD] 83.90",
        "[BOLD] 86.5"
      ],
      [
        "[ITALIC] Human",
        "[ITALIC] 7.37",
        "[ITALIC] 66.89",
        "[ITALIC] 95.29",
        "[ITALIC] 75.0"
      ]
    ],
    "id": "3fb56301-2d20-409e-adba-fc469f55b83a",
    "claim": "Surprisingly, GDPL outperforms human in completing the task, and its average dialog turns are even lower than those of humans, though GDPL is superior in terms of match rate.",
    "label": "refutes",
    "table_id": "bb609870-8753-422b-9aca-20000037e7a1"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Iteration=1",
        "0.531",
        "0.455",
        "0.353",
        "0.201"
      ],
      [
        "Iteration=2",
        "0.592",
        "0.498",
        "0.385",
        "0.375"
      ],
      [
        "Iteration=3",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ],
      [
        "Iteration=4",
        "0.601",
        "0.505",
        "0.422",
        "0.385"
      ],
      [
        "Iteration=5",
        "0.575",
        "0.495",
        "0.394",
        "0.376"
      ]
    ],
    "id": "fef58ae6-dbba-475a-a44e-29eff547da7d",
    "claim": "We find that the performance does not reach the best when iteration is set to 3.",
    "label": "refutes",
    "table_id": "e8d636ab-66cd-4c48-97ea-e05fd1ff69c0"
  },
  {
    "paper": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
    "paper_id": "1909.02622v2",
    "table_caption": "Table 5: Comparison on hard and soft alignments.",
    "table_column_names": [
      "Metrics",
      "cs-en",
      "de-en",
      "fi-en",
      "lv-en"
    ],
    "table_content_values": [
      [
        "RUSE",
        "0.624",
        "0.644",
        "0.750",
        "0.697"
      ],
      [
        "Hmd-F1 + BERT",
        "0.655",
        "0.681",
        "0.821",
        "0.712"
      ],
      [
        "Hmd-Recall + BERT",
        "0.651",
        "0.658",
        "0.788",
        "0.681"
      ],
      [
        "Hmd-Prec + BERT",
        "0.624",
        "0.669",
        "0.817",
        "0.707"
      ],
      [
        "Wmd-unigram + BERT",
        "0.651",
        "0.686",
        "<bold>0.823</bold>",
        "0.710"
      ],
      [
        "Wmd-bigram + BERT",
        "<bold>0.665</bold>",
        "<bold>0.688</bold>",
        "0.821",
        "<bold>0.712</bold>"
      ]
    ],
    "id": "edb918b8-c14e-4bdd-b9c3-0e99a12248e1",
    "claim": "[CONTINUE] We also observe that WMD-BIGRAMS slightly outperforms WMD-UNIGRAMS on 3 out of 4 language pairs.",
    "label": "supports",
    "table_id": "ce450197-224e-43a0-8c7e-7bd213655261"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 5: Scores for different training objectives on the supervised downstream tasks.",
    "table_column_names": [
      "Method",
      "SUBJ",
      "CR",
      "MR",
      "MPQA",
      "MRPC",
      "TREC",
      "SICK-E",
      "SST2",
      "SST5",
      "STS-B",
      "SICK-R"
    ],
    "table_content_values": [
      [
        "CMOW-C",
        "85.9",
        "72.1",
        "69.4",
        "87.0",
        "[BOLD] 71.9",
        "85.4",
        "74.2",
        "73.8",
        "37.6",
        "54.6",
        "71.3"
      ],
      [
        "CMOW-R",
        "[BOLD] 87.5",
        "[BOLD] 73.4",
        "[BOLD] 70.6",
        "[BOLD] 87.3",
        "69.6",
        "[BOLD] 88.0",
        "[BOLD] 77.2",
        "[BOLD] 74.7",
        "[BOLD] 37.9",
        "[BOLD] 56.5",
        "[BOLD] 76.2"
      ],
      [
        "CBOW-C",
        "[BOLD] 90.0",
        "[BOLD] 79.3",
        "[BOLD] 74.6",
        "[BOLD] 87.5",
        "[BOLD] 72.9",
        "85.0",
        "[BOLD] 80.0",
        "78.4",
        "41.0",
        "60.5",
        "[BOLD] 79.2"
      ],
      [
        "CBOW-R",
        "[BOLD] 90.0",
        "79.2",
        "74.0",
        "87.1",
        "71.6",
        "[BOLD] 85.6",
        "78.9",
        "[BOLD] 78.5",
        "[BOLD] 42.1",
        "[BOLD] 61.0",
        "78.1"
      ]
    ],
    "id": "2e23ce40-52ea-404d-bf13-83d9dedb36a6",
    "claim": "Consequently, CMOW-R also outperforms CMOW-C on 10 out of 11 supervised [CONTINUE] downstream tasks [CONTINUE] On average over all downstream tasks, the relative improvement is 20.8%.",
    "label": "supports",
    "table_id": "561d4da9-91e0-414b-bd80-5d62284815c0"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "6757f6db-0ade-4eaf-9769-d1a599048b35",
    "claim": "A complementary behavior can be observed for H-CBOW, whose scores on Word Content are increased.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE VII: Precision scores for the Analogy Test",
    "table_column_names": [
      "Methods",
      "# dims",
      "Analg. (sem)",
      "Analg. (syn)",
      "Total"
    ],
    "table_content_values": [
      [
        "GloVe",
        "300",
        "78.94",
        "64.12",
        "70.99"
      ],
      [
        "Word2Vec",
        "300",
        "81.03",
        "66.11",
        "73.03"
      ],
      [
        "OIWE-IPG",
        "300",
        "19.99",
        "23.44",
        "21.84"
      ],
      [
        "SOV",
        "3000",
        "64.09",
        "46.26",
        "54.53"
      ],
      [
        "SPINE",
        "1000",
        "17.07",
        "8.68",
        "12.57"
      ],
      [
        "Word2Sense",
        "2250",
        "12.94",
        "19.44",
        "5.84"
      ],
      [
        "Proposed",
        "300",
        "79.96",
        "63.52",
        "71.15"
      ]
    ],
    "id": "a0791422-37ec-4a81-bcf0-24e3040fc3cf",
    "claim": "we see that analogical reasoning abilities of the learned embeddings are almost close to the distributed word representations.",
    "label": "not enough info",
    "table_id": "80e04fae-0fb3-409d-9331-7f98803876fa"
  },
  {
    "paper": "Automatically Identifying Complaints in Social Media",
    "paper_id": "1906.03890v1",
    "table_caption": "Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
    "table_column_names": [
      "[BOLD] Model",
      "[BOLD] Acc",
      "[BOLD] F1",
      "[BOLD] AUC"
    ],
    "table_content_values": [
      [
        "Most Frequent Class",
        "64.2",
        "39.1",
        "0.500"
      ],
      [
        "Logistic Regression",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "Sentiment – MPQA",
        "64.2",
        "39.1",
        "0.499"
      ],
      [
        "Sentiment – NRC",
        "63.9",
        "42.2",
        "0.599"
      ],
      [
        "Sentiment – V&B",
        "68.9",
        "60.0",
        "0.696"
      ],
      [
        "Sentiment – VADER",
        "66.0",
        "54.2",
        "0.654"
      ],
      [
        "Sentiment – Stanford",
        "68.0",
        "55.6",
        "0.696"
      ],
      [
        "Complaint Specific (all)",
        "65.7",
        "55.2",
        "0.634"
      ],
      [
        "Request",
        "64.2",
        "39.1",
        "0.583"
      ],
      [
        "Intensifiers",
        "64.5",
        "47.3",
        "0.639"
      ],
      [
        "Downgraders",
        "65.4",
        "49.8",
        "0.615"
      ],
      [
        "Temporal References",
        "64.2",
        "43.7",
        "0.535"
      ],
      [
        "Pronoun Types",
        "64.1",
        "39.1",
        "0.545"
      ],
      [
        "POS Bigrams",
        "72.2",
        "66.8",
        "0.756"
      ],
      [
        "LIWC",
        "71.6",
        "65.8",
        "0.784"
      ],
      [
        "Word2Vec Clusters",
        "67.7",
        "58.3",
        "0.738"
      ],
      [
        "Bag-of-Words",
        "79.8",
        "77.5",
        "0.866"
      ],
      [
        "All Features",
        "[BOLD] 80.5",
        "[BOLD] 78.0",
        "[BOLD] 0.873"
      ],
      [
        "Neural Networks",
        "[EMPTY]",
        "[EMPTY]",
        "[EMPTY]"
      ],
      [
        "MLP",
        "78.3",
        "76.2",
        "0.845"
      ],
      [
        "LSTM",
        "80.2",
        "77.0",
        "0.864"
      ]
    ],
    "id": "40dbe297-6a42-4c99-80d4-e1fe1f162ad7",
    "claim": "However, models trained using linguistic features on the training data do not obtain significantly higher predictive accuracy.",
    "label": "refutes",
    "table_id": "caac7a4b-bc0e-4b38-bc24-9d0ad39f2fa7"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions",
    "table_column_names": [
      "[EMPTY]",
      "GloVe",
      "Imparted"
    ],
    "table_content_values": [
      [
        "Participants 1 to 5",
        "80/88/82/78/97",
        "212/170/207/229/242"
      ],
      [
        "Mean/Std",
        "85/6.9",
        "212/24.4"
      ]
    ],
    "id": "60de77be-65de-40b2-825e-584f482018c5",
    "claim": "RSI  “119.99”  requires  “RSI  <  120.00”  and RSI = `89.20` therefore does not require.",
    "label": "not enough info",
    "table_id": "2f5b7862-207a-488a-a3ff-ef3d8b475081"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 2: Precisions on the Wikidata dataset.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Rank+ExATT",
        "0.584",
        "0.535",
        "0.487",
        "0.392"
      ],
      [
        "PCNN+ATT (m)",
        "0.365",
        "0.317",
        "0.213",
        "0.204"
      ],
      [
        "PCNN+ATT (1)",
        "0.665",
        "0.517",
        "0.413",
        "0.396"
      ],
      [
        "Our Model",
        "0.650",
        "0.519",
        "0.422",
        "[BOLD] 0.405"
      ]
    ],
    "id": "d5f3c5c5-453a-4fea-b639-0ab741aac988",
    "claim": "from this Table, we can clearly see the effect of exploring hierarchical structure is more significant at higher recall rates, so we can improve performance via attention mechanism at higher recall rate.",
    "label": "not enough info",
    "table_id": "e7585b2d-3ad0-4ef0-bedb-fbe70c768b56"
  },
  {
    "paper": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction",
    "paper_id": "1812.11321v1",
    "table_caption": "Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
    "table_column_names": [
      "Recall",
      "0.1",
      "0.2",
      "0.3",
      "AUC"
    ],
    "table_content_values": [
      [
        "Iteration=1",
        "0.531",
        "0.455",
        "0.353",
        "0.201"
      ],
      [
        "Iteration=2",
        "0.592",
        "0.498",
        "0.385",
        "0.375"
      ],
      [
        "Iteration=3",
        "0.650",
        "0.519",
        "0.422",
        "0.405"
      ],
      [
        "Iteration=4",
        "0.601",
        "0.505",
        "0.422",
        "0.385"
      ],
      [
        "Iteration=5",
        "0.575",
        "0.495",
        "0.394",
        "0.376"
      ]
    ],
    "id": "0aa1974d-d983-4dcb-9007-35874e8431fc",
    "claim": "We find that the performance reach the best when iteration is set to 3.",
    "label": "supports",
    "table_id": "e8d636ab-66cd-4c48-97ea-e05fd1ff69c0"
  },
  {
    "paper": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution",
    "paper_id": "1906.01753v1",
    "table_caption": "Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
    "table_column_names": [
      "<bold>Model</bold>",
      "R",
      "MUC P",
      "<italic>F</italic>1",
      "R",
      "B3 P",
      "<italic>F</italic>1",
      "R",
      "CEAF-<italic>e</italic> P",
      "<italic>F</italic>1",
      "CoNLL <italic>F</italic>1"
    ],
    "table_content_values": [
      [
        "Cluster+Lemma",
        "71.3",
        "83",
        "76.7",
        "53.4",
        "84.9",
        "65.6",
        "70.1",
        "52.5",
        "60",
        "67.4"
      ],
      [
        "Disjoint",
        "76.7",
        "80.8",
        "78.7",
        "63.2",
        "78.2",
        "69.9",
        "65.3",
        "58.3",
        "61.6",
        "70"
      ],
      [
        "Joint",
        "78.6",
        "80.9",
        "79.7",
        "65.5",
        "76.4",
        "70.5",
        "65.4",
        "61.3",
        "63.3",
        "<bold>71.2</bold>"
      ]
    ],
    "id": "98afd6c9-ab67-4bbb-afa2-4fd3d470b8ae",
    "claim": "we also removed the duplicate mentions identified by the lemmatisation-based method (reduced), and the effect was to boost cross-document results on the best of these sets (Joint+reduced) by a further 0.9% for all measures.",
    "label": "not enough info",
    "table_id": "b15aafc3-565b-40a2-bd60-36ea181c3b13"
  },
  {
    "paper": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "paper_id": "1807.07279v3",
    "table_caption": "TABLE IX: Accuracies (%) for Sentiment Classification Task",
    "table_column_names": [
      "GloVe",
      "Word2Vec",
      "OIWE-IPG",
      "SOV",
      "SPINE",
      "Word2Sense",
      "Proposed"
    ],
    "table_content_values": [
      [
        "77.34",
        "77.91",
        "74.27",
        "78.43",
        "74.13",
        "81.21",
        "78.26"
      ]
    ],
    "id": "f15c9ba1-b45b-4353-9e9a-50abe7adc4a1",
    "claim": "compared to GloVe and Word2Vec, our sense-based distributed representations can be considered as an initial attempt to incorporate sense-level information.",
    "label": "not enough info",
    "table_id": "630067fd-fd20-4652-b9b6-6a748a9aa8fc"
  },
  {
    "paper": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "paper_id": "1909.01214v1",
    "table_caption": "Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
    "table_column_names": [
      "Metric",
      "[ITALIC] ρ",
      "[ITALIC] r",
      "G-Pre",
      "G-Rec"
    ],
    "table_content_values": [
      [
        "ROUGE-1",
        ".290",
        ".304",
        ".392",
        ".428"
      ],
      [
        "ROUGE-2",
        ".259",
        ".278",
        ".408",
        ".444"
      ],
      [
        "ROUGE-L",
        ".274",
        ".297",
        ".390",
        ".426"
      ],
      [
        "ROUGE-SU4",
        ".282",
        ".279",
        ".404",
        ".440"
      ],
      [
        "BLEU-1",
        ".256",
        ".281",
        ".409",
        ".448"
      ],
      [
        "BLEU-2",
        ".301",
        ".312",
        ".411",
        ".446"
      ],
      [
        "BLEU-3",
        ".317",
        ".312",
        ".409",
        ".444"
      ],
      [
        "BLEU-4",
        ".311",
        ".307",
        ".409",
        ".446"
      ],
      [
        "BLEU-5",
        ".308",
        ".303",
        ".420",
        ".459"
      ],
      [
        "METEOR",
        ".305",
        ".285",
        ".409",
        ".444"
      ],
      [
        "InferSent-Cosine",
        "[BOLD] .329",
        "[BOLD] .339",
        ".417",
        ".460"
      ],
      [
        "BERT-Cosine",
        ".312",
        ".335",
        "[BOLD] .440",
        "[BOLD] .484"
      ]
    ],
    "id": "9bf538d5-cb82-4db3-a4ba-e64ae1c67891",
    "claim": "BERT cosine performs the best.",
    "label": "not enough info",
    "table_id": "1952ac36-8106-448f-af01-1fafc9557f0b"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 7: Negation classifier performance for scope detection with gold cues and scope.",
    "table_column_names": [
      "[EMPTY]",
      "[BOLD] Punctuation",
      "[BOLD] BiLSTM",
      "[BOLD] Proposed"
    ],
    "table_content_values": [
      [
        "In-scope (F)",
        "0.66",
        "0.88",
        "0.85"
      ],
      [
        "Out-scope (F)",
        "0.87",
        "0.97",
        "0.97"
      ],
      [
        "PCS",
        "0.52",
        "0.72",
        "0.72"
      ]
    ],
    "id": "617855fc-ec87-4254-aded-7cee3956b79e",
    "claim": "For a training set of 0.9M training examples, the proposed method reaches comparable classification performance to a BiLSTM approach.",
    "label": "not enough info",
    "table_id": "8e3a17e6-6a5a-4335-86c2-33751204e9ff"
  },
  {
    "paper": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations",
    "paper_id": "1906.04706v1",
    "table_caption": "Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
    "table_column_names": [
      "Classifier",
      "Positive Sentiment Precision",
      "Positive Sentiment Recall",
      "Positive Sentiment Fscore"
    ],
    "table_content_values": [
      [
        "SVM-w/o neg.",
        "0.57",
        "0.72",
        "0.64"
      ],
      [
        "SVM-Punct. neg.",
        "0.58",
        "0.70",
        "0.63"
      ],
      [
        "SVM-our-neg.",
        "0.58",
        "0.73",
        "0.65"
      ],
      [
        "CNN",
        "0.63",
        "0.83",
        "0.72"
      ],
      [
        "CNN-LSTM",
        "0.71",
        "0.72",
        "0.72"
      ],
      [
        "CNN-LSTM-Our-neg-Ant",
        "[BOLD] 0.78",
        "[BOLD] 0.77",
        "[BOLD] 0.78"
      ],
      [
        "[EMPTY]",
        "Negative Sentiment",
        "Negative Sentiment",
        "Negative Sentiment"
      ],
      [
        "[EMPTY]",
        "Precision",
        "Recall",
        "Fscore"
      ],
      [
        "SVM-w/o neg.",
        "0.78",
        "0.86",
        "0.82"
      ],
      [
        "SVM-Punct. neg.",
        "0.78",
        "0.87",
        "0.83"
      ],
      [
        "SVM-Our neg.",
        "0.80",
        "0.87",
        "0.83"
      ],
      [
        "CNN",
        "0.88",
        "0.72",
        "0.79"
      ],
      [
        "CNN-LSTM.",
        "0.83",
        "0.83",
        "0.83"
      ],
      [
        "CNN-LSTM-our-neg-Ant",
        "[BOLD] 0.87",
        "[BOLD] 0.87",
        "[BOLD] 0.87"
      ],
      [
        "[EMPTY]",
        "Train",
        "[EMPTY]",
        "Test"
      ],
      [
        "Positive tweets",
        "5121",
        "[EMPTY]",
        "1320"
      ],
      [
        "Negative tweets",
        "9094",
        "[EMPTY]",
        "2244"
      ]
    ],
    "id": "e88c610b-a5ef-4041-9abb-e72a4de16777",
    "claim": "our evaluation F1-score (Macro) is 82.28%, which is slightly lower than those reported in [23] (87.5%).",
    "label": "not enough info",
    "table_id": "a9cff8bc-0731-4912-9c6d-f1be35b98bcd"
  },
  {
    "paper": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources",
    "paper_id": "7",
    "table_caption": "Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
    "table_column_names": [
      "[EMPTY]",
      "WN-N P",
      "WN-N R",
      "WN-N F",
      "WN-V P",
      "WN-V R",
      "WN-V F",
      "VN P",
      "VN R",
      "VN F"
    ],
    "table_content_values": [
      [
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2",
        "Context: w2"
      ],
      [
        "type",
        ".700",
        ".654",
        ".676",
        ".535",
        ".474",
        ".503",
        ".327",
        ".309",
        ".318"
      ],
      [
        "x+POS",
        ".699",
        ".651",
        ".674",
        ".544",
        ".472",
        ".505",
        ".339",
        ".312",
        ".325"
      ],
      [
        "lemma",
        ".706",
        ".660",
        ".682",
        ".576",
        ".520",
        ".547",
        ".384",
        ".360",
        ".371"
      ],
      [
        "x+POS",
        "<bold>.710</bold>",
        "<bold>.662</bold>",
        "<bold>.685</bold>",
        "<bold>.589</bold>",
        "<bold>.529</bold>",
        "<bold>.557</bold>",
        "<bold>.410</bold>",
        "<bold>.389</bold>",
        "<bold>.399</bold>"
      ],
      [
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep",
        "Context: dep"
      ],
      [
        "type",
        ".712",
        ".661",
        ".686",
        ".545",
        ".457",
        ".497",
        ".324",
        ".296",
        ".310"
      ],
      [
        "x+POS",
        ".715",
        ".659",
        ".686",
        ".560",
        ".464",
        ".508",
        ".349",
        ".320",
        ".334"
      ],
      [
        "lemma",
        "<bold>.725</bold>",
        "<bold>.668</bold>",
        "<bold>.696</bold>",
        ".591",
        ".512",
        ".548",
        ".408",
        ".371",
        ".388"
      ],
      [
        "x+POS",
        ".722",
        ".666",
        ".693",
        "<bold>.609</bold>",
        "<bold>.527</bold>",
        "<bold>.565</bold>",
        "<bold>.412</bold>",
        "<bold>.381</bold>",
        "<bold>.396</bold>"
      ]
    ],
    "id": "7219f414-9986-4c5d-aea4-67f4f4dc1ed0",
    "claim": "Lemma-based targets do not significantly outperform type-based targets in terms of F-measure in all cases.",
    "label": "refutes",
    "table_id": "051fe422-03d4-44d5-836c-7f982b328555"
  },
  {
    "paper": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
    "paper_id": "1911.00225v1",
    "table_caption": "Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
    "table_column_names": [
      "Cue",
      "App.",
      "Prod.",
      "Cov."
    ],
    "table_content_values": [
      [
        "in",
        "47",
        "55.3",
        "9.40"
      ],
      [
        "was",
        "55",
        "61.8",
        "11.0"
      ],
      [
        "to",
        "82",
        "40.2",
        "16.4"
      ],
      [
        "the",
        "85",
        "38.8",
        "17.0"
      ],
      [
        "a",
        "106",
        "57.5",
        "21.2"
      ]
    ],
    "id": "558e297e-a6a9-4acc-9cbd-745799ce92bb",
    "claim": "for example, for [cue:was] the 61.8% of the outcome categories are produced by instances whose premise begins with [cue:was].",
    "label": "not enough info",
    "table_id": "f8558359-3c23-41be-a6b9-df8c38137db1"
  },
  {
    "paper": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "paper_id": "1902.06423v1",
    "table_caption": "Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.",
    "table_column_names": [
      "Dim",
      "Method",
      "Depth",
      "BShift",
      "SubjNum",
      "Tense",
      "CoordInv",
      "Length",
      "ObjNum",
      "TopConst",
      "SOMO",
      "WC"
    ],
    "table_content_values": [
      [
        "400",
        "CBOW/400",
        "32.5",
        "50.2",
        "78.9",
        "78.7",
        "53.6",
        "73.6",
        "79.0",
        "69.6",
        "48.9",
        "86.7"
      ],
      [
        "400",
        "CMOW/400",
        "[BOLD] 34.4",
        "68.8",
        "80.1",
        "[BOLD] 79.9",
        "[BOLD] 59.8",
        "81.9",
        "[BOLD] 79.2",
        "[BOLD] 70.7",
        "[BOLD] 50.3",
        "70.7"
      ],
      [
        "400",
        "H-CBOW",
        "31.2",
        "50.2",
        "77.2",
        "78.8",
        "52.6",
        "77.5",
        "76.1",
        "66.1",
        "49.2",
        "[BOLD] 87.2"
      ],
      [
        "400",
        "H-CMOW",
        "32.3",
        "[BOLD] 70.8",
        "[BOLD] 81.3",
        "76.0",
        "59.6",
        "[BOLD] 82.3",
        "77.4",
        "70.0",
        "50.2",
        "38.2"
      ],
      [
        "784",
        "CBOW/784",
        "33.0",
        "49.6",
        "79.3",
        "78.4",
        "53.6",
        "74.5",
        "78.6",
        "72.0",
        "49.6",
        "[BOLD] 89.5"
      ],
      [
        "784",
        "CMOW/784",
        "[BOLD] 35.1",
        "[BOLD] 70.8",
        "[BOLD] 82.0",
        "80.2",
        "[BOLD] 61.8",
        "82.8",
        "[BOLD] 79.7",
        "74.2",
        "[BOLD] 50.7",
        "72.9"
      ],
      [
        "800",
        "Hybrid",
        "35.0",
        "[BOLD] 70.8",
        "81.7",
        "[BOLD] 81.0",
        "59.4",
        "[BOLD] 84.4",
        "79.0",
        "[BOLD] 74.3",
        "49.3",
        "87.6"
      ],
      [
        "-",
        "cmp. CBOW",
        "+6.1%",
        "+42.7%",
        "+3%",
        "+3.3%",
        "+10.8%",
        "+13.3%",
        "+0.5%",
        "+3.2%",
        "-0.6%",
        "-2.1%"
      ],
      [
        "-",
        "cmp. CMOW",
        "-0.3%",
        "+-0%",
        "-0.4%",
        "+1%",
        "-3.9%",
        "+1.9%",
        "-0.9%",
        "+0.1%",
        "-2.8%",
        "+20.9%"
      ]
    ],
    "id": "c77a5686-dae1-4f25-896b-87bf07e2494a",
    "claim": "The hybrid model yields scores close to or even above the better model of the two on all tasks.",
    "label": "supports",
    "table_id": "954bdf8d-a92d-4036-a667-cb6a5c14a94b"
  },
  {
    "paper": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "paper_id": "1905.07189v2",
    "table_caption": "Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
    "table_column_names": [
      "System",
      "All P",
      "All R",
      "All F1",
      "In  [ITALIC] E+ P",
      "In  [ITALIC] E+ R",
      "In  [ITALIC] E+ F1"
    ],
    "table_content_values": [
      [
        "Name matching",
        "15.03",
        "15.03",
        "15.03",
        "29.13",
        "29.13",
        "29.13"
      ],
      [
        "MIL (model 1)",
        "35.87",
        "35.87",
        "35.87 ±0.72",
        "69.38",
        "69.38",
        "69.38 ±1.29"
      ],
      [
        "MIL-ND (model 2)",
        "37.42",
        "[BOLD] 37.42",
        "37.42 ±0.35",
        "72.50",
        "[BOLD] 72.50",
        "[BOLD] 72.50 ±0.68"
      ],
      [
        "[ITALIC] τMIL-ND (model 2)",
        "[BOLD] 38.91",
        "36.73",
        "[BOLD] 37.78 ±0.26",
        "[BOLD] 73.19",
        "71.15",
        "72.16 ±0.48"
      ],
      [
        "Supervised learning",
        "42.90",
        "42.90",
        "42.90 ±0.59",
        "83.12",
        "83.12",
        "83.12 ±1.15"
      ]
    ],
    "id": "7a288829-0028-438d-a925-79cb2943fdc4",
    "claim": "[CONTINUE] MIL-ND significantly outperforms MIL: the 95% confidence intervals for them do not overlap.",
    "label": "supports",
    "table_id": "ce21ef70-1b61-4a48-9c97-69426861922c"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
    "table_column_names": [
      "Model & Decoding Scheme",
      "Act # w/o",
      "Act # w/",
      "Slot # w/o",
      "Slot # w/"
    ],
    "table_content_values": [
      [
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines",
        "Single-Action Baselines"
      ],
      [
        "DAMD + greedy",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "1.95",
        "[BOLD] 2.51"
      ],
      [
        "HDSA + fixed threshold",
        "[BOLD] 1.00",
        "[BOLD] 1.00",
        "2.07",
        "[BOLD] 2.40"
      ],
      [
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation",
        "5-Action Generation"
      ],
      [
        "DAMD + beam search",
        "2.67",
        "[BOLD] 2.87",
        "3.36",
        "[BOLD] 4.39"
      ],
      [
        "DAMD + diverse beam search",
        "2.68",
        "[BOLD] 2.88",
        "3.41",
        "[BOLD] 4.50"
      ],
      [
        "DAMD + top-k sampling",
        "3.08",
        "[BOLD] 3.43",
        "3.61",
        "[BOLD] 4.91"
      ],
      [
        "DAMD + top-p sampling",
        "3.08",
        "[BOLD] 3.40",
        "3.79",
        "[BOLD] 5.20"
      ],
      [
        "HDSA + sampled threshold",
        "1.32",
        "[BOLD] 1.50",
        "3.08",
        "[BOLD] 3.31"
      ],
      [
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation",
        "10-Action Generation"
      ],
      [
        "DAMD + beam search",
        "3.06",
        "[BOLD] 3.39",
        "4.06",
        "[BOLD] 5.29"
      ],
      [
        "DAMD + diverse beam search",
        "3.05",
        "[BOLD] 3.39",
        "4.05",
        "[BOLD] 5.31"
      ],
      [
        "DAMD + top-k sampling",
        "3.59",
        "[BOLD] 4.12",
        "4.21",
        "[BOLD] 5.77"
      ],
      [
        "DAMD + top-p sampling",
        "3.53",
        "[BOLD] 4.02",
        "4.41",
        "[BOLD] 6.17"
      ],
      [
        "HDSA + sampled threshold",
        "1.54",
        "[BOLD] 1.83",
        "3.42",
        "[BOLD] 3.92"
      ]
    ],
    "id": "d864633b-50b5-40b7-b5e3-1818ef277bfb",
    "claim": "the results in Table 1 clearly show that the action number threshold, being the simplest, achieves the worst performance on almost all metrics.",
    "label": "not enough info",
    "table_id": "669b05f6-2e67-476f-87c6-71d1777edcf2"
  },
  {
    "paper": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
    "paper_id": "1911.10484v2",
    "table_caption": "Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
    "table_column_names": [
      "Model",
      "Diversity",
      "App",
      "Good%",
      "OK%",
      "Invalid%"
    ],
    "table_content_values": [
      [
        "DAMD",
        "3.12",
        "2.50",
        "56.5%",
        "[BOLD] 37.4%",
        "6.1%"
      ],
      [
        "DAMD (+)",
        "[BOLD] 3.65",
        "[BOLD] 2.53",
        "[BOLD] 63.0%",
        "27.1%",
        "9.9%"
      ],
      [
        "HDSA (+)",
        "2.14",
        "2.47",
        "57.5%",
        "32.5%",
        "[BOLD] 10.0%"
      ]
    ],
    "id": "1dbdb0d9-43e6-49b1-972c-03f0c29e7fb8",
    "claim": "the data augmentation strategy significantly improves the human evaluation performance (Wilcoxon signed-rank test, p-value",
    "label": "not enough info",
    "table_id": "f52e6e90-7740-4727-9608-ce20e4e0e5e8"
  }
]